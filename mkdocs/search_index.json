{
    "docs": [
        {
            "location": "/",
            "text": "Welcome\n\n\nThis is course of computational proability in python.\n\n\n\n\nThis a test block\n\n\nthis is a test block $ax^2 + bx +c = 0$ \n\n x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}",
            "title": "Home"
        },
        {
            "location": "/#welcome",
            "text": "This is course of computational proability in python.   This a test block  this is a test block $ax^2 + bx +c = 0$   x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}",
            "title": "Welcome"
        },
        {
            "location": "/week01/01 Simulating Coin Filps/",
            "text": "Simulating Coin Filps\n\n\n%matplotlib inline\n\n\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nflip_fair_coin()\n\n\n\n\n'heads'\n\n\n\nflips = flip_fair_coins(100)\nplot_discrete_histogram(flips)\n\n\n\n\n\n\nplot_discrete_histogram(flip_fair_coins(10), frequency=True)\n\n\n\n\n\n\nn = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "01 Simulating Coin Filps"
        },
        {
            "location": "/week01/01 Simulating Coin Filps/#simulating-coin-filps",
            "text": "%matplotlib inline  import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *  flip_fair_coin()  'heads'  flips = flip_fair_coins(100)\nplot_discrete_histogram(flips)   plot_discrete_histogram(flip_fair_coins(10), frequency=True)   n = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))  import matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "Simulating Coin Filps"
        },
        {
            "location": "/week01/02 Modeling Uncertainty/",
            "text": "Modeling Uncertainty\n\n\nmodel = {'heads': 1/2, 'tails': 1/2}\n\n\n\n\n\u03a9 = set(model.keys())\n\n\n\n\n\u03a9\n\n\n\n\n{'heads', 'tails'}\n\n\n\nExercise\n\n\nWe have a probabilistic model coded in Python as the following dictionary:\n\n\n{1: 0.4, 2: 0.3, 'cat': 0.3}\n\n\n\n\nWhat is the sample space? (Express your answer as a Python set. For example, the set {spam, eggs} is expressed in Python as {'spam', 'eggs'}.)\n\n\nmodel = {1: 0.4, 2: 0.3, 'cat': 0.3}\n\n\n\n\n\u03a9 = set(model.keys())\n\u03a9\n\n\n\n\n{1, 2, 'cat'}\n\n\n\nValid probabilistic model\n\n\ndef is_valid_model(model):\n    \n\n    Return true if the model is valid probabilistic model else retrun false.\n\n    \n is_valid_model({'hearts': 0, 'clubs': 0.4, 'diamonds': 0.7, 'spades': 0.2})\n    False\n\n    \n is_valid_model({'apple': 0.5, 'orange': 0.4, 'pear': 0.2, 'banana': -0.1})\n    False\n\n    \n is_valid_model({1: 0.4, 2: 0.3, 'cat': 0.3})\n    True\n    \n\n    sum_of_values = 0\n    for key, value in model.items():\n        if value \n 0:\n            return False \n\n        else:\n            sum_of_values += value\n\n    if abs(sum_of_values - 1) \n 0.00001: \n        return True\n\n    else:\n        return False\n        print(sum_of_values)\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nmodel = {'hearts': 0, 'clubs': 0.4, 'diamonds': 0.7, 'spades': 0.2}\nis_valid_model(model)\n\n\n\n\nFalse\n\n\n\nmodel = {'apple': 0.5, 'orange': 0.4, 'pear': 0.2, 'banana': -0.1}\nis_valid_model(model)\n\n\n\n\nFalse\n\n\n\nRolling A Fair Dice\n\n\ndice = {}\nfor i in range(6):\n    dice[i+1] = 1/6\n\n\n\n\nis_valid_model(dice)\n\n\n\n\nTrue\n\n\n\nRolling Tow Fair Dice\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\ntwo_dice\n\n\n\n\n{(1, 1): 0.027777777777777776,\n (1, 2): 0.027777777777777776,\n (1, 3): 0.027777777777777776,\n (1, 4): 0.027777777777777776,\n (1, 5): 0.027777777777777776,\n (1, 6): 0.027777777777777776,\n (2, 1): 0.027777777777777776,\n (2, 2): 0.027777777777777776,\n (2, 3): 0.027777777777777776,\n (2, 4): 0.027777777777777776,\n (2, 5): 0.027777777777777776,\n (2, 6): 0.027777777777777776,\n (3, 1): 0.027777777777777776,\n (3, 2): 0.027777777777777776,\n (3, 3): 0.027777777777777776,\n (3, 4): 0.027777777777777776,\n (3, 5): 0.027777777777777776,\n (3, 6): 0.027777777777777776,\n (4, 1): 0.027777777777777776,\n (4, 2): 0.027777777777777776,\n (4, 3): 0.027777777777777776,\n (4, 4): 0.027777777777777776,\n (4, 5): 0.027777777777777776,\n (4, 6): 0.027777777777777776,\n (5, 1): 0.027777777777777776,\n (5, 2): 0.027777777777777776,\n (5, 3): 0.027777777777777776,\n (5, 4): 0.027777777777777776,\n (5, 5): 0.027777777777777776,\n (5, 6): 0.027777777777777776,\n (6, 1): 0.027777777777777776,\n (6, 2): 0.027777777777777776,\n (6, 3): 0.027777777777777776,\n (6, 4): 0.027777777777777776,\n (6, 5): 0.027777777777777776,\n (6, 6): 0.027777777777777776}\n\n\n\nis_valid_model(two_dice)\n\n\n\n\nTrue",
            "title": "02 Modeling Uncertainty"
        },
        {
            "location": "/week01/02 Modeling Uncertainty/#modeling-uncertainty",
            "text": "model = {'heads': 1/2, 'tails': 1/2}  \u03a9 = set(model.keys())  \u03a9  {'heads', 'tails'}",
            "title": "Modeling Uncertainty"
        },
        {
            "location": "/week01/02 Modeling Uncertainty/#exercise",
            "text": "We have a probabilistic model coded in Python as the following dictionary:  {1: 0.4, 2: 0.3, 'cat': 0.3}  What is the sample space? (Express your answer as a Python set. For example, the set {spam, eggs} is expressed in Python as {'spam', 'eggs'}.)  model = {1: 0.4, 2: 0.3, 'cat': 0.3}  \u03a9 = set(model.keys())\n\u03a9  {1, 2, 'cat'}",
            "title": "Exercise"
        },
        {
            "location": "/week01/02 Modeling Uncertainty/#valid-probabilistic-model",
            "text": "def is_valid_model(model):\n     \n    Return true if the model is valid probabilistic model else retrun false.\n\n      is_valid_model({'hearts': 0, 'clubs': 0.4, 'diamonds': 0.7, 'spades': 0.2})\n    False\n\n      is_valid_model({'apple': 0.5, 'orange': 0.4, 'pear': 0.2, 'banana': -0.1})\n    False\n\n      is_valid_model({1: 0.4, 2: 0.3, 'cat': 0.3})\n    True\n     \n    sum_of_values = 0\n    for key, value in model.items():\n        if value   0:\n            return False \n\n        else:\n            sum_of_values += value\n\n    if abs(sum_of_values - 1)   0.00001: \n        return True\n\n    else:\n        return False\n        print(sum_of_values)\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  model = {'hearts': 0, 'clubs': 0.4, 'diamonds': 0.7, 'spades': 0.2}\nis_valid_model(model)  False  model = {'apple': 0.5, 'orange': 0.4, 'pear': 0.2, 'banana': -0.1}\nis_valid_model(model)  False",
            "title": "Valid probabilistic model"
        },
        {
            "location": "/week01/02 Modeling Uncertainty/#rolling-a-fair-dice",
            "text": "dice = {}\nfor i in range(6):\n    dice[i+1] = 1/6  is_valid_model(dice)  True",
            "title": "Rolling A Fair Dice"
        },
        {
            "location": "/week01/02 Modeling Uncertainty/#rolling-tow-fair-dice",
            "text": "two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  two_dice  {(1, 1): 0.027777777777777776,\n (1, 2): 0.027777777777777776,\n (1, 3): 0.027777777777777776,\n (1, 4): 0.027777777777777776,\n (1, 5): 0.027777777777777776,\n (1, 6): 0.027777777777777776,\n (2, 1): 0.027777777777777776,\n (2, 2): 0.027777777777777776,\n (2, 3): 0.027777777777777776,\n (2, 4): 0.027777777777777776,\n (2, 5): 0.027777777777777776,\n (2, 6): 0.027777777777777776,\n (3, 1): 0.027777777777777776,\n (3, 2): 0.027777777777777776,\n (3, 3): 0.027777777777777776,\n (3, 4): 0.027777777777777776,\n (3, 5): 0.027777777777777776,\n (3, 6): 0.027777777777777776,\n (4, 1): 0.027777777777777776,\n (4, 2): 0.027777777777777776,\n (4, 3): 0.027777777777777776,\n (4, 4): 0.027777777777777776,\n (4, 5): 0.027777777777777776,\n (4, 6): 0.027777777777777776,\n (5, 1): 0.027777777777777776,\n (5, 2): 0.027777777777777776,\n (5, 3): 0.027777777777777776,\n (5, 4): 0.027777777777777776,\n (5, 5): 0.027777777777777776,\n (5, 6): 0.027777777777777776,\n (6, 1): 0.027777777777777776,\n (6, 2): 0.027777777777777776,\n (6, 3): 0.027777777777777776,\n (6, 4): 0.027777777777777776,\n (6, 5): 0.027777777777777776,\n (6, 6): 0.027777777777777776}  is_valid_model(two_dice)  True",
            "title": "Rolling Tow Fair Dice"
        },
        {
            "location": "/week01/03 Probabilities with Events and Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03 Probabilities with Events and Code"
        },
        {
            "location": "/week01/03 Probabilities with Events and Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/week01/03 Probabilities with Events and Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/week01/04 Random Variable/",
            "text": "Random Variable\n\n\nTo mathematically reason about a random variable, we need to somehow keep track of the full range of possibilities for what the random variable's value could be, and how probable different instantiations of the random variable are. The resulting formalism may at first seem a bit odd but as we progress through the course, it will become more apparent how this formalism helps us study real-world problems and address these problems with powerful solutions.\n\n\nTo build up to the formalism, first note, computationally, what happened in the code in the previous part.\n\n\n\n\nFirst, there is an underlying probability space $(\u03a9,P)$, where $\\Omega = { \\text {sunny}, \\text {rainy}, \\text {snowy}}$, and\n\n\n\n\n\n\n\\begin{eqnarray}\n\\mathbb{P}(\\text{sunny}) &=& 1/2, \\\\\n\\mathbb{P}(\\text{rainy}) &=& 1/6, \\\\\n\\mathbb{P}(\\text{snowy}) &=& 1/3.\n\\end{eqnarray}\n\n\n\n\n\n\n\n\nA random outcome $\u03c9\u2208\u03a9$ is sampled using the probabilities given by the probability space (\u03a9,P). This step corresponds to an underlying experiment happening.\n\n\n\n\n\n\nTwo random variables are generated:\n\n\n\n\n\n\nW is set to be equal to \u03c9. As an equation:\n\n\\begin{eqnarray}\nW(\\omega) &=&\\omega\\quad\\text{for }\\omega\\in\\{\\text{sunny},\\text{rainy},\\text{snowy}\\}.\n\\end{eqnarray}\n\nThis step perhaps seems entirely unnecessary, as you might wonder \u201cWhy not just call the random outcome $W$ instead of $\u03c9$?\" Indeed, this step isn't actually necessary for this particular example, but the formalism for random variables has this step to deal with what happens when we encounter a random variable like $I$.\n\n\n\n\n\n\n$I$ is set to $1$ if $\u03c9=\\text{sunny}$, and $0$ otherwise. As an equation:\n\n\\begin{eqnarray}\nI(\\omega)\n&=&\n\\begin{cases}\n  1 & \\text{if }\\omega=\\text{sunny}, \\\\\n  0 & \\text{if }\\omega\\in\\{\\text{rainy},\\text{snowy}\\}.\n\\end{cases}\n\\end{eqnarray}\n\nImportantly, multiple possible outcomes (rainy or snowy) get mapped to the same value $0$ that $I$ can take on.\n\n\n\n\n\n\n\n\n\n\nWe see that random variable $W$ maps the sample space $\\Omega ={ \\text {sunny},\\text {rainy},\\text {snowy}}$ to the same set ${ \\text {sunny},\\text {rainy},\\text {snowy}}$. Meanwhile, random variable I maps the sample space $\\Omega ={ \\text {sunny},\\text {rainy},\\text {snowy}}$ to the set ${0,1}$.\n\n\nIn general:\n\n\nDefinition of a \u201cfinite random variable\" (in this course, we will just call this a \u201crandom variable\"):\n Given a finite probability space $(\u03a9,P)$, a finite random variable $X$ is a mapping from the sample space \u03a9 to a set of values $X$ that random variable $X$ can take on. (We will often call $X$ the \u201calphabet\" of random variable $X$.)\n\n\nFor example, random variable $W$ takes on values in the alphabet ${ \\text {sunny},\\text {rainy},\\text {snowy}}$, and random variable I takes on values in the alphabet ${0,1}$.\n\n\nQuick summary:\n There's an underlying experiment corresponding to probability space $(\u03a9,P)$. Once the experiment is run, let $\u03c9\u2208\u03a9$ denote the outcome of the experiment. Then the random variable takes on the specific value of $X(\u03c9)\u2208X$.\n\n\nExplanation using a picture:\n Continuing with the weather example, we can pictorially see what's going on by looking at the probability tables for: the original probability space, the random variable $W$, and the random variable $I$:\n\n\n\n\nThese tables make it clear that a \u201crandom variable\" really is just reassigning/relabeling what the values are for the possible outcomes in the underlying probability space (given by the top left table):\n\n\n\n\n\n\nIn the top right table, random variable $W$ does not do any sort of relabeling so its probability table looks the same as that of the underlying probability space.\n\n\n\n\n\n\nIn the bottom left table, the random variable $I$ relabels/reassigns \u201csunny\" to 1, and both \u201crainy\" and \u201csnowy\" to $0$. Intuitively, since two of the rows now have the same label $0$, it makes sense to just combine these two rows, adding their probabilities $(16+13=12)$. This results in the bottom right table.\n\n\n\n\n\n\nTechnical note:\n Even though the formal definition of a finite random variable doesn't actually make use of the probability assignment P, the probability assignment will become essential as soon as we talk about how probability works with random variables.\n\n\nTwo Ways to Specify a Random Variable in Code\n\n\nApproach 1.\n Go with the mathematical definition of a random variable. First, specify what the underlying probability space is:\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\n\n\n\n\nW_mapping = {'sunny': 'sunny', 'rainy': 'rainy', 'snowy': 'snowy'}\nI_mapping = {'sunny': 1, 'rainy': 0, 'snowy': 0}\n\n\n\n\nrandom_outcome = sample_from_finite_probability_space(prob_space)\nW = W_mapping[random_outcome]\nI = I_mapping[random_outcome]\n\n\n\n\nW\n\n\n\n\n'rainy'\n\n\n\nApproach 2.\n Remember how we wrote out probability tables for random variables W and I? Let's directly store these probability tables:\n\n\nW_table = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nI_table = {0: 1/2, 1: 1/2}\n\n\n\n\nW = sample_from_finite_probability_space(W_table)\nI = sample_from_finite_probability_space(I_table)\n\n\n\n\nRandom Variables Notation and Terminology\n\n\nIn this course, we denote random variables with capital/uppercase letters, such as $X$, $W$, $I$, etc. We use the phrases \u201cprobability table\", \u201cprobability mass function\" (abbreviated as PMF), and \u201cprobability distribution\" (often simply called a distribution) to mean the same thing, and in particular we denote the probability table for $X$ to be $p_X$ or $p_X(\u22c5)$.\n\n\nWe write $p_X(x)$ to denote the entry of the probability table that has label $x\u2208X$ where $X$ is the set of values that random variable $X$ takes on. Note that we use lowercase letters like $x$ to denote variables storing nonrandom values. We can also look up values in a probability table using specific outcomes, e.g., from earlier, we have $p_W(\\text{rainy})=1/6$ and $p_I(1)=1/2$.\n\n\nNote that we use the same notation as in math where a function f might also be written as $f(\u22c5)$ to explicitly indicate that it is the function of one variable. Both $f$ and $f(\u22c5)$ refer to a function whereas $f(x)$ refers to the value of the function $f$ evaluated at the point $x$.\n\n\nAs an example of how to use all this notation, recall that a probability table consists of nonnegative entries that add up to $1$. In fact, each of the entries is at most $1$ (otherwise the numbers would add to more than $1$). For a random variable $X$ taking on values in $X$, we can write out these constraints as:\n\n\n\n\n0 \\le p_ X(x) \\le 1\\quad \\text {for all }x\\in \\mathcal{X}, \\qquad \\sum _{x \\in \\mathcal{X}} p_ X(x) = 1.\n\n\n\n\nOften in the course, if we are making statements about all possible outcomes of $X$, we will omit writing out the alphabet $X$ explicitly. For example, instead of the above, we might write the following equivalent statement:\n\n\n\n\n0 \\le p_ X(x) \\le 1\\quad \\text {for all }x, \\qquad \\sum _ x p_ X(x) = 1.\n\n\n\n\nExercise\n\n\nConsider the following probability space:\n\n\nprob_space = {'cat': 0.2, 'dog':0.7, 'shark':0.1}\n\n\n\n\nLet's define a random variable $X$ that maps 'cat' and 'dog' both to $5$, and 'shark' to $7$.\n\n\nWhat is the set of values that X can take on? Express your answer as a Python set.\n\n\nmodel = {'cat': 0.2, 'dog':0.7, 'shark':0.1}\nmapping = {'cat': 5, 'dog': 5, 'shark':7}\n\n\n\n\ndef PMF(mapping, model):\n    \nReturns to prob. dist. function for given probabiliy model and random variable\n\n    \n model = {'cat': 0.2, 'dog':0.7, 'shark':0.1}\n    \n mapping = {'cat': 5, 'dog':5, 'shark':7}\n    \n PMF(mapping, model)\n    {5: 0.8999999999999999, 7: 0.1}\n    \n\n\n    new_model = dict()\n    for key, value in mapping.items():\n        if value in new_model:\n            new_model[value] += model[key]\n\n        else:\n            new_model[value] = model[key]\n\n    return new_model\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nnew_model = PMF(mapping, model)\nnew_model\n\n\n\n\n{5: 0.8999999999999999, 7: 0.1}\n\n\n\nX = set(new_model.keys())\nX\n\n\n\n\n{5, 7}\n\n\n\nExercise: Probability with Dice\n\n\nLet random variable $X$ be the sum of rolls of two fair six-sided dice with faces numbered $1$ through $6$.\n\n\nHow many different values can $X$ can take on?\n\n\ntwo_dice = {(i+1, j+1): 1/36 for i in range(6) for j in range(6)}\n\n\n\n\nmapping = {key: sum(key) for key, value in two_dice.items()}\n\n\n\n\nX_sum = PMF(mapping, two_dice)\nX_sum\n\n\n\n\n{2: 0.027777777777777776,\n 3: 0.05555555555555555,\n 4: 0.08333333333333333,\n 5: 0.1111111111111111,\n 6: 0.1388888888888889,\n 7: 0.16666666666666669,\n 8: 0.1388888888888889,\n 9: 0.1111111111111111,\n 10: 0.08333333333333333,\n 11: 0.05555555555555555,\n 12: 0.027777777777777776}\n\n\n\nWhat is the probability that $X=7$? (Hint: An earlier exercise asked you for the event that the two faces sum to $7$.)\n\n\nX_sum[7]\n\n\n\n\n0.16666666666666669\n\n\n\nExercise: Functions of Random Variables\n\n\nConsider the random variable $W$ that we have seen before, where $W=\\text{sunny}$ with probability $1/2$, $W=\\text{rainy}$ with probability $1/6$, and $W=\\text{snowy}$ with probability $1/3$. Consider a function $f$ that maps 'sunny' and 'rainy' to $3$, and 'snowy' to $42$.\n\n\nmodel   = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nmapping = {'sunny': 3, 'rainy': 3, 'snowy': 42}\n\n\n\n\n$f(W)$ is also a random variable. Express the probability table for $f(W)$ as a Python dictionary. (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least 5 decimal places.)\n\n\nnew_model = PMF(mapping, model)\nnew_model\n\n\n\n\n{3: 0.6666666666666666, 42: 0.3333333333333333}\n\n\n\nIs $(f(W))^2$ also a random variable? If yes, provide the probability table for $(f(W))^2$ as a Python dictionary.\n\n\nmodel_square = {key: value**2 for key, value in new_model.items()}\n\n\n\n\ndef is_valid_model(model):\n    \n\n    Return true if the model is valid probabilistic model else retrun false.\n\n    \n is_valid_model({'hearts': 0, 'clubs': 0.4, 'diamonds': 0.7, 'spades': 0.2})\n    False\n\n    \n is_valid_model({'apple': 0.5, 'orange': 0.4, 'pear': 0.2, 'banana': -0.1})\n    False\n\n    \n is_valid_model({1: 0.4, 2: 0.3, 'cat': 0.3})\n    True\n    \n\n    sum_of_values = 0\n    for key, value in model.items():\n        if value \n 0:\n            return False \n\n        else:\n            sum_of_values += value\n\n    if abs(sum_of_values - 1) \n 0.00001: \n        return True\n\n    else:\n        return False\n        print(sum_of_values)\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nis_valid_model(model_square)\n\n\n\n\nFalse",
            "title": "04 Random Variable"
        },
        {
            "location": "/week01/04 Random Variable/#random-variable",
            "text": "To mathematically reason about a random variable, we need to somehow keep track of the full range of possibilities for what the random variable's value could be, and how probable different instantiations of the random variable are. The resulting formalism may at first seem a bit odd but as we progress through the course, it will become more apparent how this formalism helps us study real-world problems and address these problems with powerful solutions.  To build up to the formalism, first note, computationally, what happened in the code in the previous part.   First, there is an underlying probability space $(\u03a9,P)$, where $\\Omega = { \\text {sunny}, \\text {rainy}, \\text {snowy}}$, and    \\begin{eqnarray}\n\\mathbb{P}(\\text{sunny}) &=& 1/2, \\\\\n\\mathbb{P}(\\text{rainy}) &=& 1/6, \\\\\n\\mathbb{P}(\\text{snowy}) &=& 1/3.\n\\end{eqnarray}     A random outcome $\u03c9\u2208\u03a9$ is sampled using the probabilities given by the probability space (\u03a9,P). This step corresponds to an underlying experiment happening.    Two random variables are generated:    W is set to be equal to \u03c9. As an equation: \\begin{eqnarray}\nW(\\omega) &=&\\omega\\quad\\text{for }\\omega\\in\\{\\text{sunny},\\text{rainy},\\text{snowy}\\}.\n\\end{eqnarray} \nThis step perhaps seems entirely unnecessary, as you might wonder \u201cWhy not just call the random outcome $W$ instead of $\u03c9$?\" Indeed, this step isn't actually necessary for this particular example, but the formalism for random variables has this step to deal with what happens when we encounter a random variable like $I$.    $I$ is set to $1$ if $\u03c9=\\text{sunny}$, and $0$ otherwise. As an equation: \\begin{eqnarray}\nI(\\omega)\n&=&\n\\begin{cases}\n  1 & \\text{if }\\omega=\\text{sunny}, \\\\\n  0 & \\text{if }\\omega\\in\\{\\text{rainy},\\text{snowy}\\}.\n\\end{cases}\n\\end{eqnarray} \nImportantly, multiple possible outcomes (rainy or snowy) get mapped to the same value $0$ that $I$ can take on.      We see that random variable $W$ maps the sample space $\\Omega ={ \\text {sunny},\\text {rainy},\\text {snowy}}$ to the same set ${ \\text {sunny},\\text {rainy},\\text {snowy}}$. Meanwhile, random variable I maps the sample space $\\Omega ={ \\text {sunny},\\text {rainy},\\text {snowy}}$ to the set ${0,1}$.  In general:  Definition of a \u201cfinite random variable\" (in this course, we will just call this a \u201crandom variable\"):  Given a finite probability space $(\u03a9,P)$, a finite random variable $X$ is a mapping from the sample space \u03a9 to a set of values $X$ that random variable $X$ can take on. (We will often call $X$ the \u201calphabet\" of random variable $X$.)  For example, random variable $W$ takes on values in the alphabet ${ \\text {sunny},\\text {rainy},\\text {snowy}}$, and random variable I takes on values in the alphabet ${0,1}$.  Quick summary:  There's an underlying experiment corresponding to probability space $(\u03a9,P)$. Once the experiment is run, let $\u03c9\u2208\u03a9$ denote the outcome of the experiment. Then the random variable takes on the specific value of $X(\u03c9)\u2208X$.  Explanation using a picture:  Continuing with the weather example, we can pictorially see what's going on by looking at the probability tables for: the original probability space, the random variable $W$, and the random variable $I$:   These tables make it clear that a \u201crandom variable\" really is just reassigning/relabeling what the values are for the possible outcomes in the underlying probability space (given by the top left table):    In the top right table, random variable $W$ does not do any sort of relabeling so its probability table looks the same as that of the underlying probability space.    In the bottom left table, the random variable $I$ relabels/reassigns \u201csunny\" to 1, and both \u201crainy\" and \u201csnowy\" to $0$. Intuitively, since two of the rows now have the same label $0$, it makes sense to just combine these two rows, adding their probabilities $(16+13=12)$. This results in the bottom right table.    Technical note:  Even though the formal definition of a finite random variable doesn't actually make use of the probability assignment P, the probability assignment will become essential as soon as we talk about how probability works with random variables.",
            "title": "Random Variable"
        },
        {
            "location": "/week01/04 Random Variable/#two-ways-to-specify-a-random-variable-in-code",
            "text": "Approach 1.  Go with the mathematical definition of a random variable. First, specify what the underlying probability space is:  import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}  W_mapping = {'sunny': 'sunny', 'rainy': 'rainy', 'snowy': 'snowy'}\nI_mapping = {'sunny': 1, 'rainy': 0, 'snowy': 0}  random_outcome = sample_from_finite_probability_space(prob_space)\nW = W_mapping[random_outcome]\nI = I_mapping[random_outcome]  W  'rainy'  Approach 2.  Remember how we wrote out probability tables for random variables W and I? Let's directly store these probability tables:  W_table = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nI_table = {0: 1/2, 1: 1/2}  W = sample_from_finite_probability_space(W_table)\nI = sample_from_finite_probability_space(I_table)",
            "title": "Two Ways to Specify a Random Variable in Code"
        },
        {
            "location": "/week01/04 Random Variable/#random-variables-notation-and-terminology",
            "text": "In this course, we denote random variables with capital/uppercase letters, such as $X$, $W$, $I$, etc. We use the phrases \u201cprobability table\", \u201cprobability mass function\" (abbreviated as PMF), and \u201cprobability distribution\" (often simply called a distribution) to mean the same thing, and in particular we denote the probability table for $X$ to be $p_X$ or $p_X(\u22c5)$.  We write $p_X(x)$ to denote the entry of the probability table that has label $x\u2208X$ where $X$ is the set of values that random variable $X$ takes on. Note that we use lowercase letters like $x$ to denote variables storing nonrandom values. We can also look up values in a probability table using specific outcomes, e.g., from earlier, we have $p_W(\\text{rainy})=1/6$ and $p_I(1)=1/2$.  Note that we use the same notation as in math where a function f might also be written as $f(\u22c5)$ to explicitly indicate that it is the function of one variable. Both $f$ and $f(\u22c5)$ refer to a function whereas $f(x)$ refers to the value of the function $f$ evaluated at the point $x$.  As an example of how to use all this notation, recall that a probability table consists of nonnegative entries that add up to $1$. In fact, each of the entries is at most $1$ (otherwise the numbers would add to more than $1$). For a random variable $X$ taking on values in $X$, we can write out these constraints as:   0 \\le p_ X(x) \\le 1\\quad \\text {for all }x\\in \\mathcal{X}, \\qquad \\sum _{x \\in \\mathcal{X}} p_ X(x) = 1.   Often in the course, if we are making statements about all possible outcomes of $X$, we will omit writing out the alphabet $X$ explicitly. For example, instead of the above, we might write the following equivalent statement:   0 \\le p_ X(x) \\le 1\\quad \\text {for all }x, \\qquad \\sum _ x p_ X(x) = 1.",
            "title": "Random Variables Notation and Terminology"
        },
        {
            "location": "/week01/04 Random Variable/#exercise",
            "text": "Consider the following probability space:  prob_space = {'cat': 0.2, 'dog':0.7, 'shark':0.1}  Let's define a random variable $X$ that maps 'cat' and 'dog' both to $5$, and 'shark' to $7$.  What is the set of values that X can take on? Express your answer as a Python set.  model = {'cat': 0.2, 'dog':0.7, 'shark':0.1}\nmapping = {'cat': 5, 'dog': 5, 'shark':7}  def PMF(mapping, model):\n     Returns to prob. dist. function for given probabiliy model and random variable\n\n      model = {'cat': 0.2, 'dog':0.7, 'shark':0.1}\n      mapping = {'cat': 5, 'dog':5, 'shark':7}\n      PMF(mapping, model)\n    {5: 0.8999999999999999, 7: 0.1}\n     \n\n    new_model = dict()\n    for key, value in mapping.items():\n        if value in new_model:\n            new_model[value] += model[key]\n\n        else:\n            new_model[value] = model[key]\n\n    return new_model\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  new_model = PMF(mapping, model)\nnew_model  {5: 0.8999999999999999, 7: 0.1}  X = set(new_model.keys())\nX  {5, 7}",
            "title": "Exercise"
        },
        {
            "location": "/week01/04 Random Variable/#exercise-probability-with-dice",
            "text": "Let random variable $X$ be the sum of rolls of two fair six-sided dice with faces numbered $1$ through $6$.  How many different values can $X$ can take on?  two_dice = {(i+1, j+1): 1/36 for i in range(6) for j in range(6)}  mapping = {key: sum(key) for key, value in two_dice.items()}  X_sum = PMF(mapping, two_dice)\nX_sum  {2: 0.027777777777777776,\n 3: 0.05555555555555555,\n 4: 0.08333333333333333,\n 5: 0.1111111111111111,\n 6: 0.1388888888888889,\n 7: 0.16666666666666669,\n 8: 0.1388888888888889,\n 9: 0.1111111111111111,\n 10: 0.08333333333333333,\n 11: 0.05555555555555555,\n 12: 0.027777777777777776}  What is the probability that $X=7$? (Hint: An earlier exercise asked you for the event that the two faces sum to $7$.)  X_sum[7]  0.16666666666666669",
            "title": "Exercise: Probability with Dice"
        },
        {
            "location": "/week01/04 Random Variable/#exercise-functions-of-random-variables",
            "text": "Consider the random variable $W$ that we have seen before, where $W=\\text{sunny}$ with probability $1/2$, $W=\\text{rainy}$ with probability $1/6$, and $W=\\text{snowy}$ with probability $1/3$. Consider a function $f$ that maps 'sunny' and 'rainy' to $3$, and 'snowy' to $42$.  model   = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nmapping = {'sunny': 3, 'rainy': 3, 'snowy': 42}  $f(W)$ is also a random variable. Express the probability table for $f(W)$ as a Python dictionary. (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least 5 decimal places.)  new_model = PMF(mapping, model)\nnew_model  {3: 0.6666666666666666, 42: 0.3333333333333333}  Is $(f(W))^2$ also a random variable? If yes, provide the probability table for $(f(W))^2$ as a Python dictionary.  model_square = {key: value**2 for key, value in new_model.items()}  def is_valid_model(model):\n     \n    Return true if the model is valid probabilistic model else retrun false.\n\n      is_valid_model({'hearts': 0, 'clubs': 0.4, 'diamonds': 0.7, 'spades': 0.2})\n    False\n\n      is_valid_model({'apple': 0.5, 'orange': 0.4, 'pear': 0.2, 'banana': -0.1})\n    False\n\n      is_valid_model({1: 0.4, 2: 0.3, 'cat': 0.3})\n    True\n     \n    sum_of_values = 0\n    for key, value in model.items():\n        if value   0:\n            return False \n\n        else:\n            sum_of_values += value\n\n    if abs(sum_of_values - 1)   0.00001: \n        return True\n\n    else:\n        return False\n        print(sum_of_values)\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  is_valid_model(model_square)  False",
            "title": "Exercise: Functions of Random Variables"
        },
        {
            "location": "/week02/01 Jointly Distributed Random Variables/",
            "text": "Jointly Distributed Random Variables\n\n\nAt the most basic level, inference refers to using an observation to reason about some unknown quantity. In this course, the observation and the unknown quantity are represented by random variables. The main modeling question is: How do these random variables relate?\n\n\nLet's build on our earlier weather example, where now another outcome of interest appears, the temperature, which we quantize into to possible values \u201chot\" and \u201ccold\". Let's suppose that we have the following probability space:\n\n\n\n\nYou can check that the nonnegative entries do add to $1$. If we let random variable $W$ be the weather (sunny, rainy, snowy) and random variable $T$ be the temperature (hot, cold), then notice that we could rearrange the table in the following fashion:\n\n\n\n\nWhen we talk about two separate random variables, we could view them either as a single \u201csuper\" random variable that happens to consist of a pair of values (the first table; notice the label for each outcome corresponds to a pair of values), or we can view the two separate variables along their own different axes (the second table).\n\n\nThe first table tells us what the underlying probability space is, which includes what the sample space is (just read off the outcome names) and what the probability is for each of the possible outcomes for the underlying experiment at hand.\n\n\nThe second table is called a joint probability table $p_{W,T}$ for random variables $W$ and $T$, and we say that random variables $W$ and $T$ are jointly distributed with the above distribution. Since this table is a rearrangement of the earlier table, it also consists of nonnegative entries that add to $1$.\n\n\nThe joint probability table gives probabilities in which $W$ and $T$ co-occur with specific values. For example, in the above, the event that \u201c$W=\\text{sunny}$\" and the event that \u201c$T=\\text{hot}$\" co-occur with probability $3/10$. Notationally, we write\n\n\n\n\np_{W,T}(\\text {sunny},\\text {hot})=\\mathbb {P}(W=\\text {sunny},T=\\text {hot})=\\frac{3}{10}.\n\n\n\n\nConceptual note:\n Given the joint probability table, we can easily go backwards and write out the first table above, which is the underlying probability space.\n\n\nPreview of inference:\n Inference is all about answering questions like \u201cif we observe that the weather is rainy, what is the probability that the temperature is cold?\" Let's take a look at how one might answer this question.\n\n\nFirst, if we observe that it is rainy, then we know that \u201csunny\" and \u201csnowy\" didn't happen so those rows aren't relevant anymore. So the space of possible realizations of the world has shrunk to two options now: $(W=\\text {rainy},T=\\text {hot})$ or $(W=\\text {rainy},T=\\text {cold})$. But what about the probabilities of these two realizations? It's not just $1/30$ and $2/15$ since these don't sum to $1$ \u2014 by observing things, adjustments can be made to the probabilities of different realizations but they should still form a valid probability space.\n\n\nWhy not just scale both $1/30$ and $2/15$ by the same constant so that they sum to $1$? This can be done by dividing 1/30 and 2/15 by their sum:\n\n\n\n\n\\text {hot:}\\quad \\frac{\\frac{1}{30}}{\\frac{1}{30}+\\frac{2}{15}}=\\frac{1}{5},\\qquad \\text {cold}:\\quad \\frac{\\frac{2}{15}}{\\frac{1}{30}+\\frac{2}{15}}=\\frac{4}{5}.\n\n\n\n\nNow they sum to 1. It turns out that, given that we'ved observed the weather to be rainy, these are the correct probabilities for the two options \u201chot\" and \u201ccold\". Let's formalize the steps. We work backwards, first explaining what the the denominator \u201c$130+215=16$\" above comes from.\n\n\nRepresenting a Joint Probability Table in Code\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nApproach 1: Use dictionaries within a dictionary.\n\n\nprob_W_T = {\n    'sunny' : {'hot': 3/10, 'cold': 1/5 },\n    'rainy' : {'hot': 1/30, 'cold': 2/15},\n    'snowy' : {'hot': 0   , 'cold': 1/3 }\n}\n\n\n\n\nprint_joint_prob_table_dict(prob_W_T)\n\n\n\n\n           cold       hot\nrainy  0.133333  0.033333\nsnowy  0.333333  0.000000\nsunny  0.200000  0.300000\n\n\n\nIf we want the probability of $P(W=\\text{rainy} \\textrm{ and }T=\\text{cold})$, we write:\n\n\nprob_W_T['rainy']['cold']\n\n\n\n\n0.13333333333333333\n\n\n\nNOTE:\n The problem with this representation is that, we can easily retrive a row but getting a column is ussually hard. But if the distribution is sparse(most entries are zero) then writing in dictiaonay will save a lot of space. \n\n\nApproach 2: Use a 2D array.\n\n\nAnother approach is to directly store the joint probability table as a 2D array, separately keeping track of what the rows and columns are. We use a NumPy array (but really you could use Python lists within a Python list, much like how the previous approach used dictionaries within a dictionary; the indexing syntax changes only slightly):\n\n\nimport numpy as np\nprob_W_T_rows = ['sunny', 'rainy', 'snowy']\nprob_W_T_cols = ['hot', 'cold']\nprob_W_T_array = np.array([[3/10, 1/5], [1/30, 2/15], [0, 1/3]])\nprint_joint_prob_table_array(prob_W_T_array, prob_W_T_rows, prob_W_T_cols)\n\n\n\n\n            hot      cold\nsunny  0.300000  0.200000\nrainy  0.033333  0.133333\nsnowy  0.000000  0.333333\n\n\n\nIf we want the probability of $P(W=\\text{rainy} \\textrm{ and }T=\\text{cold})$, we write:\n\n\nprob_W_T_array[prob_W_T_rows.index('rainy'), prob_W_T_cols.index('cold')]\n\n\n\n\n0.13333333333333333",
            "title": "01 Jointly Distributed Random Variables"
        },
        {
            "location": "/week02/01 Jointly Distributed Random Variables/#jointly-distributed-random-variables",
            "text": "At the most basic level, inference refers to using an observation to reason about some unknown quantity. In this course, the observation and the unknown quantity are represented by random variables. The main modeling question is: How do these random variables relate?  Let's build on our earlier weather example, where now another outcome of interest appears, the temperature, which we quantize into to possible values \u201chot\" and \u201ccold\". Let's suppose that we have the following probability space:   You can check that the nonnegative entries do add to $1$. If we let random variable $W$ be the weather (sunny, rainy, snowy) and random variable $T$ be the temperature (hot, cold), then notice that we could rearrange the table in the following fashion:   When we talk about two separate random variables, we could view them either as a single \u201csuper\" random variable that happens to consist of a pair of values (the first table; notice the label for each outcome corresponds to a pair of values), or we can view the two separate variables along their own different axes (the second table).  The first table tells us what the underlying probability space is, which includes what the sample space is (just read off the outcome names) and what the probability is for each of the possible outcomes for the underlying experiment at hand.  The second table is called a joint probability table $p_{W,T}$ for random variables $W$ and $T$, and we say that random variables $W$ and $T$ are jointly distributed with the above distribution. Since this table is a rearrangement of the earlier table, it also consists of nonnegative entries that add to $1$.  The joint probability table gives probabilities in which $W$ and $T$ co-occur with specific values. For example, in the above, the event that \u201c$W=\\text{sunny}$\" and the event that \u201c$T=\\text{hot}$\" co-occur with probability $3/10$. Notationally, we write   p_{W,T}(\\text {sunny},\\text {hot})=\\mathbb {P}(W=\\text {sunny},T=\\text {hot})=\\frac{3}{10}.   Conceptual note:  Given the joint probability table, we can easily go backwards and write out the first table above, which is the underlying probability space.  Preview of inference:  Inference is all about answering questions like \u201cif we observe that the weather is rainy, what is the probability that the temperature is cold?\" Let's take a look at how one might answer this question.  First, if we observe that it is rainy, then we know that \u201csunny\" and \u201csnowy\" didn't happen so those rows aren't relevant anymore. So the space of possible realizations of the world has shrunk to two options now: $(W=\\text {rainy},T=\\text {hot})$ or $(W=\\text {rainy},T=\\text {cold})$. But what about the probabilities of these two realizations? It's not just $1/30$ and $2/15$ since these don't sum to $1$ \u2014 by observing things, adjustments can be made to the probabilities of different realizations but they should still form a valid probability space.  Why not just scale both $1/30$ and $2/15$ by the same constant so that they sum to $1$? This can be done by dividing 1/30 and 2/15 by their sum:   \\text {hot:}\\quad \\frac{\\frac{1}{30}}{\\frac{1}{30}+\\frac{2}{15}}=\\frac{1}{5},\\qquad \\text {cold}:\\quad \\frac{\\frac{2}{15}}{\\frac{1}{30}+\\frac{2}{15}}=\\frac{4}{5}.   Now they sum to 1. It turns out that, given that we'ved observed the weather to be rainy, these are the correct probabilities for the two options \u201chot\" and \u201ccold\". Let's formalize the steps. We work backwards, first explaining what the the denominator \u201c$130+215=16$\" above comes from.",
            "title": "Jointly Distributed Random Variables"
        },
        {
            "location": "/week02/01 Jointly Distributed Random Variables/#representing-a-joint-probability-table-in-code",
            "text": "import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *",
            "title": "Representing a Joint Probability Table in Code"
        },
        {
            "location": "/week02/01 Jointly Distributed Random Variables/#approach-1-use-dictionaries-within-a-dictionary",
            "text": "prob_W_T = {\n    'sunny' : {'hot': 3/10, 'cold': 1/5 },\n    'rainy' : {'hot': 1/30, 'cold': 2/15},\n    'snowy' : {'hot': 0   , 'cold': 1/3 }\n}  print_joint_prob_table_dict(prob_W_T)             cold       hot\nrainy  0.133333  0.033333\nsnowy  0.333333  0.000000\nsunny  0.200000  0.300000  If we want the probability of $P(W=\\text{rainy} \\textrm{ and }T=\\text{cold})$, we write:  prob_W_T['rainy']['cold']  0.13333333333333333  NOTE:  The problem with this representation is that, we can easily retrive a row but getting a column is ussually hard. But if the distribution is sparse(most entries are zero) then writing in dictiaonay will save a lot of space.",
            "title": "Approach 1: Use dictionaries within a dictionary."
        },
        {
            "location": "/week02/01 Jointly Distributed Random Variables/#approach-2-use-a-2d-array",
            "text": "Another approach is to directly store the joint probability table as a 2D array, separately keeping track of what the rows and columns are. We use a NumPy array (but really you could use Python lists within a Python list, much like how the previous approach used dictionaries within a dictionary; the indexing syntax changes only slightly):  import numpy as np\nprob_W_T_rows = ['sunny', 'rainy', 'snowy']\nprob_W_T_cols = ['hot', 'cold']\nprob_W_T_array = np.array([[3/10, 1/5], [1/30, 2/15], [0, 1/3]])\nprint_joint_prob_table_array(prob_W_T_array, prob_W_T_rows, prob_W_T_cols)              hot      cold\nsunny  0.300000  0.200000\nrainy  0.033333  0.133333\nsnowy  0.000000  0.333333  If we want the probability of $P(W=\\text{rainy} \\textrm{ and }T=\\text{cold})$, we write:  prob_W_T_array[prob_W_T_rows.index('rainy'), prob_W_T_cols.index('cold')]  0.13333333333333333",
            "title": "Approach 2: Use a 2D array."
        },
        {
            "location": "/week02/02 Marginalization/",
            "text": "Marginalization\n\n\nGiven a joint probability table, often we'll want to know what the probability table is for just one of the random variables. We can do this by just summing or \u201cmarginalizing\" out the other random variables. For example, to get the probability table for random variable $W$, we do the following:\n\n\n\n\nWe take the joint probability table (left-hand side) and compute out the row sums (which we've written in the margin).\n\n\nThe right-hand side table is the probability table pW for random variable $W$; we call this resulting probability distribution the marginal distribution of $W$ (put another way, it is the distribution obtained by marginalizing out the random variables that aren't $W$).\n\n\nIn terms of notation, the above marginalization procedure whereby we used the joint distribution of $W$ and $T$ to produce the marginal distribution of $W$ is written:\n\n\n\n\np_{W}(w)=\\sum _{t\\in \\mathcal{T}}p_{W,T}(w,t),\n\n\n\n\nwhere $\\mathcal{T}$ is the set of values that random variable $T$ can take on. In fact, throughout this course, we will often omit explicitly writing out the alphabet of values that a random variable takes on, e.g., writing instead\n\n\n\n\np_{W}(w)=\\sum _{t}p_{W,T}(w,t).\n\n\n\n\nIt's clear from context that we're summing over all possible values for $t$, which is going to be the values that random variable $T$ can possibly take on.\n\n\nAs a specific example,\n\n\n\n\np_{W}(\\text {rainy})=\\sum _{t}p_{W,T}(\\text {rainy},t)=\\underbrace{p_{W,T}(\\text {rainy},\\text {hot})}_{1/30}+\\underbrace{p_{W,T}(\\text {rainy},\\text {cold})}_{2/15}=\\frac{1}{6}.\n\n\n\n\nWe could similarly marginalize out random variable $W$ to get the marginal distribution $p_T$ for random variable $T$:\n\n\n\n\n(Note that whether we write a probability table for a single variable horizontally or vertically doesn't actually matter.)\n\n\nAs a formula, we would write:\n\n\n\n\np_{T}(t)=\\sum _{w}p_{W,T}(w,t).\n\n\n\n\nFor example,\n\n\n\n\np_{T}(\\text {hot})=\\sum _{w}p_{W,T}(w,\\text {hot})=\\underbrace{p_{W,T}(\\text {sunny},\\text {hot})}_{3/10}+\\underbrace{p_{W,T}(\\text {rainy},\\text {hot})}_{1/30}+\\underbrace{p_{W,T}(\\text {snowy},\\text {hot})}_{0}=\\frac{1}{3}.\n\n\n\n\nIn general:\n\n\nMarginalization:\n Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$. For any $x\u2208\\mathcal{X}$, the marginal probability that $X=x$ is given by\n\n\n\n\np_{X}(x)=\\sum _{y}p_{X,Y}(x,y).\n\n\n\n\nExercise: Marginalization\n\n\nConsider the following two joint probability tables.\n\n\n\n\nExpress the probability table for random variable $X$ as a Python dictionary (the keys should be the Python strings 'sunny', 'rainy', and 'snowy'). (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least $5$ decimal places.)\n\n\np_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\n\n\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\n\n\n\n{'rainy': 0.16666666666666666, 'snowy': 0.3333333333333333, 'sunny': 0.5}\n\n\n\nExpress the probability table for random variable $Y$ as a Python dictionary (the keys should be the Python integers $0$ and $1$). (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least 5 decimal places.)\n\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y           \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\nFor two random variables $U$ and $V$ that take on values in the same alphabet, we say that $U$ and $V$ have the same distribution if $p_U(a)=p_V(a)$ for all $a$. For the above tables:\n\n\nDo $W$ and $X$ have the same distribution?\n\n\np_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\n\n\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\n\n\n\n{'rainy': 0.16666666666666666, 'snowy': 0.3333333333333333, 'sunny': 0.5}\n\n\n\ndef is_samePMF(X, Y):\n    \n\n    Retrun True if both are same PMF else false\n\n    \n is_samePMF({5: 0.8999999999999999, 7: 0.1}, {5: 0.9, 7: 0.1})\n    True\n\n    \n is_samePMF({5: 0.7, 7: 0.3}, {5: 0.9, 7: 0.1})\n    False\n\n    \n is_samePMF({6: 0.8999999999999999, 7: 0.1}, {5: 0.9, 7: 0.1})\n    False\n\n    \n is_samePMF(dict(), {5: 0.9, 7: 0.1})\n    False\n\n    \n is_samePMF({5: 0.9, 7: 0.1}, {5: 0.1})\n    False\n    \n\n    if not X.keys() == Y.keys():\n        return False \n\n    else:\n        for key, value in X.items():\n            if (abs(X[key] - Y[key]) \n 0.00001):\n                return False\n\n        return True\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nis_samePMF(p_W, p_X)\n\n\n\n\nTrue\n\n\n\nDo I and Y have the same distribution?\n\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n#             print(p_I, key1, value1)\n            p_I[key1] += value1\n\np_I      \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\nis_samePMF(p_I, p_Y)\n\n\n\n\nTrue\n\n\n\nTrue or false:\n Consider two random variables (S,T) and (U,V), where S and U have the same distribution, and T and V have the same distribution. Then (S,T) and (U,V) have the same joint distribution.\n\n\nFalse\n\n\nMarginalization for Many Random Variables\n\n\nWhat happens when we have more than two random variables? Let's build on our earlier example and suppose that in addition to weather $W$ and temperature $T$, we also had a random variable $H$ for humidity that takes on values in the alphabet {dry, humid}. Then having a third random variable, we can draw out a 3D joint probability table for random variables $W, T$, and $H$. As an example, we could have the following:\n\n\n\n\nHere, each of the cubes/boxes stores a probability. Not visible are two of the cubes in the back left column, which for this particular example both have probability values of $0$.\n\n\nThen to marginalize out the humidity $H$, we would add values as follows:\n\n\n\n\nThe result is the joint probability table for weather W and temperature $T$, shown still in 3D cubes with each cube storing a single probability.\n\n\nAs an equation:\n\n\n\n\np_{W,T}(w,t) = \\sum _ h p_{W,T,H}(w, t, h).\n\n\n\n\nIn general, for three random variables $X, Y$, and $Z$ with joint probability table $p_{X,Y,Z}$, we have\n\n\n\n\n\nbegin{align}\np_{X,Y}(x,y) &=& \\sum_{z} p_{X,Y,Z}(x,y,z), \\\\\np_{X,Z}(x,z) &=& \\sum_{y} p_{X,Y,Z}(x,y,z), \\\\\np_{Y,Z}(y,z) &=& \\sum_{x} p_{X,Y,Z}(x,y,z).\n\\end{align}\n\n\n\n\n\nNote that we can marginalize out different random variables in succession. For example, given joint probability table $p_{X,Y,Z}$, if we wanted the probability table $p_X$, we can get it by marginalizing out the two random variables $Y$ and $Z$:\n\n\n\n\np_ X(x) = \\sum _{y} p_{X,Y}(x,y) = \\sum _{y} \\Big( \\sum _{z} p_{X,Y,Z}(x,y,z) \\Big).\n\n\n\n\nEven with more than three random variables, the idea is the same. For example, with four random variables $W, X, Y$, and $Z$ with joint probability table $p_{X,Y,Z}$, if we want the joint probability table for $X$ and $Y$, we would do the following:\n\n\n\n\np_{X,Y}(x, y) = \\sum_w \\Big( \\sum_z p_{W,X,Y,Z}(w,x,y,z) \\Big).\n\n\n\n\nExercise: Marginalization for Many Random Variables\n\n\nSuppose that we have the joint probability table $p_{V,W,X,Y,Z}$ where random variable $V$ takes on $k$ values (i.e., the alphabet for $V$ has $k$ elements in it), $W$ takes on $\u2113$ values, $X$ takes on $m$ values, $Y$ takes on $n$ values, and $Z$ takes on $o$ values.\n\n\n\n\n\n\nQuestion:\n How many entries are in the joint probability table $p_{V,W,X,Y,Z}$?\n\n\nAnswer:\n $k \\times \\ell \\times m \\times n \\times o$\n\n\n$ $\n2.  \nQuestion:\n If we marginalize out $X$ and $Z$, the resulting joint probability table is for which random    variables? (You can select multiple options.)\n\n\nAnswer:\n $V, W$ and $Y$\n\n\n$ $\n3. \nQuestion:\n If we marginalize out $V, Y,$ and $Z$, the resulting joint probability table has how many entries?\n\n\nAnswer:\n $\\ell \\times m$\n\n\n\n\n\n\nNow suppose that we have the joint probability table $p_{X,Y,Z}$ for three random variables $X, Y,$ and $Z$. We want to compute the probability table $p_X$ for random variable $X$.\n\n\n\n\n\n\nTrue or false:\n If we marginalize out $Z$ first and then $Y$, or if we marginalize out $Y$ first and then $Z$, we get the same answer for the probability table $p_X$. In other words, we have\n\n\n\n\np_X(x) = \\sum _y \\Big( \\sum_z p_{X,Y,Z}(x,y,z)\\Big) = \\sum_z \\Big( \\sum_y p_{X,Y,Z}(x,y,z) \\Big).\n\n\n\n\nAnswer:\n True",
            "title": "02 Marginalization"
        },
        {
            "location": "/week02/02 Marginalization/#marginalization",
            "text": "Given a joint probability table, often we'll want to know what the probability table is for just one of the random variables. We can do this by just summing or \u201cmarginalizing\" out the other random variables. For example, to get the probability table for random variable $W$, we do the following:   We take the joint probability table (left-hand side) and compute out the row sums (which we've written in the margin).  The right-hand side table is the probability table pW for random variable $W$; we call this resulting probability distribution the marginal distribution of $W$ (put another way, it is the distribution obtained by marginalizing out the random variables that aren't $W$).  In terms of notation, the above marginalization procedure whereby we used the joint distribution of $W$ and $T$ to produce the marginal distribution of $W$ is written:   p_{W}(w)=\\sum _{t\\in \\mathcal{T}}p_{W,T}(w,t),   where $\\mathcal{T}$ is the set of values that random variable $T$ can take on. In fact, throughout this course, we will often omit explicitly writing out the alphabet of values that a random variable takes on, e.g., writing instead   p_{W}(w)=\\sum _{t}p_{W,T}(w,t).   It's clear from context that we're summing over all possible values for $t$, which is going to be the values that random variable $T$ can possibly take on.  As a specific example,   p_{W}(\\text {rainy})=\\sum _{t}p_{W,T}(\\text {rainy},t)=\\underbrace{p_{W,T}(\\text {rainy},\\text {hot})}_{1/30}+\\underbrace{p_{W,T}(\\text {rainy},\\text {cold})}_{2/15}=\\frac{1}{6}.   We could similarly marginalize out random variable $W$ to get the marginal distribution $p_T$ for random variable $T$:   (Note that whether we write a probability table for a single variable horizontally or vertically doesn't actually matter.)  As a formula, we would write:   p_{T}(t)=\\sum _{w}p_{W,T}(w,t).   For example,   p_{T}(\\text {hot})=\\sum _{w}p_{W,T}(w,\\text {hot})=\\underbrace{p_{W,T}(\\text {sunny},\\text {hot})}_{3/10}+\\underbrace{p_{W,T}(\\text {rainy},\\text {hot})}_{1/30}+\\underbrace{p_{W,T}(\\text {snowy},\\text {hot})}_{0}=\\frac{1}{3}.   In general:  Marginalization:  Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$. For any $x\u2208\\mathcal{X}$, the marginal probability that $X=x$ is given by   p_{X}(x)=\\sum _{y}p_{X,Y}(x,y).",
            "title": "Marginalization"
        },
        {
            "location": "/week02/02 Marginalization/#exercise-marginalization",
            "text": "Consider the following two joint probability tables.   Express the probability table for random variable $X$ as a Python dictionary (the keys should be the Python strings 'sunny', 'rainy', and 'snowy'). (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least $5$ decimal places.)  p_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}  p_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X  {'rainy': 0.16666666666666666, 'snowy': 0.3333333333333333, 'sunny': 0.5}  Express the probability table for random variable $Y$ as a Python dictionary (the keys should be the Python integers $0$ and $1$). (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least 5 decimal places.)  p_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y             {0: 0.5, 1: 0.5}  For two random variables $U$ and $V$ that take on values in the same alphabet, we say that $U$ and $V$ have the same distribution if $p_U(a)=p_V(a)$ for all $a$. For the above tables:  Do $W$ and $X$ have the same distribution?  p_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}  p_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W  {'rainy': 0.16666666666666666, 'snowy': 0.3333333333333333, 'sunny': 0.5}  def is_samePMF(X, Y):\n     \n    Retrun True if both are same PMF else false\n\n      is_samePMF({5: 0.8999999999999999, 7: 0.1}, {5: 0.9, 7: 0.1})\n    True\n\n      is_samePMF({5: 0.7, 7: 0.3}, {5: 0.9, 7: 0.1})\n    False\n\n      is_samePMF({6: 0.8999999999999999, 7: 0.1}, {5: 0.9, 7: 0.1})\n    False\n\n      is_samePMF(dict(), {5: 0.9, 7: 0.1})\n    False\n\n      is_samePMF({5: 0.9, 7: 0.1}, {5: 0.1})\n    False\n     \n    if not X.keys() == Y.keys():\n        return False \n\n    else:\n        for key, value in X.items():\n            if (abs(X[key] - Y[key])   0.00001):\n                return False\n\n        return True\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  is_samePMF(p_W, p_X)  True  Do I and Y have the same distribution?  p_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n#             print(p_I, key1, value1)\n            p_I[key1] += value1\n\np_I        {0: 0.5, 1: 0.5}  is_samePMF(p_I, p_Y)  True  True or false:  Consider two random variables (S,T) and (U,V), where S and U have the same distribution, and T and V have the same distribution. Then (S,T) and (U,V) have the same joint distribution.  False",
            "title": "Exercise: Marginalization"
        },
        {
            "location": "/week02/02 Marginalization/#marginalization-for-many-random-variables",
            "text": "What happens when we have more than two random variables? Let's build on our earlier example and suppose that in addition to weather $W$ and temperature $T$, we also had a random variable $H$ for humidity that takes on values in the alphabet {dry, humid}. Then having a third random variable, we can draw out a 3D joint probability table for random variables $W, T$, and $H$. As an example, we could have the following:   Here, each of the cubes/boxes stores a probability. Not visible are two of the cubes in the back left column, which for this particular example both have probability values of $0$.  Then to marginalize out the humidity $H$, we would add values as follows:   The result is the joint probability table for weather W and temperature $T$, shown still in 3D cubes with each cube storing a single probability.  As an equation:   p_{W,T}(w,t) = \\sum _ h p_{W,T,H}(w, t, h).   In general, for three random variables $X, Y$, and $Z$ with joint probability table $p_{X,Y,Z}$, we have   \nbegin{align}\np_{X,Y}(x,y) &=& \\sum_{z} p_{X,Y,Z}(x,y,z), \\\\\np_{X,Z}(x,z) &=& \\sum_{y} p_{X,Y,Z}(x,y,z), \\\\\np_{Y,Z}(y,z) &=& \\sum_{x} p_{X,Y,Z}(x,y,z).\n\\end{align}   Note that we can marginalize out different random variables in succession. For example, given joint probability table $p_{X,Y,Z}$, if we wanted the probability table $p_X$, we can get it by marginalizing out the two random variables $Y$ and $Z$:   p_ X(x) = \\sum _{y} p_{X,Y}(x,y) = \\sum _{y} \\Big( \\sum _{z} p_{X,Y,Z}(x,y,z) \\Big).   Even with more than three random variables, the idea is the same. For example, with four random variables $W, X, Y$, and $Z$ with joint probability table $p_{X,Y,Z}$, if we want the joint probability table for $X$ and $Y$, we would do the following:   p_{X,Y}(x, y) = \\sum_w \\Big( \\sum_z p_{W,X,Y,Z}(w,x,y,z) \\Big).",
            "title": "Marginalization for Many Random Variables"
        },
        {
            "location": "/week02/02 Marginalization/#exercise-marginalization-for-many-random-variables",
            "text": "Suppose that we have the joint probability table $p_{V,W,X,Y,Z}$ where random variable $V$ takes on $k$ values (i.e., the alphabet for $V$ has $k$ elements in it), $W$ takes on $\u2113$ values, $X$ takes on $m$ values, $Y$ takes on $n$ values, and $Z$ takes on $o$ values.    Question:  How many entries are in the joint probability table $p_{V,W,X,Y,Z}$?  Answer:  $k \\times \\ell \\times m \\times n \\times o$  $ $\n2.   Question:  If we marginalize out $X$ and $Z$, the resulting joint probability table is for which random    variables? (You can select multiple options.)  Answer:  $V, W$ and $Y$  $ $\n3.  Question:  If we marginalize out $V, Y,$ and $Z$, the resulting joint probability table has how many entries?  Answer:  $\\ell \\times m$    Now suppose that we have the joint probability table $p_{X,Y,Z}$ for three random variables $X, Y,$ and $Z$. We want to compute the probability table $p_X$ for random variable $X$.    True or false:  If we marginalize out $Z$ first and then $Y$, or if we marginalize out $Y$ first and then $Z$, we get the same answer for the probability table $p_X$. In other words, we have   p_X(x) = \\sum _y \\Big( \\sum_z p_{X,Y,Z}(x,y,z)\\Big) = \\sum_z \\Big( \\sum_y p_{X,Y,Z}(x,y,z) \\Big).   Answer:  True",
            "title": "Exercise: Marginalization for Many Random Variables"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/",
            "text": "Conditioning for Random Variables\n\n\nWhen we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.\n\n\nWhen we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:\n\n\n\n\nSecond, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:\n\n\n\n\nNotation:\n The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.\n\n\n\n\nIn general:\n\n\nConditioning:\n Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y)\n0$, the \nconditional probability\n of event $X=x$ given event $Y=y$ has happened is\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.\n\n\n\n\nFor example,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.\n\n\n\n\nComputational interpretation:\n To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.\n\n\nExercise: Conditioning for Random Variables\n\n\nConsider the following two joint probability tables.\n\n\n\n\nQuestion 1:\n What is $p_{W|I}(\\text {sunny}|1)$?\n\n\np_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]\n\n\n\n\n1.0\n\n\n\nQuestion 2:\n What is $p_{X|Y}(\\text {sunny}|1)$?\n\n\np_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y     \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]\n\n\n\n\n0.5\n\n\n\nQuestion 3:\n What is $p_{I|W}(1|\\text {snowy})$? \n\n\n# p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)\n\n\n\n\n0\n\n\n\nQuestion 4:\n What is $p_{Y|X}(1|\\text {snowy})$?\n\n\n# p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']\n\n\n\n\n0.5\n\n\n\nExercise: Simpson's Paradox\n\n\nThis problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.\n\n\nWe have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).\n\n\nThus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.\n\n\nThe joint probability table is provided in the file \nsimpsons_paradox_data.py\n. Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).\n\n\nNow let's load in everything from \nsimpsons_paradox_data.py\n:\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nPlease open up \nsimpsons_paradox_data.py\n to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.\n\n\nFor example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:\n\n\njoint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]\n\n\n\n\nSome terminology:\n In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.\n\n\nLet's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).\n\n\nMarginalization is easy to do with NumPy:\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\n\n\n\n\nOn the right-hand side,\n.sum(axis=1)\n says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable \njoint_prob_gender_admission\n stores a 2D joint probability table for random variables $G$ and $A$.\n\n\nNow, for example, the probability that a woman applies and is admitted is given by:\n\n\njoint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\n\n\n\n\nNow let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:\n\n\n\n\n\\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}\n\n\n\n\nLet's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:\n\n\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\n\n\n\n\nNow this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!\n\n\nprob_admission_given_female = female_only / np.sum(female_only)\n\n\n\n\nThis is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:\n\n\nprob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)\n\n\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint(\nProbability of admitted female: {0:.5f}\n.format(prob_admitted_given_female))\n\n\n\n\nProbability of admitted female: 0.30334\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nmale_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint(\nProbability of admitted male: {0:.5f}\n.format(prob_admitted_given_male))\n\n\n\n\nProbability of admitted male: 0.44520\n\n\n\nSo it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.\n\n\nBut before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:\n\n\nadmitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]\n\n\n\n\nNotice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.\n\n\nThus, the conditional probability table of gender given admitted is:\n\n\nprob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)\n\n\n\n\nAll right, now let's look at which departments are favoring men over women.\n\n\nFor the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:\n\n\nfemale_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]\n\n\n\n\nNow let's determine the probability of getting admitted given each gender and each department.\n\n\nDepartment A:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\n# A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint(\nProbability of admitted female in department A: {0:.3f}\n.format(p_A_given_F_and_A))\n\n\n\n\nProbability of admitted female in department A: 0.820\n\n\n\nGeneral Approach\n\n\n# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep\n\n\n\n\narray([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])\n\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]\n\n\n\n\n0.8200000000000004\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]\n\n\n\n\n0.62000000000000011\n\n\n\nDepartment B:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]\n\n\n\n\n0.67999999999999705\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]\n\n\n\n\n0.63000000000000034\n\n\n\nDepartment C\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]\n\n\n\n\n0.34000000000000008\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]\n\n\n\n\n0.37000000000000005\n\n\n\nDepartment D:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]\n\n\n\n\n0.34999999999999998\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment E\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]\n\n\n\n\n0.23999999999999955\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment F\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]\n\n\n\n\n0.069999999999999701\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nQuestion:\n How many of these departments have a higher probability of admitting women than of admitting men?\n\n\nfor index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value \n 0:\n        print(department_labels[index])\n\n\n\n\nA\nB\nD\nF\n\n\n\nSomehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!\n\n\nTake-away message:\n We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!\n\n\nMore General Story for Conditioning\n\n\nJointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.\n\n\nWe just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "03 Conditioning for Random Variables"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#conditioning-for-random-variables",
            "text": "When we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.  When we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:   Second, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:   Notation:  The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.   In general:  Conditioning:  Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y) 0$, the  conditional probability  of event $X=x$ given event $Y=y$ has happened is   p_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.   For example,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.   Computational interpretation:  To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.",
            "title": "Conditioning for Random Variables"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#exercise-conditioning-for-random-variables",
            "text": "Consider the following two joint probability tables.   Question 1:  What is $p_{W|I}(\\text {sunny}|1)$?  p_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I   {0: 0.5, 1: 0.5}  # p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]  1.0  Question 2:  What is $p_{X|Y}(\\text {sunny}|1)$?  p_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y       {0: 0.5, 1: 0.5}  # p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]  0.5  Question 3:  What is $p_{I|W}(1|\\text {snowy})$?   # p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)  0  Question 4:  What is $p_{Y|X}(1|\\text {snowy})$?  # p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']  0.5",
            "title": "Exercise: Conditioning for Random Variables"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#exercise-simpsons-paradox",
            "text": "This problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.  We have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).  Thus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.  The joint probability table is provided in the file  simpsons_paradox_data.py . Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).  Now let's load in everything from  simpsons_paradox_data.py :  from simpsons_paradox_data import *  Please open up  simpsons_paradox_data.py  to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.  For example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:  joint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]  Some terminology:  In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.  Let's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).  Marginalization is easy to do with NumPy:  joint_prob_gender_admission = joint_prob_table.sum(axis=1)  On the right-hand side, .sum(axis=1)  says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable  joint_prob_gender_admission  stores a 2D joint probability table for random variables $G$ and $A$.  Now, for example, the probability that a woman applies and is admitted is given by:  joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]  Now let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:   \\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}   Let's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:  female_only = joint_prob_gender_admission[gender_mapping['female']]  Now this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!  prob_admission_given_female = female_only / np.sum(female_only)  This is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:  prob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)  from simpsons_paradox_data import *  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  joint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint( Probability of admitted female: {0:.5f} .format(prob_admitted_given_female))  Probability of admitted female: 0.30334  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  male_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint( Probability of admitted male: {0:.5f} .format(prob_admitted_given_male))  Probability of admitted male: 0.44520  So it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.  But before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:  admitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]  Notice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.  Thus, the conditional probability table of gender given admitted is:  prob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)  All right, now let's look at which departments are favoring men over women.  For the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:  female_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]  Now let's determine the probability of getting admitted given each gender and each department.",
            "title": "Exercise: Simpson's Paradox"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#department-a",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  # A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint( Probability of admitted female in department A: {0:.3f} .format(p_A_given_F_and_A))  Probability of admitted female in department A: 0.820",
            "title": "Department A:"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#general-approach",
            "text": "# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep  array([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]  0.8200000000000004  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]  0.62000000000000011",
            "title": "General Approach"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#department-b",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]  0.67999999999999705  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]  0.63000000000000034",
            "title": "Department B:"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#department-c",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]  0.34000000000000008  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]  0.37000000000000005",
            "title": "Department C"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#department-d",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]  0.34999999999999998  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department D:"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#department-e",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]  0.23999999999999955  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department E"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#department-f",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]  0.069999999999999701  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004  Question:  How many of these departments have a higher probability of admitting women than of admitting men?  for index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value   0:\n        print(department_labels[index])  A\nB\nD\nF  Somehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!  Take-away message:  We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!",
            "title": "Department F"
        },
        {
            "location": "/week02/03 Conditioning for Random Variables/#more-general-story-for-conditioning",
            "text": "Jointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.  We just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "More General Story for Conditioning"
        },
        {
            "location": "/week02/04 Conditioning on Events/",
            "text": "Conditioning on Events Intro\n\n\nGiven two events $\\mathcal{A}$ and $\\mathcal{B}$ (both of which have positive probability), the $\\mathbb {P}(\\mathcal{A} | \\mathcal{B})$ $i.e.$, the probability $\\mathcal{A}$ given $\\mathcal{B}$ is computed as\n\n\n\n\n\\mathbb {P}(\\mathcal{A} | \\mathcal{B}) = \\frac{\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})}{\\mathbb {P}(\\mathcal{B})}. \n\n\n\n\nImportant: We account for observations using conditioning. It turns out that often we can solve inference problems \nwithout\n using random variables at all and only using events. In this sequence on \u201cConditioning on Events\", to solve the problems presented, you should do them without using our machinery for random variables from earlier.\n\n\nOf course, later on in the course and even beyond the course, depending on the inference problem you're trying to solve, you may find it easier to use events and not random variables, or random variables and not events, or both random variables and events. But for now, let's make sure you can use events and not random variables!\n\n\nExercise: Conditioning on Events\n\n\nThe six possible outcomes of a fair die roll are all equally likely.\n\n\nQuestion:\n If we are told that the outcome of a roll is even, what is the probability that the outcome is $6$? (Please be precise with at least $3$ decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\ndice = {i+1: 1/6 for i in range(6)}\nA = {i for i in dice if i % 2 == 0}\nB = {6}\np_B_given_A = len(A \n B)/len(A) \np_B_given_A\n\n\n\n\n0.3333333333333333\n\n\n\nNow suppose we roll two fair six-sided dice. Let $\\mathcal{A}$ denote the event that the outcome of the roll of first die is an even number, and let $\\mathcal{B}$ denote the event that the outcome of the second die roll is $3$.\n\n\ndef prob_event(event, model):\n    \n\n    Gives the probability of event.\n\n    \n dice = {i+1: 1/6 for i in range(6)}\n    \n prob_event({1, 3, 6}, dice)\n    0.5\n\n    \n two_dice = {(i+1, j+1): 1/36 for i in range(6) for j in range(6)}\n    \n event = {(2, 3), (4, 3), (6, 3)}\n    \n prob_event(event, two_dice)\n    0.08333333333333333\n    \n\n    return sum([model[key] for key in event])\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()    \n\n\n\n\nQuestion:\n Determine $\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})$. To do this, first figure out what outcomes are contained in $\\mathcal{A} \\cap \\mathcal{B}$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nfrom fractions import Fraction\ntwo_dice = {(i+1, j+1): 1/36 for i in range(6) for j in range(6)}\nA = {key for key in two_dice if key[0] % 2 == 0}\nB = {key for key in two_dice if key[1] == 3}\nprint(Fraction(prob_event((A \n B), two_dice)).limit_denominator())\n\n\n\n\n1/12\n\n\n\nQuestion:\n Determine $\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})$. To do this, first figure out what outcomes are contained in $\\mathcal{A} \\cap \\mathcal{B}$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprint(Fraction(prob_event((A | B), two_dice)).limit_denominator())\n\n\n\n\n7/12\n\n\n\nQuestion:\n Determine $\\mathbb {P}(\\mathcal{A} | \\mathcal{B})$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprint(Fraction(prob_event(A \n B, two_dice)/ prob_event(B, two_dice)).limit_denominator())\n\n\n\n\n1/2\n\n\n\nExercise: Boy or Girl Paradox\n\n\nAlice has two children. Let's look at the probability that both children are girls, given different observations. We'll assume that the underlying finite probability space is as follows for Alice's children:\n\n\n\n\nQuestion:\n What is the probability that both children are girls? (This is an unconditional probability in that we aren't given any observations.) (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\njoint_prob = {(i, j): 1/4 for i in ['B', 'G'] for j in ['B', 'G']} \nboth_girl = {('G', 'G')}\nprob_event(both_girl, joint_prob)\n\n\n\n\n0.25\n\n\n\nQuestion:\n What is the probability that both children are girls given that the younger child is a girl? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nyounger_girl = {key for key in joint_prob if key[1] == 'G'}\nprob_event(both_girl \n younger_girl, joint_prob) / prob_event(younger_girl, joint_prob)\n\n\n\n\n0.5\n\n\n\nQuestion:\n What is the probability that both children are girls given that at least one child is a girl? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\none_girl = {key for key in joint_prob if 'G' in key}\nprob_event(both_girl \n one_girl, joint_prob) / prob_event(one_girl, joint_prob)\n\n\n\n\n0.3333333333333333",
            "title": "04 Conditioning on Events"
        },
        {
            "location": "/week02/04 Conditioning on Events/#conditioning-on-events-intro",
            "text": "Given two events $\\mathcal{A}$ and $\\mathcal{B}$ (both of which have positive probability), the $\\mathbb {P}(\\mathcal{A} | \\mathcal{B})$ $i.e.$, the probability $\\mathcal{A}$ given $\\mathcal{B}$ is computed as   \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) = \\frac{\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})}{\\mathbb {P}(\\mathcal{B})}.    Important: We account for observations using conditioning. It turns out that often we can solve inference problems  without  using random variables at all and only using events. In this sequence on \u201cConditioning on Events\", to solve the problems presented, you should do them without using our machinery for random variables from earlier.  Of course, later on in the course and even beyond the course, depending on the inference problem you're trying to solve, you may find it easier to use events and not random variables, or random variables and not events, or both random variables and events. But for now, let's make sure you can use events and not random variables!",
            "title": "Conditioning on Events Intro"
        },
        {
            "location": "/week02/04 Conditioning on Events/#exercise-conditioning-on-events",
            "text": "The six possible outcomes of a fair die roll are all equally likely.  Question:  If we are told that the outcome of a roll is even, what is the probability that the outcome is $6$? (Please be precise with at least $3$ decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  dice = {i+1: 1/6 for i in range(6)}\nA = {i for i in dice if i % 2 == 0}\nB = {6}\np_B_given_A = len(A   B)/len(A) \np_B_given_A  0.3333333333333333  Now suppose we roll two fair six-sided dice. Let $\\mathcal{A}$ denote the event that the outcome of the roll of first die is an even number, and let $\\mathcal{B}$ denote the event that the outcome of the second die roll is $3$.  def prob_event(event, model):\n     \n    Gives the probability of event.\n\n      dice = {i+1: 1/6 for i in range(6)}\n      prob_event({1, 3, 6}, dice)\n    0.5\n\n      two_dice = {(i+1, j+1): 1/36 for i in range(6) for j in range(6)}\n      event = {(2, 3), (4, 3), (6, 3)}\n      prob_event(event, two_dice)\n    0.08333333333333333\n     \n    return sum([model[key] for key in event])\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()      Question:  Determine $\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})$. To do this, first figure out what outcomes are contained in $\\mathcal{A} \\cap \\mathcal{B}$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  from fractions import Fraction\ntwo_dice = {(i+1, j+1): 1/36 for i in range(6) for j in range(6)}\nA = {key for key in two_dice if key[0] % 2 == 0}\nB = {key for key in two_dice if key[1] == 3}\nprint(Fraction(prob_event((A   B), two_dice)).limit_denominator())  1/12  Question:  Determine $\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})$. To do this, first figure out what outcomes are contained in $\\mathcal{A} \\cap \\mathcal{B}$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  print(Fraction(prob_event((A | B), two_dice)).limit_denominator())  7/12  Question:  Determine $\\mathbb {P}(\\mathcal{A} | \\mathcal{B})$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  print(Fraction(prob_event(A   B, two_dice)/ prob_event(B, two_dice)).limit_denominator())  1/2",
            "title": "Exercise: Conditioning on Events"
        },
        {
            "location": "/week02/04 Conditioning on Events/#exercise-boy-or-girl-paradox",
            "text": "Alice has two children. Let's look at the probability that both children are girls, given different observations. We'll assume that the underlying finite probability space is as follows for Alice's children:   Question:  What is the probability that both children are girls? (This is an unconditional probability in that we aren't given any observations.) (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  joint_prob = {(i, j): 1/4 for i in ['B', 'G'] for j in ['B', 'G']} \nboth_girl = {('G', 'G')}\nprob_event(both_girl, joint_prob)  0.25  Question:  What is the probability that both children are girls given that the younger child is a girl? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  younger_girl = {key for key in joint_prob if key[1] == 'G'}\nprob_event(both_girl   younger_girl, joint_prob) / prob_event(younger_girl, joint_prob)  0.5  Question:  What is the probability that both children are girls given that at least one child is a girl? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  one_girl = {key for key in joint_prob if 'G' in key}\nprob_event(both_girl   one_girl, joint_prob) / prob_event(one_girl, joint_prob)  0.3333333333333333",
            "title": "Exercise: Boy or Girl Paradox"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/",
            "text": "Bayes Theorem for Events\n\n\nGiven two events $\\mathcal{A}$ and $\\mathcal{B}$ (both of which have positive probability), Bayes' theorem, also called Bayes' rule or Bayes' law, gives a way to compute $\\mathbb {P}(\\mathcal{A} | \\mathcal{B})$ in terms of $\\mathbb {P}(\\mathcal{B} | \\mathcal{A})$. This result turns out to be extremely useful for inference because often times we want to compute one of these, and the other is known or otherwise straightforward to compute.\n\n\nBayes' theorem is given by\n\n\n\n\n\\mathbb {P}(\\mathcal{A} | \\mathcal{B}) = \\frac{\\mathbb {P}(\\mathcal{B} | \\mathcal{A}) \\mathbb {P}(\\mathcal{A})}{\\mathbb {P}(\\mathcal{B})}.\n\n\n\n\nThe proof of why this is the case is a one liner:\n\n\n\n\n\\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \\overset {(a)}{=} \\frac{\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})}{\\mathbb {P}(\\mathcal{B})} \\overset {(b)}{=} \\frac{\\mathbb {P}(\\mathcal{B} | \\mathcal{A}) \\mathbb {P}(\\mathcal{A})}{\\mathbb {P}(\\mathcal{B})},\n\n\n\n\nwhere step (a) is by the definition of conditional probability for events, and step (b) is due to the product rule for events (which follows from rearranging the definition of conditional probability for $\\mathbb {P}(\\mathcal{B} | \\mathcal{A})$.\n\n\nThe Law of Total Probability\n\n\nIf $B_1, \\ldots, B_n$ are partition of sample space $\\Omega$, Then \n\n\n\n\n \\mathbb{P}(A) = \\sum_{i=1}^n \\mathbb{P}(A \\cap B_i)\n\n\n\n\nExercise: Bayes' Theorem and Total Probability\n\n\nA new test is developed to determine whether a patient has an antibiotic-resistant bacterial infection. The test is correct $99\\%$ of the time: that is, if a patient has the infection, there is a $99\\%$ chance the test will return positive, and if a patient doesn't have the infection, there is a $99\\%$ chance the test will return negative. Suppose $0.1\\%$ of the general population has this infection. If a randomly selected person is administered this test and gets a positive result, what is the probability that she or he actually has the infection?\n\n\nTo solve this problem, let $\\mathcal{T}$ be the event that the patient has a positive test result, and $\\mathcal{S}$ be the event that the patient has the associated bacterial infection. Thus, what the problem is asking for is precisely the quantity $\\mathbb {P}(\\mathcal{S} | \\mathcal{T})$. So somehow we have to figure out what this probability is.\n\n\nWhat do we have access to? Well, from the problem statement, we are directly given the following quantities. What are they? Please provide the exact answer for these three quantities.\n\n\n\n\n\n\n$\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) = 0.99$\n\n\n\n\n\n\n$\\mathbb {P}(\\mathcal{T}^ c | \\mathcal{S}^ c) =0.99$\n\n\n\n\n\n\n$\\mathbb {P}(\\mathcal{S}) = 0.001$ \n\n\n\n\n\n\nLet's see if Bayes' rule can help us:\n\n\n\n\n\\mathbb {P}(\\mathcal{S} | \\mathcal{T}) = \\frac{\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) \\mathbb {P}(\\mathcal{S})}{\\mathbb {P}(\\mathcal{T})}.\n\n\n\n\nThere's a slight problem. We don't know $\\mathbb{P}(\\mathcal{T})$, i.e., the probability that the patient has a positive test result. But let's break this into two cases. The patient either has the infection or not. So\n\n\n\n\n\\begin{eqnarray}\n\\mathbb{P}(\\mathcal{T})\n&=&\n\\mathbb{P}(\\text{patient has positive test result}) \\\\\n&=&\n\\mathbb{P}(\\text{patient has positive test result and patient has infection}) \\\\\n&&+\n\\mathbb{P}(\\text{patient has positive test result and patient does not have infection}) \\\\\n&=&\n\\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S})\n+ \\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S}^c).\n\\end{eqnarray}\n\n\n\n\nQuestion: What result did we just use?\n\n\n[$\\times    $] Bayes' Theorem\n\n[$\\times    $] Product Rule\n\n[$\\checkmark$] Law of Total Probability\n\n[$\\times    $] Marginalization\n\n\nWe can go one step further:\n\n\n\n\n\\begin{eqnarray}\n\\mathbb{P}(\\mathcal{T})\n&=&\n\\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S})\n  + \\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S}^c) \\\\\n&=&\n\\mathbb{P}(\\mathcal{T} | \\mathcal{S})\\mathbb{P}(\\mathcal{S})\n  + \\mathbb{P}(\\mathcal{T} | \\mathcal{S}^c)\\mathbb{P}(\\mathcal{S}^c).\n\\end{eqnarray}\n\n\n\n\nQuestion: In the last equality, what result did we use?\n\n\n[$\\times    $] Bayes' Theorem\n\n[$\\checkmark$] Product Rule\n\n[$\\times    $] Law of Total Probability\n\n[$\\times    $] Marginalization\n\n\nSo what we have is:\n\n\n\n\n\\mathbb {P}(\\mathcal{S} | \\mathcal{T}) = \\frac{\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) \\mathbb {P}(\\mathcal{S})}{\\mathbb {P}(\\mathcal{T})} = \\frac{\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) \\mathbb {P}(\\mathcal{S})}{\\mathbb {P}(\\mathcal{T} | \\mathcal{S})\\mathbb {P}(\\mathcal{S}) + \\mathbb {P}(\\mathcal{T} | \\mathcal{S}^ c)\\mathbb {P}(\\mathcal{S}^ c)}.\n\n\n\n\nThere are just two probabilities that we haven't figured out. But we can from the information we are given! Determine what the following are. Provide exact answers for these.\n\n\n\n\n\n\n$\\mathbb {P}(\\mathcal{S}^ c) = 0.999$ \n\n\n\n\n\n\n$\\mathbb {P}(\\mathcal{T} | \\mathcal{S}^ c) = 0.01$\n\n\n\n\n\n\nNow we can plug in all the probabilities into our very last big equation above!\n\n\nWhat is $\\mathbb {P}(\\mathcal{S} | \\mathcal{T})$, the probability that given a randomly selected person has a positive test result, she or he has the infection? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(0.99 * 0.001) / (0.99 * 0.001 + 0.01 * 0.999) \n\n\n\n\n0.09016393442622951\n\n\n\nDoes the probability seem low given how accurate the test is? Be sure to check out the solutions for a detailed explanation.\n\n\nPractice Problem: Bayes' Theorem and Total Probability\n\n\nThis problem gives practice for using Bayes' Theorem and the Law of Total Probability. The solution is provided on the next page. Please try to solve it before looking at the solution. Since this is a practice problem, you do not have to submit anything.\n\n\nYour problem set is due in 15 minutes! It's in one of your drawers, but they are messy, and you're not sure which one it's in.\n\n\nThe probability that the problem set is in drawer $k$ is $d_k$. If drawer $k$ has the problem set and you search there, you have probability $p_k$ of finding it. There are a total of $m$ drawers.\n\n\nSuppose you search drawer $i$ and do not find the problem set.\n\n\n(a) Find the probability that the paper is in drawer $j$, where $j\u2260i$.\n\n\n(b) Find the probability that the paper is in drawer $i$.\n\n\nSolution:\n Let $A_k$ be the event that the problem set is in drawer $k$, and $B_k$ be the event that you find the problem set in drawer $k$.\n\n\n(a) We'll express the desired probability as $\\mathbb {P}(A_ j|B_ i^ c)$. Since this quantity is difficult to reason about directly, we'll use Bayes' rule:\n\n\n\n\n\\mathbb {P}(A_ j|B_ i^ c) = \\frac{\\mathbb {P}(B_ i^ c | A_ j) \\mathbb {P}(A_ j)}{\\mathbb {P}(B_ i^ c)}\n\n\n\n\nThe first probability, $\\mathbb {P}(B_ i^ c | A_ j)$, expresses the probability of not finding the problem set in drawer $i$ given that it's in a different drawer j. Since it's impossible to find the paper in a drawer it isn't in, this is just 1.\n\n\nThe second quantity, $\\mathbb{P}(A_j)$, is given to us in the problem statement as $d_j$.\n\n\nThe third probability, $\\mathbb {P}(B_ i^ c) = 1-\\mathbb {P}(B_ i)$, is difficult to reason about directly. But, if we knew whether or not the paper was in the drawer, it would become easier. So, we'll use total probability:\n\n\n\n\n\\begin{eqnarray}\n            \\mathbb{P}(B_i) &=& \\mathbb{P}(B_i|A_i)\\mathbb{P}(A_i) + \\mathbb{P}(B_i|A_i^c)\\mathbb{P}(A_i^c) \\\\\n            &=& p_i d_i + 0 (1-d_i)\n\\end{eqnarray}\n\n\n\n\nPutting these terms together, we find that\n\n\n\n\n\\mathbb {P}(A_ j|B_ i^ c) = \\frac{d_ j}{1-p_ i d_ i}\n\n\n\n\nAlternate method to compute the denominator $\\mathbb{P}(B_i^c)$:\n We could use the law of total probability to decompose $\\mathbb{P}(B_i^c)$ depending on which drawer the homework is actually in. We have\n\n\n\n\n\\begin{eqnarray}\n\\mathbb{P}(B_i^c) &=& \\sum_{k=1}^m\n               \\underbrace{\\mathbb{P}(A_k)}_{d_k}\n               \\underbrace{\\mathbb{P}(B_i^c|A_k)}_{\\substack{1\\text{ if }k\\ne i,\\\\\n                                                     (1-p_i)\\text{ if }k=i}} \\\\\n          &=& \\sum_{\\substack{k=1,\\\\\n                             k\\ne i}}^m d_k\n             + (1-p_i)d_i \\\\\n          &=& \\sum_{k=1}^m d_k - p_i d_i \\\\\n          &=& 1 - p_i d_i.\n\\end{eqnarray}\n\n\n\n\n(b) Similarly, we'll use Bayes' rule:\n\n\n\n\n\\mathbb{P}(A_i | B_i^c) = \\frac{\\mathbb{P}(B_i^c | A_i) \\mathbb {P}(A_i)}{\\mathbb {P}(B_i^c)} = \\frac{(1-p_i) d_i}{1 - p_i d_i}\n\n\n\n\nRelating Conditioning on Events Back to Random Variables\n\n\nWe have alrady defined conditional probability in terms of events as follows\n\n\n\n\n \\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)}\n\n\n\n\nLet $A = {X = x}$ and $B= {Y=y}$ are two events in terms of probability distribution. Then \n\n\n\n\n\\mathbb{P}(Y=y|X=x) = \\frac{\\mathbb{P}(X=x,Y=y)}{\\mathbb{P}(X=x)} = \\frac{p_{X,Y}(x, y)}{p_X(x)}\n\n\n\n\nIf we fix the value of $x$ and change the value of $y$ then we get a new distribution called conditional distribution of $Y$ given $X$ as follows\n\n\n\n\np_{Y\\mid X}(y\\mid x) = \\sum_y p_{Y\\mid X}(y\\mid x) = 1\n\n\n\n\nRandom Variables Conditioned on Events\n\n\nLet $Y$ is a random variable takes values $1, 2, 3$ and $4$. The the conditional distribution of $Y$ given $A$ is given by \n\n\n\n\n \\mathbb{P}_{Y|A}(y) = \\frac{\\mathbb{P}(Y=y, A)}{\\mathbb{P}(A)}",
            "title": "05 Bayes Theorem for Events"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/#bayes-theorem-for-events",
            "text": "Given two events $\\mathcal{A}$ and $\\mathcal{B}$ (both of which have positive probability), Bayes' theorem, also called Bayes' rule or Bayes' law, gives a way to compute $\\mathbb {P}(\\mathcal{A} | \\mathcal{B})$ in terms of $\\mathbb {P}(\\mathcal{B} | \\mathcal{A})$. This result turns out to be extremely useful for inference because often times we want to compute one of these, and the other is known or otherwise straightforward to compute.  Bayes' theorem is given by   \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) = \\frac{\\mathbb {P}(\\mathcal{B} | \\mathcal{A}) \\mathbb {P}(\\mathcal{A})}{\\mathbb {P}(\\mathcal{B})}.   The proof of why this is the case is a one liner:   \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \\overset {(a)}{=} \\frac{\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B})}{\\mathbb {P}(\\mathcal{B})} \\overset {(b)}{=} \\frac{\\mathbb {P}(\\mathcal{B} | \\mathcal{A}) \\mathbb {P}(\\mathcal{A})}{\\mathbb {P}(\\mathcal{B})},   where step (a) is by the definition of conditional probability for events, and step (b) is due to the product rule for events (which follows from rearranging the definition of conditional probability for $\\mathbb {P}(\\mathcal{B} | \\mathcal{A})$.",
            "title": "Bayes Theorem for Events"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/#the-law-of-total-probability",
            "text": "If $B_1, \\ldots, B_n$ are partition of sample space $\\Omega$, Then     \\mathbb{P}(A) = \\sum_{i=1}^n \\mathbb{P}(A \\cap B_i)",
            "title": "The Law of Total Probability"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/#exercise-bayes-theorem-and-total-probability",
            "text": "A new test is developed to determine whether a patient has an antibiotic-resistant bacterial infection. The test is correct $99\\%$ of the time: that is, if a patient has the infection, there is a $99\\%$ chance the test will return positive, and if a patient doesn't have the infection, there is a $99\\%$ chance the test will return negative. Suppose $0.1\\%$ of the general population has this infection. If a randomly selected person is administered this test and gets a positive result, what is the probability that she or he actually has the infection?  To solve this problem, let $\\mathcal{T}$ be the event that the patient has a positive test result, and $\\mathcal{S}$ be the event that the patient has the associated bacterial infection. Thus, what the problem is asking for is precisely the quantity $\\mathbb {P}(\\mathcal{S} | \\mathcal{T})$. So somehow we have to figure out what this probability is.  What do we have access to? Well, from the problem statement, we are directly given the following quantities. What are they? Please provide the exact answer for these three quantities.    $\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) = 0.99$    $\\mathbb {P}(\\mathcal{T}^ c | \\mathcal{S}^ c) =0.99$    $\\mathbb {P}(\\mathcal{S}) = 0.001$     Let's see if Bayes' rule can help us:   \\mathbb {P}(\\mathcal{S} | \\mathcal{T}) = \\frac{\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) \\mathbb {P}(\\mathcal{S})}{\\mathbb {P}(\\mathcal{T})}.   There's a slight problem. We don't know $\\mathbb{P}(\\mathcal{T})$, i.e., the probability that the patient has a positive test result. But let's break this into two cases. The patient either has the infection or not. So   \\begin{eqnarray}\n\\mathbb{P}(\\mathcal{T})\n&=&\n\\mathbb{P}(\\text{patient has positive test result}) \\\\\n&=&\n\\mathbb{P}(\\text{patient has positive test result and patient has infection}) \\\\\n&&+\n\\mathbb{P}(\\text{patient has positive test result and patient does not have infection}) \\\\\n&=&\n\\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S})\n+ \\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S}^c).\n\\end{eqnarray}   Question: What result did we just use?  [$\\times    $] Bayes' Theorem \n[$\\times    $] Product Rule \n[$\\checkmark$] Law of Total Probability \n[$\\times    $] Marginalization  We can go one step further:   \\begin{eqnarray}\n\\mathbb{P}(\\mathcal{T})\n&=&\n\\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S})\n  + \\mathbb{P}(\\mathcal{T} \\cap \\mathcal{S}^c) \\\\\n&=&\n\\mathbb{P}(\\mathcal{T} | \\mathcal{S})\\mathbb{P}(\\mathcal{S})\n  + \\mathbb{P}(\\mathcal{T} | \\mathcal{S}^c)\\mathbb{P}(\\mathcal{S}^c).\n\\end{eqnarray}   Question: In the last equality, what result did we use?  [$\\times    $] Bayes' Theorem \n[$\\checkmark$] Product Rule \n[$\\times    $] Law of Total Probability \n[$\\times    $] Marginalization  So what we have is:   \\mathbb {P}(\\mathcal{S} | \\mathcal{T}) = \\frac{\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) \\mathbb {P}(\\mathcal{S})}{\\mathbb {P}(\\mathcal{T})} = \\frac{\\mathbb {P}(\\mathcal{T} | \\mathcal{S}) \\mathbb {P}(\\mathcal{S})}{\\mathbb {P}(\\mathcal{T} | \\mathcal{S})\\mathbb {P}(\\mathcal{S}) + \\mathbb {P}(\\mathcal{T} | \\mathcal{S}^ c)\\mathbb {P}(\\mathcal{S}^ c)}.   There are just two probabilities that we haven't figured out. But we can from the information we are given! Determine what the following are. Provide exact answers for these.    $\\mathbb {P}(\\mathcal{S}^ c) = 0.999$     $\\mathbb {P}(\\mathcal{T} | \\mathcal{S}^ c) = 0.01$    Now we can plug in all the probabilities into our very last big equation above!  What is $\\mathbb {P}(\\mathcal{S} | \\mathcal{T})$, the probability that given a randomly selected person has a positive test result, she or he has the infection? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (0.99 * 0.001) / (0.99 * 0.001 + 0.01 * 0.999)   0.09016393442622951  Does the probability seem low given how accurate the test is? Be sure to check out the solutions for a detailed explanation.",
            "title": "Exercise: Bayes' Theorem and Total Probability"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/#practice-problem-bayes-theorem-and-total-probability",
            "text": "This problem gives practice for using Bayes' Theorem and the Law of Total Probability. The solution is provided on the next page. Please try to solve it before looking at the solution. Since this is a practice problem, you do not have to submit anything.  Your problem set is due in 15 minutes! It's in one of your drawers, but they are messy, and you're not sure which one it's in.  The probability that the problem set is in drawer $k$ is $d_k$. If drawer $k$ has the problem set and you search there, you have probability $p_k$ of finding it. There are a total of $m$ drawers.  Suppose you search drawer $i$ and do not find the problem set.  (a) Find the probability that the paper is in drawer $j$, where $j\u2260i$.  (b) Find the probability that the paper is in drawer $i$.  Solution:  Let $A_k$ be the event that the problem set is in drawer $k$, and $B_k$ be the event that you find the problem set in drawer $k$.  (a) We'll express the desired probability as $\\mathbb {P}(A_ j|B_ i^ c)$. Since this quantity is difficult to reason about directly, we'll use Bayes' rule:   \\mathbb {P}(A_ j|B_ i^ c) = \\frac{\\mathbb {P}(B_ i^ c | A_ j) \\mathbb {P}(A_ j)}{\\mathbb {P}(B_ i^ c)}   The first probability, $\\mathbb {P}(B_ i^ c | A_ j)$, expresses the probability of not finding the problem set in drawer $i$ given that it's in a different drawer j. Since it's impossible to find the paper in a drawer it isn't in, this is just 1.  The second quantity, $\\mathbb{P}(A_j)$, is given to us in the problem statement as $d_j$.  The third probability, $\\mathbb {P}(B_ i^ c) = 1-\\mathbb {P}(B_ i)$, is difficult to reason about directly. But, if we knew whether or not the paper was in the drawer, it would become easier. So, we'll use total probability:   \\begin{eqnarray}\n            \\mathbb{P}(B_i) &=& \\mathbb{P}(B_i|A_i)\\mathbb{P}(A_i) + \\mathbb{P}(B_i|A_i^c)\\mathbb{P}(A_i^c) \\\\\n            &=& p_i d_i + 0 (1-d_i)\n\\end{eqnarray}   Putting these terms together, we find that   \\mathbb {P}(A_ j|B_ i^ c) = \\frac{d_ j}{1-p_ i d_ i}   Alternate method to compute the denominator $\\mathbb{P}(B_i^c)$:  We could use the law of total probability to decompose $\\mathbb{P}(B_i^c)$ depending on which drawer the homework is actually in. We have   \\begin{eqnarray}\n\\mathbb{P}(B_i^c) &=& \\sum_{k=1}^m\n               \\underbrace{\\mathbb{P}(A_k)}_{d_k}\n               \\underbrace{\\mathbb{P}(B_i^c|A_k)}_{\\substack{1\\text{ if }k\\ne i,\\\\\n                                                     (1-p_i)\\text{ if }k=i}} \\\\\n          &=& \\sum_{\\substack{k=1,\\\\\n                             k\\ne i}}^m d_k\n             + (1-p_i)d_i \\\\\n          &=& \\sum_{k=1}^m d_k - p_i d_i \\\\\n          &=& 1 - p_i d_i.\n\\end{eqnarray}   (b) Similarly, we'll use Bayes' rule:   \\mathbb{P}(A_i | B_i^c) = \\frac{\\mathbb{P}(B_i^c | A_i) \\mathbb {P}(A_i)}{\\mathbb {P}(B_i^c)} = \\frac{(1-p_i) d_i}{1 - p_i d_i}",
            "title": "Practice Problem: Bayes' Theorem and Total Probability"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/#relating-conditioning-on-events-back-to-random-variables",
            "text": "We have alrady defined conditional probability in terms of events as follows    \\mathbb{P}(B|A) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(A)}   Let $A = {X = x}$ and $B= {Y=y}$ are two events in terms of probability distribution. Then    \\mathbb{P}(Y=y|X=x) = \\frac{\\mathbb{P}(X=x,Y=y)}{\\mathbb{P}(X=x)} = \\frac{p_{X,Y}(x, y)}{p_X(x)}   If we fix the value of $x$ and change the value of $y$ then we get a new distribution called conditional distribution of $Y$ given $X$ as follows   p_{Y\\mid X}(y\\mid x) = \\sum_y p_{Y\\mid X}(y\\mid x) = 1",
            "title": "Relating Conditioning on Events Back to Random Variables"
        },
        {
            "location": "/week02/05 Bayes Theorem for Events/#random-variables-conditioned-on-events",
            "text": "Let $Y$ is a random variable takes values $1, 2, 3$ and $4$. The the conditional distribution of $Y$ given $A$ is given by     \\mathbb{P}_{Y|A}(y) = \\frac{\\mathbb{P}(Y=y, A)}{\\mathbb{P}(A)}",
            "title": "Random Variables Conditioned on Events"
        },
        {
            "location": "/week02/06 Homework/",
            "text": "Homework Problem: Alice Hunts Dragons\n\n\nWhen she is not calculating marginal distributions, Alice spends her time hunting dragons. For every dragon she encounters, Alice measures its fire power $X$ (measured on a scale from $1$ to $4$) and its roar volume $Y$ (measured on a scale from $1$ to $3$). She notices that the proportion of dragons with certain fire power and roar volume in the population behaves as the following function:\n\n\n\n\n\\begin{eqnarray}\nf(x,y) = \\begin{cases} x^2+y^2 &\\text{if } x \\in \\{1,2,4\\} \\text{ and } y \\in \\{1,3\\} \\\\\n0 &\\text{otherwise}. \\end{cases}\n\\end{eqnarray}\n\n\n\n\nIn other words, the joint probability table $p_{X,Y}$ is of the form\n\n\n\n\np_{X,Y}(x,y) = c f(x, y) \\qquad \\text {for }x \\in \\{ 1, 2, 3, 4\\} , y \\in \\{ 1, 2, 3\\} ,\n\n\n\n\nfor some constant $c\n0$ that you will determine.\n\n\nQuestion:\n Determine the constant $c$, which ensures that $p_{X,Y}$ is a valid probability distribution. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n\u03a9 = {(i, j) for i in range(1, 5) for j in range(1, 4)}\nfrom fractions import Fraction\njoint_X_Y = {(i, j): (i**2 + j**2) for i in {1, 2, 4} for j in {1, 3}}\nc = 1/sum(joint_X_Y.values())\nprint(Fraction(c).limit_denominator())\n\n\n\n\n1/72\n\n\n\nQuestion:\n Determine $P(Y\nX)$. (Note that ${Y\nX}$ is an event. Think about what outcomes are in it.)\n\n\n(Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nY_lt_X = {x for x in joint_X_Y if x[1] \n x[0]}\nprob_Y_lt_X = sum([joint_X_Y[x] for x in Y_lt_X]) * c\nprint(Fraction(prob_Y_lt_X).limit_denominator())\n\n\n\n\n47/72\n\n\n\nQuestion:\n Determine $P(X\nY)$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nX_lt_Y = {x for x in joint_X_Y if x[1] \n x[0]}\nprob_X_lt_Y = sum([joint_X_Y[x] for x in X_lt_Y]) * c\nprint(Fraction(prob_X_lt_Y).limit_denominator())\n\n\n\n\n23/72\n\n\n\nQuestion:\n Determine $P(Y=X)$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nX_eq_Y = {x for x in joint_X_Y if x[1] == x[0]}\nprob_X_eq_Y = sum([joint_X_Y[x] for x in X_eq_Y]) * c\nprint(Fraction(prob_X_eq_Y).limit_denominator())\n\n\n\n\n1/36\n\n\n\nQuestion:\n Determine $P(Y=3)$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nY_eq_3 = {x for x in joint_X_Y if x[1] == 3}\nprob_Y_eq_3 = sum([joint_X_Y[x] for x in Y_eq_3]) * c\nprint(Fraction(prob_Y_eq_3).limit_denominator())\n\n\n\n\n2/3\n\n\n\nQuestion:\n Find the probability tables for $p_X$ and $p_Y$. Express your answers as Python dictionaries. (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least 5 decimal places.)\n\n\n$p_X$ probability table (the dictionary keys should be the Python integers 1, 2, 3, 4): \n\n\nprob_X = {i: 0 for i in range(1, 5)} # initialize the dictionary with 0 \nfor key, values in joint_X_Y.items():\n    if key[0] in prob_X:\n        prob_X[key[0]] += values * c\n\nprob_X        \n\n\n\n\n{1: 0.16666666666666669, 2: 0.25, 3: 0, 4: 0.5833333333333333}\n\n\n\n$p_Y$ probability table (the dictionary keys should be the Python integers 1, 2, 3): \n\n\nprob_Y = {i: 0 for i in range(1, 4)} # initialize the dictionary with 0 \nfor key, values in joint_X_Y.items():\n    if key[1] in prob_Y:\n        prob_Y[key[1]] += values * c\n\nprob_Y\n\n\n\n\n{1: 0.33333333333333337, 2: 0, 3: 0.6666666666666666}\n\n\n\nHomework Problem: Alice's Coins\n\n\nAlice has five coins in a bag: two coins are normal, two are double-headed, and the last one is double-tailed. She reaches into the bag and randomly pulls out a coin. Without looking at the coin she drew, she tosses it.\n\n\nQuestion:\n What is the probability that once the coin lands, the side of the coin that is face-down is heads? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nAnswer:\n Let $X$ is the random variable for selecting the coin. Let $X = {F, H, T}$ takes values $F$ for fair coin, $H$ for double-headed coin and $T$ for double-tailed coin. Also $Y = {h, t}$ is the random variable for getting $h$ for face down head and $t$ for face down tail. Then the joint probability distribution is given by \n\n\n\n\n\n\n\n\n\n\nY=h\n\n\nY=t\n\n\nX\nmarginal\n\n\n\n\n\n\n\n\n\n\nX=F\n\n\n1/5\n\n\n1/5\n\n\n2/5\n\n\n\n\n\n\nX=H\n\n\n2/5\n\n\n0\n\n\n2/5\n\n\n\n\n\n\nX=T\n\n\n0\n\n\n1/5\n\n\n1/5\n\n\n\n\n\n\nY\nmarginal\n\n\n3/5\n\n\n2/5\n\n\n\n\n\n\n\n\n\n\nThe probability of getting face down head is given by marginal probibility $\\mathbb{P}(Y=h) = 3/5$.\n\n\nQuestion:\n The coin lands and shows heads face-up. What is the probability that the face-down side is heads? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nAnswer:\n This question asking about conditional probability \n\n\\mathbb{P}(X=H|Y=h) = \\frac{\\mathbb{P}(X=H, Y=h)}{\\mathbb{P}(Y=h) } = \\frac{2/5}{3/5} = \\frac{2}{3} \n\n\n\n\nAlice discards the first coin (the one from part (b) that landed and showed heads face-up), reaches again into the bag and draws out a random coin. Again, without looking at it, she tosses it.\n\n\nQuestion:\n What is the probability that the coin shows heads face-up? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nAnswer:\n Let $Z$ is the random varable for getting second coin face-up heads or tails. Also note that the first coin with face-up heads is discarded. There are two possibiliy of getting heads on face-up if $X=F$ or $X=H$. Let $W$ is the random variable for withdrawing second coin. \n\n\nThe joint probability distribution of $W$ and $Z$ given $X=F$, $i.e.$, $P(W,Z|X=F)$\n\n\n\n\n\n\n\n\n\n\nZ=h\n\n\nZ=t\n\n\nW\nmarginal\n\n\n\n\n\n\n\n\n\n\nW=F\n\n\n1/8\n\n\n1/8\n\n\n1/4\n\n\n\n\n\n\nW=H\n\n\n1/2\n\n\n0\n\n\n1/2\n\n\n\n\n\n\nW=T\n\n\n0\n\n\n1/4\n\n\n1/4\n\n\n\n\n\n\nZ\nmarginal\n\n\n5/8\n\n\n3/8\n\n\n\n\n\n\n\n\n\n\nThe joint probability distribution of $W$ and $Z$ given $X=H$, $i.e.$, $P(W,Z|X=H)$\n\n\n\n\n\n\n\n\n\n\nZ=h\n\n\nZ=t\n\n\nW\nmarginal\n\n\n\n\n\n\n\n\n\n\nW=F\n\n\n1/4\n\n\n1/4\n\n\n1/2\n\n\n\n\n\n\nW=H\n\n\n1/4\n\n\n0\n\n\n1/4\n\n\n\n\n\n\nW=T\n\n\n0\n\n\n1/4\n\n\n1/4\n\n\n\n\n\n\nZ\nmarginal\n\n\n1/2\n\n\n1/2\n\n\n\n\n\n\n\n\n\n\nHence the probability of getting heads in face-up is given by\n\n\n\n\n\n\\begin{align}\n\\mathbb{P}(Z=h|Y=h) \n&= \\mathbb{P}(Z=h|X=F) \\times \\mathbb{P}(X=F|Y=h) + \\mathbb{P}(Z=h|X=H) \\times \\mathbb{P}(X=H|Y=h) \\\\\n&= \\mathbb{P}(Z=h|X=F) \\times \\frac{\\mathbb{P}(X=F, Y=h)}{ \\mathbb{P}(Y=h)} + \\mathbb{P}(Z=h|X=H) \\times \\frac{\\mathbb{P}(X=H, Y=h)}{ \\mathbb{P}(Y=h)} \\\\\n&= \\frac{5}{8} \\times \\frac{1/5}{3/5} + \\frac{1}{2} \\times \\frac{2/5}{3/5}\\\\\n&= \\frac{13}{24}\n\\end{align}",
            "title": "06 Homework"
        },
        {
            "location": "/week02/06 Homework/#homework-problem-alice-hunts-dragons",
            "text": "When she is not calculating marginal distributions, Alice spends her time hunting dragons. For every dragon she encounters, Alice measures its fire power $X$ (measured on a scale from $1$ to $4$) and its roar volume $Y$ (measured on a scale from $1$ to $3$). She notices that the proportion of dragons with certain fire power and roar volume in the population behaves as the following function:   \\begin{eqnarray}\nf(x,y) = \\begin{cases} x^2+y^2 &\\text{if } x \\in \\{1,2,4\\} \\text{ and } y \\in \\{1,3\\} \\\\\n0 &\\text{otherwise}. \\end{cases}\n\\end{eqnarray}   In other words, the joint probability table $p_{X,Y}$ is of the form   p_{X,Y}(x,y) = c f(x, y) \\qquad \\text {for }x \\in \\{ 1, 2, 3, 4\\} , y \\in \\{ 1, 2, 3\\} ,   for some constant $c 0$ that you will determine.  Question:  Determine the constant $c$, which ensures that $p_{X,Y}$ is a valid probability distribution. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  \u03a9 = {(i, j) for i in range(1, 5) for j in range(1, 4)}\nfrom fractions import Fraction\njoint_X_Y = {(i, j): (i**2 + j**2) for i in {1, 2, 4} for j in {1, 3}}\nc = 1/sum(joint_X_Y.values())\nprint(Fraction(c).limit_denominator())  1/72  Question:  Determine $P(Y X)$. (Note that ${Y X}$ is an event. Think about what outcomes are in it.)  (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  Y_lt_X = {x for x in joint_X_Y if x[1]   x[0]}\nprob_Y_lt_X = sum([joint_X_Y[x] for x in Y_lt_X]) * c\nprint(Fraction(prob_Y_lt_X).limit_denominator())  47/72  Question:  Determine $P(X Y)$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  X_lt_Y = {x for x in joint_X_Y if x[1]   x[0]}\nprob_X_lt_Y = sum([joint_X_Y[x] for x in X_lt_Y]) * c\nprint(Fraction(prob_X_lt_Y).limit_denominator())  23/72  Question:  Determine $P(Y=X)$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  X_eq_Y = {x for x in joint_X_Y if x[1] == x[0]}\nprob_X_eq_Y = sum([joint_X_Y[x] for x in X_eq_Y]) * c\nprint(Fraction(prob_X_eq_Y).limit_denominator())  1/36  Question:  Determine $P(Y=3)$. (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  Y_eq_3 = {x for x in joint_X_Y if x[1] == 3}\nprob_Y_eq_3 = sum([joint_X_Y[x] for x in Y_eq_3]) * c\nprint(Fraction(prob_Y_eq_3).limit_denominator())  2/3  Question:  Find the probability tables for $p_X$ and $p_Y$. Express your answers as Python dictionaries. (Your answer should be the Python dictionary itself, and not the dictionary assigned to a variable, so please do not include, for instance, \u201cprob_table =\" before specifying your answer. You can use fractions. If you use decimals instead, please be accurate and use at least 5 decimal places.)  $p_X$ probability table (the dictionary keys should be the Python integers 1, 2, 3, 4):   prob_X = {i: 0 for i in range(1, 5)} # initialize the dictionary with 0 \nfor key, values in joint_X_Y.items():\n    if key[0] in prob_X:\n        prob_X[key[0]] += values * c\n\nprob_X          {1: 0.16666666666666669, 2: 0.25, 3: 0, 4: 0.5833333333333333}  $p_Y$ probability table (the dictionary keys should be the Python integers 1, 2, 3):   prob_Y = {i: 0 for i in range(1, 4)} # initialize the dictionary with 0 \nfor key, values in joint_X_Y.items():\n    if key[1] in prob_Y:\n        prob_Y[key[1]] += values * c\n\nprob_Y  {1: 0.33333333333333337, 2: 0, 3: 0.6666666666666666}",
            "title": "Homework Problem: Alice Hunts Dragons"
        },
        {
            "location": "/week02/06 Homework/#homework-problem-alices-coins",
            "text": "Alice has five coins in a bag: two coins are normal, two are double-headed, and the last one is double-tailed. She reaches into the bag and randomly pulls out a coin. Without looking at the coin she drew, she tosses it.  Question:  What is the probability that once the coin lands, the side of the coin that is face-down is heads? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  Answer:  Let $X$ is the random variable for selecting the coin. Let $X = {F, H, T}$ takes values $F$ for fair coin, $H$ for double-headed coin and $T$ for double-tailed coin. Also $Y = {h, t}$ is the random variable for getting $h$ for face down head and $t$ for face down tail. Then the joint probability distribution is given by       Y=h  Y=t  X marginal      X=F  1/5  1/5  2/5    X=H  2/5  0  2/5    X=T  0  1/5  1/5    Y marginal  3/5  2/5      The probability of getting face down head is given by marginal probibility $\\mathbb{P}(Y=h) = 3/5$.  Question:  The coin lands and shows heads face-up. What is the probability that the face-down side is heads? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  Answer:  This question asking about conditional probability  \\mathbb{P}(X=H|Y=h) = \\frac{\\mathbb{P}(X=H, Y=h)}{\\mathbb{P}(Y=h) } = \\frac{2/5}{3/5} = \\frac{2}{3}    Alice discards the first coin (the one from part (b) that landed and showed heads face-up), reaches again into the bag and draws out a random coin. Again, without looking at it, she tosses it.  Question:  What is the probability that the coin shows heads face-up? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  Answer:  Let $Z$ is the random varable for getting second coin face-up heads or tails. Also note that the first coin with face-up heads is discarded. There are two possibiliy of getting heads on face-up if $X=F$ or $X=H$. Let $W$ is the random variable for withdrawing second coin.   The joint probability distribution of $W$ and $Z$ given $X=F$, $i.e.$, $P(W,Z|X=F)$      Z=h  Z=t  W marginal      W=F  1/8  1/8  1/4    W=H  1/2  0  1/2    W=T  0  1/4  1/4    Z marginal  5/8  3/8      The joint probability distribution of $W$ and $Z$ given $X=H$, $i.e.$, $P(W,Z|X=H)$      Z=h  Z=t  W marginal      W=F  1/4  1/4  1/2    W=H  1/4  0  1/4    W=T  0  1/4  1/4    Z marginal  1/2  1/2      Hence the probability of getting heads in face-up is given by   \n\\begin{align}\n\\mathbb{P}(Z=h|Y=h) \n&= \\mathbb{P}(Z=h|X=F) \\times \\mathbb{P}(X=F|Y=h) + \\mathbb{P}(Z=h|X=H) \\times \\mathbb{P}(X=H|Y=h) \\\\\n&= \\mathbb{P}(Z=h|X=F) \\times \\frac{\\mathbb{P}(X=F, Y=h)}{ \\mathbb{P}(Y=h)} + \\mathbb{P}(Z=h|X=H) \\times \\frac{\\mathbb{P}(X=H, Y=h)}{ \\mathbb{P}(Y=h)} \\\\\n&= \\frac{5}{8} \\times \\frac{1/5}{3/5} + \\frac{1}{2} \\times \\frac{2/5}{3/5}\\\\\n&= \\frac{13}{24}\n\\end{align}",
            "title": "Homework Problem: Alice's Coins"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/",
            "text": "Product Rule for Random Variables\n\n\nWe introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\n\n\nImportant\n\n\n\n\nRemember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when     $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}  (y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}  {=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the  product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nSome times we won't actually care about doing this second renormalization step because we will only be   interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP) \n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01 Product Rule for Random Variables"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#product-rule-for-random-variables",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.  We now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#product-rule-for-random-variables_1",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.   Important   Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when     $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}  (y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}  {=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the  product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.      Note   Some times we won't actually care about doing this second renormalization step because we will only be   interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)   healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/week03/01 Product Rule for Random Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/week03/02 Independence Structure/",
            "text": "Introduction to Independence\n\n\nWith a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?\n\n\nThe answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".\n\n\nWe have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.\n\n\nNot only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.\n\n\nIndependet Events\n\n\nTwo events $A$ and $B$ are independent denoted by $A \\perp !! \\perp B$ if\n\n\n\n\n\\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)\n\n\n\n\nExample:\n If we toss two coin then probability of heads facing up is multiple of probability of heads for each coin. \n\n\n\n\nImportant\n\n\n\n\nIn terms of conditional probability we \n  \n \\begin{align} \n  \\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n  \\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n  \\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n  \\end{align}\n\n  Thus if $A \\perp !! \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$, $i.e.$, the occuring of event  $B$ doesn't depend weather event $A$ has occured or not.\n\n\nExercise: Bernoulli and Bin\n\n\nThis problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.\n\n\nThese two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!\n\n\nAs mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.\n\n\nA Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.\n\n\n(a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.\n\n\nAnswer:\n FALSE\n\n\n(b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nsequence = \nHTHTTTTTHH\n\nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob    \n\n\n\n\n0.0005308416000000001\n\n\n\n(c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?\n\n\n[$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails. \n\n[$\\times    $] The probability is different depending on the ordering of heads and tails.\n\n\n(d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)\n\n\ndef ncr(n, r):\n    \n\n    If calculates the n choose r for n \n= r.\n\n    \n ncr(10, 4)\n    210.0\n\n    \n ncr(4, 4)\n    1.0\n    \n\n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ == \n__main__\n:\n    import doctest \n    doctest.testmod()\n\n\n\n\nncr(10, 4)\n\n\n\n\n210.0\n\n\n\n(e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprob * ncr(10, 4)\n\n\n\n\n0.11147673600000002\n\n\n\nIn general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.\n\n\nPlease be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.\n\n\nExercise: The Soda Machine\n\n\n3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.\n\n\n(a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/7)**14\n\n\n\n\n2.4157243620710218e-08\n\n\n\n(b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/5)**14\n\n\n\n\n2.684354560000002e-06\n\n\n\n(c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/3)**14\n\n\n\n\n0.003425487390781748\n\n\n\nExercise: Gambler's Fallacy\n\n\nSuppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.\n\n\n(a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**100\n\n\n\n\n0.9770407138326136\n\n\n\n(b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**99\n\n\n\n\n0.9761576643646371\n\n\n\n(c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n $1 - (26/27)**(100-n)$\n\n\n(d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?\n\n\n[$\\checkmark$] Probability decreases as $n$ increases \n\n[$\\times    $] Probability increases as $n$ increases\n\n\nimport matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel(\nNo of trials without getting $27$\n)\nplt.ylabel(\nProb of getting $27$\n)\nplt.show()\n\n\n\n\n\n\nIndependet Random Variable\n\n\nTwo random variable $X$ and $Y$ are independent denoted by $X \\perp !! \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by \n\n\n\n\np_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y\n\n\n\n\nIndepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability \n\n\n\n\np_{X\\mid Y}(x\\mid y) = p_X(x) \n\n\n\n\nExercise: Independent Random Variables\n\n\nIn this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.\n\n\nConsider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.\n\n\n\n\nIn Python:\n\n\nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\n\n\n\n\nNote that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.\n\n\nWe can get the marginal distributions $p_W$ and $p_I$:\n\n\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n\n\n\n\nThen if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:\n\n\n\n\np_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.\n\n\n\n\nNote that variables \nprob_W\n and \nprob_I\n at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.\n\n\nWe could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of \nprob_W\n and \nprob_I\n yields what the joint distribution would be if $W$ and $I$ were independent:\n\n\n\n\n\\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}\n\n\n\n\nThe left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.\n\n\nTo compute and print the right-hand side, we do:\n\n\nprint(np.outer(prob_W, prob_I))\n\n\n\n\nQuestion:\n Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?\n\n\nAnswer:\n FALSE\n\n\nimport numpy as np\nfrom numpy.linalg import norm \nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, np.inf)\n\n\n\n\n0.5\n\n\n\nQuestion:\n Are X and Y independent?\n\n\nAnswer:\n TRUE\n\n\nprob_X_Y = np.array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, np.inf)\n\n\n\n\n0.0\n\n\n\nMutual and Pairwise Independence\n\n\nThree random variable $X, Y$ and $Z$ are \nmutually independent\n if \n\n\n\n\np_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z) \n\n\n\n\nThree random variable $X,Y$ and $Z$ are \npairwise independence\n if \n\n\n\n\np_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J\n\n\n\n\nThroughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.\n\n\nExercise: Mutual vs Pairwise Independence\n\n\nSuppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise. \nIn this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution\n (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).\n\n\nSuppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.\n\n\nSelect all of the following that are true:\n\n\n[$\\checkmark$] The distributions $p_X, p_Y$, and $p_Z$ are the same. \n\n[$\\checkmark$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same. \n\n[$\\checkmark$] $X, Y$, and $Z$ are pairwise independent.\n\n[$\\times    $] $X, Y$, and $Z$ are mutually independent.\n\n\nHere we have $X$ and $Y$ independent and identically distributed as $\\text{Bernoulli}(1/2)$, and $Z$ was the exclusive-or (\nXOR\n) of $X$ and $Y$. Notice that $Z$ takes on value $-1$ precisely when $X$ and $Y$ are different, and $1$ otherwise. Hopefully that should sound like \nXOR\n. Basically $-1$ is what used to be $1$, and $1$ is what used to be $0$. \n\n\nAs a reminder, you can check that $p_X$, $p_Y$, and $p_Z$ are each going to have $1/2-1/2$ chance of being either $1$ or $-1$, so they have the same distribution, and when we look at any pair of the random variables, they are going to appear independent with $(1, 1), (1, -1), (-1, 1)$, and $(-1, -1)$ equally likely so the pairs of random variables also have the same distribution. However, as before, when we look at all three random variables, they are not mutually independent!",
            "title": "02 Independence Structure"
        },
        {
            "location": "/week03/02 Independence Structure/#introduction-to-independence",
            "text": "With a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?  The answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".  We have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.  Not only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.",
            "title": "Introduction to Independence"
        },
        {
            "location": "/week03/02 Independence Structure/#independet-events",
            "text": "Two events $A$ and $B$ are independent denoted by $A \\perp !! \\perp B$ if   \\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)   Example:  If we toss two coin then probability of heads facing up is multiple of probability of heads for each coin.    Important   In terms of conditional probability we \n    \\begin{align} \n  \\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n  \\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n  \\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n  \\end{align} \n  Thus if $A \\perp !! \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$, $i.e.$, the occuring of event  $B$ doesn't depend weather event $A$ has occured or not.",
            "title": "Independet Events"
        },
        {
            "location": "/week03/02 Independence Structure/#exercise-bernoulli-and-bin",
            "text": "This problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.  These two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!  As mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.  A Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.  (a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.  Answer:  FALSE  (b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  sequence =  HTHTTTTTHH \nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob      0.0005308416000000001  (c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?  [$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails.  \n[$\\times    $] The probability is different depending on the ordering of heads and tails.  (d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)  def ncr(n, r):\n     \n    If calculates the n choose r for n  = r.\n\n      ncr(10, 4)\n    210.0\n\n      ncr(4, 4)\n    1.0\n     \n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ ==  __main__ :\n    import doctest \n    doctest.testmod()  ncr(10, 4)  210.0  (e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prob * ncr(10, 4)  0.11147673600000002  In general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.  Please be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.",
            "title": "Exercise: Bernoulli and Bin"
        },
        {
            "location": "/week03/02 Independence Structure/#exercise-the-soda-machine",
            "text": "3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.  (a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/7)**14  2.4157243620710218e-08  (b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/5)**14  2.684354560000002e-06  (c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/3)**14  0.003425487390781748",
            "title": "Exercise: The Soda Machine"
        },
        {
            "location": "/week03/02 Independence Structure/#exercise-gamblers-fallacy",
            "text": "Suppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.  (a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**100  0.9770407138326136  (b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**99  0.9761576643646371  (c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  $1 - (26/27)**(100-n)$  (d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?  [$\\checkmark$] Probability decreases as $n$ increases  \n[$\\times    $] Probability increases as $n$ increases  import matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel( No of trials without getting $27$ )\nplt.ylabel( Prob of getting $27$ )\nplt.show()",
            "title": "Exercise: Gambler's Fallacy"
        },
        {
            "location": "/week03/02 Independence Structure/#independet-random-variable",
            "text": "Two random variable $X$ and $Y$ are independent denoted by $X \\perp !! \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by    p_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y   Indepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability    p_{X\\mid Y}(x\\mid y) = p_X(x)",
            "title": "Independet Random Variable"
        },
        {
            "location": "/week03/02 Independence Structure/#exercise-independent-random-variables",
            "text": "In this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.  Consider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.   In Python:  prob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])  Note that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.  We can get the marginal distributions $p_W$ and $p_I$:  prob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)  Then if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:   p_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.   Note that variables  prob_W  and  prob_I  at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.  We could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of  prob_W  and  prob_I  yields what the joint distribution would be if $W$ and $I$ were independent:   \\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}   The left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.  To compute and print the right-hand side, we do:  print(np.outer(prob_W, prob_I))  Question:  Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?  Answer:  FALSE  import numpy as np\nfrom numpy.linalg import norm \nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, np.inf)  0.5  Question:  Are X and Y independent?  Answer:  TRUE  prob_X_Y = np.array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, np.inf)  0.0",
            "title": "Exercise: Independent Random Variables"
        },
        {
            "location": "/week03/02 Independence Structure/#mutual-and-pairwise-independence",
            "text": "Three random variable $X, Y$ and $Z$ are  mutually independent  if    p_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z)    Three random variable $X,Y$ and $Z$ are  pairwise independence  if    p_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J   Throughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.",
            "title": "Mutual and Pairwise Independence"
        },
        {
            "location": "/week03/02 Independence Structure/#exercise-mutual-vs-pairwise-independence",
            "text": "Suppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise.  In this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution  (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).  Suppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.  Select all of the following that are true:  [$\\checkmark$] The distributions $p_X, p_Y$, and $p_Z$ are the same.  \n[$\\checkmark$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same.  \n[$\\checkmark$] $X, Y$, and $Z$ are pairwise independent. \n[$\\times    $] $X, Y$, and $Z$ are mutually independent.  Here we have $X$ and $Y$ independent and identically distributed as $\\text{Bernoulli}(1/2)$, and $Z$ was the exclusive-or ( XOR ) of $X$ and $Y$. Notice that $Z$ takes on value $-1$ precisely when $X$ and $Y$ are different, and $1$ otherwise. Hopefully that should sound like  XOR . Basically $-1$ is what used to be $1$, and $1$ is what used to be $0$.   As a reminder, you can check that $p_X$, $p_Y$, and $p_Z$ are each going to have $1/2-1/2$ chance of being either $1$ or $-1$, so they have the same distribution, and when we look at any pair of the random variables, they are going to appear independent with $(1, 1), (1, -1), (-1, 1)$, and $(-1, -1)$ equally likely so the pairs of random variables also have the same distribution. However, as before, when we look at all three random variables, they are not mutually independent!",
            "title": "Exercise: Mutual vs Pairwise Independence"
        },
        {
            "location": "/week03/03 Conditional Independence/",
            "text": "Conditional Independence\n\n\nTwo random variable $X$ and $Y$ are conditiaonly independent given $Z$, denoted by $X \\perp !! \\perp Y \\mid Z$ if \n\n\n\n\np_{X,Y\\mid Z} (x,y\\mid z) = p_{X\\mid Z}(x\\mid z) \\, p_{Y\\mid Z}(y\\mid z)\n\n\n\n\nIn general Marginal independence doesn't imply conditional independence and vice versa. \n\n\nExample\n\n\nR: Red Sox Game \n\nA: Accident     \n\nT: Bad Traffic  \n\n\n\n\nFind the following probability\n\n\n(a) $\\mathbb{P}(R=1) = 0.5$\n\n\n(b) $\\mathbb{P}(R=1 \\mid T=1)$\n\n\n\n\n\\begin{align}p_{R,A}(r,a\\mid 1) \n&= \\frac{p_{T\\mid R,A}(1 \\mid r, a)\\, p_R(r) \\, p_A(a)}{p_T(1)}\\\\\n&= c\\cdot p_{T\\mid R,A}(1 \\mid r, a)\\end{align}\n\n\n\n\n(c) $\\mathbb{P}(R=1 \\mid T=1, A=1)= \\mathbb{P}(R=1 \\mid T=1)$\n\n\nPractice Problem: Conditional Independence\n\n\nSuppose $X_0, \\dots , X_{100}$ are random variables whose joint distribution has the following factorization:\n\n\n\n\np_{X_0, \\dots , X_{100}}(x_0, \\dots , x_{100}) = p_{X_0}(x_0) \\cdot \\prod _{i=1}^{100} p_{X_ i | X_{i-1}}(x_ i | x_{i-1})\n\n\n\n\nThis factorization is what's called a Markov chain. We'll be seeing Markov chains a lot more later on in the course.\n\n\nShow that $X_{50} \\perp !! \\perp X_{52} \\mid X_{51}$.\n\n\nAnswer:\n \n\n\n\\begin{eqnarray}\n        p_{X_{50},X_{51},X_{52}}(x_{50},x_{51},x_{52})\n        &=& \\sum_{x_{0} \\dots x_{49}} \\sum_{x_{53} \\dots x_{100}} p_{X_0, \\dots , X_{100}}(x_0, \\dots , x_{100})  \\\\\n        &=& \\sum_{x_{0} \\dots x_{49}} \\sum_{x_{53} \\dots x_{100}} \\left[p_{X_0}(x_{0}) \\prod_{i=0}^{50} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\right] \\cdot \\prod_{i=51}^{52} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\cdot \\prod_{i=53}^{100}  p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1}) \\\\\n        &=& \\underbrace{\\sum_{x_{0} \\dots x_{49}}  \\left[p_{X_0}(x_{0}) \\prod_{i=0}^{50} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\right]}_{=p_{X_{50}}(x_{50})} \\cdot \\prod_{i=51}^{52} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\cdot \\underbrace{\\sum_{x_{53} \\dots x_{100}}\\prod_{i=53}^{100}  p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})}_{=1}\\\\\n        &=& p_{X_{50}}(x_{50}) \\cdot p_{X_{51}\\mid X_{50}}(x_{51}|x_{50}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51}) \\\\\n        &=&  p_{X_{50}\\mid X_{51}}(x_{50}|x_{51}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51}) \\\\\n        \\frac{p_{X_{50},X_{51},X_{52}}(x_{50},x_{51},x_{52})}{p_{X_{51}}(x_{51})} \n        &=& p_{X_{50}\\mid X_{51}}(x_{50}|x_{51}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51})\\\\\n        p_{X_{50},X_{52}\\mid X_{51}}(x_{50},x_{52}\\mid x_{51}) \n        &=& p_{X_{50}\\mid X_{51}}(x_{50}|x_{51}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51})\n\\end{eqnarray}",
            "title": "03 Conditional Independence"
        },
        {
            "location": "/week03/03 Conditional Independence/#conditional-independence",
            "text": "Two random variable $X$ and $Y$ are conditiaonly independent given $Z$, denoted by $X \\perp !! \\perp Y \\mid Z$ if    p_{X,Y\\mid Z} (x,y\\mid z) = p_{X\\mid Z}(x\\mid z) \\, p_{Y\\mid Z}(y\\mid z)   In general Marginal independence doesn't imply conditional independence and vice versa.",
            "title": "Conditional Independence"
        },
        {
            "location": "/week03/03 Conditional Independence/#example",
            "text": "R: Red Sox Game  \nA: Accident      \nT: Bad Traffic     Find the following probability  (a) $\\mathbb{P}(R=1) = 0.5$  (b) $\\mathbb{P}(R=1 \\mid T=1)$   \\begin{align}p_{R,A}(r,a\\mid 1) \n&= \\frac{p_{T\\mid R,A}(1 \\mid r, a)\\, p_R(r) \\, p_A(a)}{p_T(1)}\\\\\n&= c\\cdot p_{T\\mid R,A}(1 \\mid r, a)\\end{align}   (c) $\\mathbb{P}(R=1 \\mid T=1, A=1)= \\mathbb{P}(R=1 \\mid T=1)$",
            "title": "Example"
        },
        {
            "location": "/week03/03 Conditional Independence/#practice-problem-conditional-independence",
            "text": "Suppose $X_0, \\dots , X_{100}$ are random variables whose joint distribution has the following factorization:   p_{X_0, \\dots , X_{100}}(x_0, \\dots , x_{100}) = p_{X_0}(x_0) \\cdot \\prod _{i=1}^{100} p_{X_ i | X_{i-1}}(x_ i | x_{i-1})   This factorization is what's called a Markov chain. We'll be seeing Markov chains a lot more later on in the course.  Show that $X_{50} \\perp !! \\perp X_{52} \\mid X_{51}$.  Answer:   \n\\begin{eqnarray}\n        p_{X_{50},X_{51},X_{52}}(x_{50},x_{51},x_{52})\n        &=& \\sum_{x_{0} \\dots x_{49}} \\sum_{x_{53} \\dots x_{100}} p_{X_0, \\dots , X_{100}}(x_0, \\dots , x_{100})  \\\\\n        &=& \\sum_{x_{0} \\dots x_{49}} \\sum_{x_{53} \\dots x_{100}} \\left[p_{X_0}(x_{0}) \\prod_{i=0}^{50} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\right] \\cdot \\prod_{i=51}^{52} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\cdot \\prod_{i=53}^{100}  p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1}) \\\\\n        &=& \\underbrace{\\sum_{x_{0} \\dots x_{49}}  \\left[p_{X_0}(x_{0}) \\prod_{i=0}^{50} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\right]}_{=p_{X_{50}}(x_{50})} \\cdot \\prod_{i=51}^{52} p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})\\cdot \\underbrace{\\sum_{x_{53} \\dots x_{100}}\\prod_{i=53}^{100}  p_{X_i\\mid X_{i-1}}(x_{i}|x_{i-1})}_{=1}\\\\\n        &=& p_{X_{50}}(x_{50}) \\cdot p_{X_{51}\\mid X_{50}}(x_{51}|x_{50}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51}) \\\\\n        &=&  p_{X_{50}\\mid X_{51}}(x_{50}|x_{51}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51}) \\\\\n        \\frac{p_{X_{50},X_{51},X_{52}}(x_{50},x_{51},x_{52})}{p_{X_{51}}(x_{51})} \n        &=& p_{X_{50}\\mid X_{51}}(x_{50}|x_{51}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51})\\\\\n        p_{X_{50},X_{52}\\mid X_{51}}(x_{50},x_{52}\\mid x_{51}) \n        &=& p_{X_{50}\\mid X_{51}}(x_{50}|x_{51}) \\cdot p_{X_{52}\\mid X_{51}}(x_{52}|x_{51})\n\\end{eqnarray}",
            "title": "Practice Problem: Conditional Independence"
        },
        {
            "location": "/week03/04 Homework/",
            "text": "Homework\n\n\nIce Cream Sales in Inferenceville\n\n\nYou have been hired to investigate a disturbing connection between ice cream sales and crime in Inferenceville. You are given a report that describes the joint distribution over random variable $S$, representing ice cream sales, and random variable $C$, representing crime. Each variable takes on a value of \u201clow\" or \u201chigh\", which we'll represent with $0$ and $1$ respectively. The joint distribution (estimated from data) is as follows:\n\n\n\n\n(a) Are random variables $S$ and $C$ independent?\n\n\n[$\\times    $] Yes \n\n[$\\checkmark$] No\n\n\ndef is_independent(p_X_Y):\n    \n\n    Returns true if the given join prbability distribution is independet.\n\n    \n import numpy as np\n    \n p_X_Y = np.array([[0.4, 0.1], [0.25, 0.25]])\n    \n is_independent(p_X_Y)\n    False\n\n    \n p_X_Y = np.array([[0.72, 0.08], [0.18, 0.02]])\n    \n is_independent(p_X_Y)\n    True\n    \n\n\n    import numpy as np\n    p_X = p_X_Y.sum(axis=1)\n    p_Y = p_X_Y.sum(axis=0)\n    \u0394 = np.outer(p_X, p_Y) - p_X_Y\n    return (np.linalg.norm(\u0394, np.inf) - 0.00001) \n 0\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nimport numpy as np\np_S_C = np.array([[0.4, 0.1], [0.25, 0.25]])\nis_independent(p_S_C)\n\n\n\n\nFalse\n\n\n\n(b) After further investigation, you discover information about the temperature, represented by $T$. This random variable also takes on values $0$ or $1$ corresponding again to \u201clow\" and \u201chigh\". You are able to obtain the conditional distribution $p_{S,C \\mid T}(s,c \\mid t)$, shown below.\n\n\n\n\nAre random variables $S$ and $C$ conditionally independent given $T$?\n\n\n[$\\checkmark$] Yes \n\n[$\\times    $] No\n\n\nimport numpy as np\np_T_1 = np.array([[0.72, 0.08], [0.18, 0.02]])\np_T_2 = np.array([[0.08, 0.12], [0.32, 0.48]])\nis_independent(p_T_1) and is_independent(p_T_2)\n\n\n\n\nTrue\n\n\n\n(c) Determine the distribution $p_T$ from the tables above. Express your answer as a Python dictionary. The keys should be the Python integers $0$ and $1$.\n\n\np_T = {}",
            "title": "04 Homework"
        },
        {
            "location": "/week03/04 Homework/#homework",
            "text": "",
            "title": "Homework"
        },
        {
            "location": "/week03/04 Homework/#ice-cream-sales-in-inferenceville",
            "text": "You have been hired to investigate a disturbing connection between ice cream sales and crime in Inferenceville. You are given a report that describes the joint distribution over random variable $S$, representing ice cream sales, and random variable $C$, representing crime. Each variable takes on a value of \u201clow\" or \u201chigh\", which we'll represent with $0$ and $1$ respectively. The joint distribution (estimated from data) is as follows:   (a) Are random variables $S$ and $C$ independent?  [$\\times    $] Yes  \n[$\\checkmark$] No  def is_independent(p_X_Y):\n     \n    Returns true if the given join prbability distribution is independet.\n\n      import numpy as np\n      p_X_Y = np.array([[0.4, 0.1], [0.25, 0.25]])\n      is_independent(p_X_Y)\n    False\n\n      p_X_Y = np.array([[0.72, 0.08], [0.18, 0.02]])\n      is_independent(p_X_Y)\n    True\n     \n\n    import numpy as np\n    p_X = p_X_Y.sum(axis=1)\n    p_Y = p_X_Y.sum(axis=0)\n    \u0394 = np.outer(p_X, p_Y) - p_X_Y\n    return (np.linalg.norm(\u0394, np.inf) - 0.00001)   0\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  import numpy as np\np_S_C = np.array([[0.4, 0.1], [0.25, 0.25]])\nis_independent(p_S_C)  False  (b) After further investigation, you discover information about the temperature, represented by $T$. This random variable also takes on values $0$ or $1$ corresponding again to \u201clow\" and \u201chigh\". You are able to obtain the conditional distribution $p_{S,C \\mid T}(s,c \\mid t)$, shown below.   Are random variables $S$ and $C$ conditionally independent given $T$?  [$\\checkmark$] Yes  \n[$\\times    $] No  import numpy as np\np_T_1 = np.array([[0.72, 0.08], [0.18, 0.02]])\np_T_2 = np.array([[0.08, 0.12], [0.32, 0.48]])\nis_independent(p_T_1) and is_independent(p_T_2)  True  (c) Determine the distribution $p_T$ from the tables above. Express your answer as a Python dictionary. The keys should be the Python integers $0$ and $1$.  p_T = {}",
            "title": "Ice Cream Sales in Inferenceville"
        },
        {
            "location": "/week04/01 Expected Value/",
            "text": "Expected value of random variable\n\n\nExpected value of random variable is generalization of taking average of numbers. It is similar to taking weighted average, where each value of random variable is multiplied by it's probability. \n\n\n\n\n\\mathbb{E}[X] = \\sum_{x \\in \\mathcal{X}} x \\cdot p_X(x) \n\n\n\n\nAlso in terms of conditional probability,\n\n\\mathbb{E}[X \\mid Y=y] = \\sum_{x \\in \\mathcal{X}} x \\cdot p_{X\\mid Y}(x\\mid y)\n\n\n\n\nIn general, let $f$ any function from $\\mathbb{R}$ to $\\mathbb{R}$, then \n\n\n\n\n \\mathbb{E}[f(X)] = \\sum_{x \\in \\mathcal{X}} f(x) \\cdot p_X(x) \n\n\n\n\nThus expectection gives a single number associated with a probability table.\n\n\nShanon Information Content\n\n\nShanon information content for event $A$ is defined as $\\log_2 \\frac{1}{\\mathbb{P}(A)}$. \n\n\n\n\nNote\n\n\nThe Shanon information content is high for an event which has very low probability. Hence if something of low probabilty happens it gives us a large number of information.\n\n\n\n\nShanon Entropy\n\n\nHow many bits to store $n$ \ni.i.d.\n samples form $p_X$?\n\n\n\n\nH(X) \\sim n \\left [ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{p_X(x)}\\right ]\n\n\n\n\nAlso, in terms of expectation, \n\n\n\n\nH(X) = \\mathbb{E}\\left[ \\log_{2}{\\frac{1}{p_X(x)}}\\right]\n\n\n\n\nInformation divergence\n\n\n\n\n\\text{Entropy:} \\quad H(X) \\sim n \\left [ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{p_X(x)}\\right ]\n\n\n\n\nSuppose we want to distribute $X \\sim q$, then the entropy will be \n\n\n\n\n \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{q(x)}\n\n\n\n\nThis is fundamental theorem of information theory that if we use any distribution other than $p$ then we have to use more bits. \n\n\nInformation divergence of $p$ and $q$ is defined as \n\n\n\n\n\\begin{align} D(p\\parallel q) &= \\sum_{x\\in \\mathcal{X}} p_X(x)  \\log_2\\frac{1}{q(x)} - \\sum_{x\\in \\mathcal{X}} p_X(x)  \\log_2\\frac{1}{p(x)} \\\\[2ex]\n&= \\sum_{x\\in \\mathcal{X}} p_X(x)\\log_2\\frac{p(x)}{q(x)} \\end{align}\n\n\n\n\nGeometric Distribution\n\n\nSuppose we have a biased coin with $\\mathbb{P}(H) = p$ and we toss it repeatedly, then the sample space will look like \n\n\n\n\n\\Omega = \\{H, TH, TTH, \\ldots \\}\n\nThen \n\n\\mathbb{P}(TH) = \\mathbb{P}(T)\\cdot \\mathbb{P}(H) = (1-p)p\n\n\n\n\nLet $X$ is the random variable defined as \n\n\n\n\nX = \\#\\text{ tosses untill first heads }\n\nThen \n\n\\begin{align}\\mathbb{P}(X=x) &= \\mathbb{P}(x-1 \\text{ tails followed by heads}) \\\\\n&= (1-p)^{x-1}p \\end{align}\n\n\nThis is probability mass function of random variable $X$. If we normalize we will get the probability distribution.",
            "title": "01 Expected Value"
        },
        {
            "location": "/week04/01 Expected Value/#expected-value-of-random-variable",
            "text": "Expected value of random variable is generalization of taking average of numbers. It is similar to taking weighted average, where each value of random variable is multiplied by it's probability.    \\mathbb{E}[X] = \\sum_{x \\in \\mathcal{X}} x \\cdot p_X(x)    Also in terms of conditional probability, \\mathbb{E}[X \\mid Y=y] = \\sum_{x \\in \\mathcal{X}} x \\cdot p_{X\\mid Y}(x\\mid y)   In general, let $f$ any function from $\\mathbb{R}$ to $\\mathbb{R}$, then     \\mathbb{E}[f(X)] = \\sum_{x \\in \\mathcal{X}} f(x) \\cdot p_X(x)    Thus expectection gives a single number associated with a probability table.",
            "title": "Expected value of random variable"
        },
        {
            "location": "/week04/01 Expected Value/#shanon-information-content",
            "text": "Shanon information content for event $A$ is defined as $\\log_2 \\frac{1}{\\mathbb{P}(A)}$.    Note  The Shanon information content is high for an event which has very low probability. Hence if something of low probabilty happens it gives us a large number of information.",
            "title": "Shanon Information Content"
        },
        {
            "location": "/week04/01 Expected Value/#shanon-entropy",
            "text": "How many bits to store $n$  i.i.d.  samples form $p_X$?   H(X) \\sim n \\left [ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{p_X(x)}\\right ]   Also, in terms of expectation,    H(X) = \\mathbb{E}\\left[ \\log_{2}{\\frac{1}{p_X(x)}}\\right]",
            "title": "Shanon Entropy"
        },
        {
            "location": "/week04/01 Expected Value/#information-divergence",
            "text": "\\text{Entropy:} \\quad H(X) \\sim n \\left [ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{p_X(x)}\\right ]   Suppose we want to distribute $X \\sim q$, then the entropy will be     \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{q(x)}   This is fundamental theorem of information theory that if we use any distribution other than $p$ then we have to use more bits.   Information divergence of $p$ and $q$ is defined as    \\begin{align} D(p\\parallel q) &= \\sum_{x\\in \\mathcal{X}} p_X(x)  \\log_2\\frac{1}{q(x)} - \\sum_{x\\in \\mathcal{X}} p_X(x)  \\log_2\\frac{1}{p(x)} \\\\[2ex]\n&= \\sum_{x\\in \\mathcal{X}} p_X(x)\\log_2\\frac{p(x)}{q(x)} \\end{align}",
            "title": "Information divergence"
        },
        {
            "location": "/week04/01 Expected Value/#geometric-distribution",
            "text": "Suppose we have a biased coin with $\\mathbb{P}(H) = p$ and we toss it repeatedly, then the sample space will look like    \\Omega = \\{H, TH, TTH, \\ldots \\} \nThen  \\mathbb{P}(TH) = \\mathbb{P}(T)\\cdot \\mathbb{P}(H) = (1-p)p   Let $X$ is the random variable defined as    X = \\#\\text{ tosses untill first heads } \nThen  \\begin{align}\\mathbb{P}(X=x) &= \\mathbb{P}(x-1 \\text{ tails followed by heads}) \\\\\n&= (1-p)^{x-1}p \\end{align} \nThis is probability mass function of random variable $X$. If we normalize we will get the probability distribution.",
            "title": "Geometric Distribution"
        }
    ]
}