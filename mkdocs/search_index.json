{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to MkDocs\n\n\nFor full documentation visit \nmkdocs.org\n.\n\n\nCommands\n\n\n\n\nmkdocs new [dir-name]\n - Create a new project.\n\n\nmkdocs serve\n - Start the live-reloading docs server.\n\n\nmkdocs build\n - Build the documentation site.\n\n\nmkdocs help\n - Print this help message.\n\n\n\n\nProject layout\n\n\nmkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n\n\n\nQuestion 1:\n If $A$ is a $5\\times 5$ real matrix with trace $15$ and if $2$ and $3$ are eigenvalues of $A$ , each with algebraic multiplicity $2$, then the determinant of $A$ is equal to\n\n\n\n\n$0$\n\n\n$24$\n\n\n$120$\n\n\n$180$\n\n\n\n\nAnswer:\n Out of five eigenvalues of matrix $A$ we know four $2,~2,~3,~3$. Let the last eigenvalue is $\\lambda$. Then,\n\n\\begin{align}& \\text{ trace }A = 2+2+3+3+\\lambda\\\\ \\Rightarrow& \\lambda = 15-10=5.\\end{align}\n\nHence the determinant, will be product of eigenvalues\n\n\\text{ det }A = 2\\cdot 2\\cdot 3\\cdot 3\\cdot 5=180.\n\n\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 2:\n For a positive integer $n$, let $P_n$ denote the vector space of polynomials in one variable $x$ with real coefficients and with degree $\\leq n$. Consider the map $T:P_2 \\rightarrow P_4$ defined by $T(p(x)) = p(x^2)$. Then\n\n\n\n\n$T$ is a linear transformation and dim range $(T) = 5$.\n\n\n$T$ is a linear transformation and dim range $(T) = 3$.\n\n\n$T$ is a linear transformation and dim range $(T) = 2$.\n\n\n$T$ is not a linear transformation.\n\n\n\n\nAnswer:\n This transformation replaces each $x$ by $x^2$. Also,\n\n\\begin{align}T(\\alpha p(x) + \\beta q(x)) &= \\alpha p(x^2) + \\beta q(x^2)\\\\ &=\\alpha T(p(x)) + \\beta T(q(x))\\end{align}\n\nHence this is a linear transformation, and\n\n\\text{ ker }T = \\{p(x) : p(x^2) = 0\\} = \\{0\\}.\n\nHence the nullity will be zero. By rank nullity theorem, we get\n\n\n\\begin{align}\n\\text{ rank }T &= \\text{ dim }P_2 - \\text{ nullity }\\\\\n&= 3 - 0 \\\\\n&= 3\n\\end{align}\n\n\n\n\n\nHence \n2\n is correct choice.\n\n\n\n\nQuestion 3:\n Let $A$ be a real $3\\times 4$ matrix of rank $2$. Then the rank of $A^tA$, where $A^t$ deonotes the transpose $A$, is\n\n\n\n\nexactly $2$\n\n\nexactly $3$\n\n\nexactly $4$\n\n\nat most $2$ but not necessarily $2$\n\n\n\n\nAnswer:\n Given that the rank of $A$ is $2$, hence the nullity of $A$ is $4-2 = 2$. Using theorem,\n\n\n\n\nNullspace of $A$ and $A^TA$ are the same.\n\n\n\n\nWe get the nullity of $A^TA$ is 2. Since $A^TA$ is $4\\times 4$ matrix. Hence rank of $A^TA$ will be $4-2 = 2$.\n\n\nHence \n1\n is correct choice.\n\n\n\n\nQuestion 4:\n  Let $S$ denote the set of all the prime numbers $p$ with the property that the matrix\n\n\n\\pmatrix{91& 31 & 0\\\\ 29 & 31 & 0\\\\ 79 & 23 & 59}\n\n\nhas an inverse in the field $\\newcommand{\\Z}{\\mathbb{Z}}\\Z/p\\Z$. Then\n\n\n\n\n$S = {31}$\n\n\n$S = {31, ~59}$\n\n\n$S = {7, ~13, ~59}$\n\n\n$S$ is infinite\n\n\n\n\nAnswer:\n The matrix is singular, if it's determinant will be zero in that field.\n\n\\begin{align}\\text{ det } &= 59\\times 31(91-29) \\\\ &= 59\\times 31 \\times  31 \\times 2.\\end{align}\n\nThe determinant will be zero in $\\Z_p$ only for $p=59, ~31, ~2$. For all other primes it will be non-zero.\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 5:\n Consider the quadratic form $Q(v) = v^tAv$, where\n\nA = \\pmatrix{1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&0&1}, v= (x,y,z,w)\n\nThen\n\n\n\n\n$Q$ has rank $3$.\n\n\n$xy+z^2 = Q(P(v))$ for some invertible $4 \\times 4$ real matrix $P$.\n\n\n$xy+y^2+ z^2 = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n$x^2+y^2-zw = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n\n\nAnswer:\n Here,\n\nQ(Pv) = (Pv)^tAPv = v^t(P^tAP)v \n\nWhen $P$ is orthogonal, $A$ and $P^tAP$ both represent equivalent quadratic forms. Then the problem is reduced to find equivalent bilinear forms.\n\n\n\n\nQuestion 6:\n Let $A \\neq I_n$ be an $n \\times n$ matrix such that $A^2 = A$, where $I_n$ is the identity matrix of order $n$. Which of the following statements is false?\n\n\n\n\n$(I_n - A)^2 = I_n - A$.\n\n\ntrace $(A)=$ rank $(A)$.\n\n\nrank $(A)$ + rank $(I_n-A)$ = n.\n\n\nThe eigenvalus of $A$ are each equal to $1$.\n\n\n\n\nAnswer:\n The matrix satisfies the polynomial $x^2 - x = 0\\Rightarrow x(x-1) = 0$. Hence minimal polynomial is a factor of $x(x-1)$. Since this polynomial has linear factors, hence matrix is diagonalizable with eigenvalues $0$ or $1$ or both. Since $A \\neq I$ then at least eigenvalues will be $0$.\n\n\nHence \n4\n is correct choice.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-mkdocs",
            "text": "For full documentation visit  mkdocs.org .",
            "title": "Welcome to MkDocs"
        },
        {
            "location": "/#commands",
            "text": "mkdocs new [dir-name]  - Create a new project.  mkdocs serve  - Start the live-reloading docs server.  mkdocs build  - Build the documentation site.  mkdocs help  - Print this help message.",
            "title": "Commands"
        },
        {
            "location": "/#project-layout",
            "text": "mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.  Question 1:  If $A$ is a $5\\times 5$ real matrix with trace $15$ and if $2$ and $3$ are eigenvalues of $A$ , each with algebraic multiplicity $2$, then the determinant of $A$ is equal to   $0$  $24$  $120$  $180$   Answer:  Out of five eigenvalues of matrix $A$ we know four $2,~2,~3,~3$. Let the last eigenvalue is $\\lambda$. Then, \\begin{align}& \\text{ trace }A = 2+2+3+3+\\lambda\\\\ \\Rightarrow& \\lambda = 15-10=5.\\end{align} \nHence the determinant, will be product of eigenvalues \\text{ det }A = 2\\cdot 2\\cdot 3\\cdot 3\\cdot 5=180.   Hence  4  is correct choice.   Question 2:  For a positive integer $n$, let $P_n$ denote the vector space of polynomials in one variable $x$ with real coefficients and with degree $\\leq n$. Consider the map $T:P_2 \\rightarrow P_4$ defined by $T(p(x)) = p(x^2)$. Then   $T$ is a linear transformation and dim range $(T) = 5$.  $T$ is a linear transformation and dim range $(T) = 3$.  $T$ is a linear transformation and dim range $(T) = 2$.  $T$ is not a linear transformation.   Answer:  This transformation replaces each $x$ by $x^2$. Also, \\begin{align}T(\\alpha p(x) + \\beta q(x)) &= \\alpha p(x^2) + \\beta q(x^2)\\\\ &=\\alpha T(p(x)) + \\beta T(q(x))\\end{align} \nHence this is a linear transformation, and \\text{ ker }T = \\{p(x) : p(x^2) = 0\\} = \\{0\\}. \nHence the nullity will be zero. By rank nullity theorem, we get \n\\begin{align}\n\\text{ rank }T &= \\text{ dim }P_2 - \\text{ nullity }\\\\\n&= 3 - 0 \\\\\n&= 3\n\\end{align}   Hence  2  is correct choice.   Question 3:  Let $A$ be a real $3\\times 4$ matrix of rank $2$. Then the rank of $A^tA$, where $A^t$ deonotes the transpose $A$, is   exactly $2$  exactly $3$  exactly $4$  at most $2$ but not necessarily $2$   Answer:  Given that the rank of $A$ is $2$, hence the nullity of $A$ is $4-2 = 2$. Using theorem,   Nullspace of $A$ and $A^TA$ are the same.   We get the nullity of $A^TA$ is 2. Since $A^TA$ is $4\\times 4$ matrix. Hence rank of $A^TA$ will be $4-2 = 2$.  Hence  1  is correct choice.   Question 4:   Let $S$ denote the set of all the prime numbers $p$ with the property that the matrix \n\\pmatrix{91& 31 & 0\\\\ 29 & 31 & 0\\\\ 79 & 23 & 59} \nhas an inverse in the field $\\newcommand{\\Z}{\\mathbb{Z}}\\Z/p\\Z$. Then   $S = {31}$  $S = {31, ~59}$  $S = {7, ~13, ~59}$  $S$ is infinite   Answer:  The matrix is singular, if it's determinant will be zero in that field. \\begin{align}\\text{ det } &= 59\\times 31(91-29) \\\\ &= 59\\times 31 \\times  31 \\times 2.\\end{align} \nThe determinant will be zero in $\\Z_p$ only for $p=59, ~31, ~2$. For all other primes it will be non-zero.  Hence  4  is correct choice.   Question 5:  Consider the quadratic form $Q(v) = v^tAv$, where A = \\pmatrix{1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&0&1}, v= (x,y,z,w) \nThen   $Q$ has rank $3$.  $xy+z^2 = Q(P(v))$ for some invertible $4 \\times 4$ real matrix $P$.  $xy+y^2+ z^2 = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.  $x^2+y^2-zw = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.   Answer:  Here, Q(Pv) = (Pv)^tAPv = v^t(P^tAP)v  \nWhen $P$ is orthogonal, $A$ and $P^tAP$ both represent equivalent quadratic forms. Then the problem is reduced to find equivalent bilinear forms.   Question 6:  Let $A \\neq I_n$ be an $n \\times n$ matrix such that $A^2 = A$, where $I_n$ is the identity matrix of order $n$. Which of the following statements is false?   $(I_n - A)^2 = I_n - A$.  trace $(A)=$ rank $(A)$.  rank $(A)$ + rank $(I_n-A)$ = n.  The eigenvalus of $A$ are each equal to $1$.   Answer:  The matrix satisfies the polynomial $x^2 - x = 0\\Rightarrow x(x-1) = 0$. Hence minimal polynomial is a factor of $x(x-1)$. Since this polynomial has linear factors, hence matrix is diagonalizable with eigenvalues $0$ or $1$ or both. Since $A \\neq I$ then at least eigenvalues will be $0$.  Hence  4  is correct choice.",
            "title": "Project layout"
        },
        {
            "location": "/Computational Probability/week01/01 Simulating Coin Filps/",
            "text": "Simulating Coin Filps\n\n\n%matplotlib inline\n\n\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nflip_fair_coin()\n\n\n\n\n'heads'\n\n\n\nflips = flip_fair_coins(100)\nplot_discrete_histogram(flips)\n\n\n\n\n\n\nplot_discrete_histogram(flip_fair_coins(10), frequency=True)\n\n\n\n\n\n\nn = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "01 Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01/01 Simulating Coin Filps/#simulating-coin-filps",
            "text": "%matplotlib inline  import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *  flip_fair_coin()  'heads'  flips = flip_fair_coins(100)\nplot_discrete_histogram(flips)   plot_discrete_histogram(flip_fair_coins(10), frequency=True)   n = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))  import matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/",
            "text": "Introduction to Independence\n\n\nWith a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?\n\n\nThe answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".\n\n\nWe have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.\n\n\nNot only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.\n\n\nIndependet Events\n\n\nTwo events $A$ and $B$ are independet denoted by $A \\perp B$ if\n\n\n\n\n\\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)\n\n\n\n\nExample:\n If we toss two coin then probability of heads up is multiple of probability of heads for each coin. \n\n\nIn terms of conditional probability we \n\n \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align}\n\nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.\n\n\nExercise: Bernoulli and Bin\n\n\nThis problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.\n\n\nThese two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!\n\n\nAs mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.\n\n\nA Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.\n\n\n(a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.\n\n\nAnswer:\n FALSE\n\n\n(b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nsequence = \nHTHTTTTTHH\n\nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob    \n\n\n\n\n0.0005308416000000001\n\n\n\n(c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?\n\n\n[$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails. \n\n[$\\times    $] The probability is different depending on the ordering of heads and tails.\n\n\n(d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)\n\n\ndef ncr(n, r):\n    \n\n    If calculates the n choose r for n \n= r.\n\n    \n ncr(10, 4)\n    210.0\n\n    \n ncr(4, 4)\n    1.0\n    \n\n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ == \n__main__\n:\n    import doctest \n    doctest.testmod()\n\n\n\n\nncr(10, 4)\n\n\n\n\n210.0\n\n\n\n(e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprob * ncr(10, 4)\n\n\n\n\n0.11147673600000002\n\n\n\nIn general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.\n\n\nPlease be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.\n\n\nExercise: The Soda Machine\n\n\n3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.\n\n\n(a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/7)**14\n\n\n\n\n2.4157243620710218e-08\n\n\n\n(b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/5)**14\n\n\n\n\n2.684354560000002e-06\n\n\n\n(c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/3)**14\n\n\n\n\n0.003425487390781748\n\n\n\nExercise: Gambler's Fallacy\n\n\nSuppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.\n\n\n(a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**100\n\n\n\n\n0.9770407138326136\n\n\n\n(b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**99\n\n\n\n\n0.9761576643646371\n\n\n\n(c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\n1 - (26/27)**(100-n)\n\n\n\n\n0.9770407138326136\n\n\n\n(d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?\n\n\n[$\\checkmark$] Probability decreases as $n$ increases \n\n[$\\times    $] Probability increases as $n$ increases\n\n\nimport matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel(\nNo of trials without getting $27$\n)\nplt.ylabel(\nProb of getting $27$\n)\nplt.show()\n\n\n\n\n\n\nIndependet Random Variable\n\n\nTwo random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by \n\n\n\n\np_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y\n\n\n\n\nIndepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability \n\n\n\n\np_{X\\mid Y}(x\\mid y) = p_X(x) \n\n\n\n\nExercise: Independent Random Variables\n\n\nIn this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.\n\n\nConsider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.\n\n\n\n\nIn Python:\n\n\nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\n\n\n\n\nNote that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.\n\n\nWe can get the marginal distributions $p_W$ and $p_I$:\n\n\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n\n\n\n\nThen if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:\n\n\n\n\np_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.\n\n\n\n\nNote that variables \nprob_W\n and \nprob_I\n at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.\n\n\nWe could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of \nprob_W\n and \nprob_I\n yields what the joint distribution would be if $W$ and $I$ were independent:\n\n\n\n\n\\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}\n\n\n\n\nThe left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.\n\n\nTo compute and print the right-hand side, we do:\n\n\nprint(np.outer(prob_W, prob_I))\n\n\n\n\nQuestion:\n Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?\n\n\nAnswer:\n FALSE\n\n\nfrom numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)\n\n\n\n\n0.5\n\n\n\nQuestion:\n Are X and Y independent?\n\n\nAnswer:\n TRUE\n\n\nprob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)\n\n\n\n\n0.0\n\n\n\nMutual and Pairwise Independence\n\n\nThree random variable $X, Y$ and $Z$ are \nmutually independent\n if \n\n\n\n\np_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z) \n\n\n\n\nThree random variable $X,Y$ and $Z$ are \npairwise independence\n if \n\n\n\n\np_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J\n\n\n\n\nThroughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.\n\n\nExercise: Mutual vs Pairwise Independence\n\n\nSuppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise. \nIn this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution\n (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).\n\n\nSuppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.\n\n\nSelect all of the following that are true:\n\n\n[$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same. \n\n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same. \n\n[$\\times$] $X, Y$, and $Z$ are pairwise independent.\n\n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "02 Independence Structure"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#introduction-to-independence",
            "text": "With a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?  The answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".  We have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.  Not only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.",
            "title": "Introduction to Independence"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#independet-events",
            "text": "Two events $A$ and $B$ are independet denoted by $A \\perp B$ if   \\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)   Example:  If we toss two coin then probability of heads up is multiple of probability of heads for each coin.   In terms of conditional probability we   \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align} \nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.",
            "title": "Independet Events"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#exercise-bernoulli-and-bin",
            "text": "This problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.  These two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!  As mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.  A Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.  (a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.  Answer:  FALSE  (b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  sequence =  HTHTTTTTHH \nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob      0.0005308416000000001  (c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?  [$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails.  \n[$\\times    $] The probability is different depending on the ordering of heads and tails.  (d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)  def ncr(n, r):\n     \n    If calculates the n choose r for n  = r.\n\n      ncr(10, 4)\n    210.0\n\n      ncr(4, 4)\n    1.0\n     \n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ ==  __main__ :\n    import doctest \n    doctest.testmod()  ncr(10, 4)  210.0  (e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prob * ncr(10, 4)  0.11147673600000002  In general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.  Please be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.",
            "title": "Exercise: Bernoulli and Bin"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#exercise-the-soda-machine",
            "text": "3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.  (a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/7)**14  2.4157243620710218e-08  (b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/5)**14  2.684354560000002e-06  (c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/3)**14  0.003425487390781748",
            "title": "Exercise: The Soda Machine"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#exercise-gamblers-fallacy",
            "text": "Suppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.  (a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**100  0.9770407138326136  (b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**99  0.9761576643646371  (c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  1 - (26/27)**(100-n)  0.9770407138326136  (d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?  [$\\checkmark$] Probability decreases as $n$ increases  \n[$\\times    $] Probability increases as $n$ increases  import matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel( No of trials without getting $27$ )\nplt.ylabel( Prob of getting $27$ )\nplt.show()",
            "title": "Exercise: Gambler's Fallacy"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#independet-random-variable",
            "text": "Two random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by    p_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y   Indepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability    p_{X\\mid Y}(x\\mid y) = p_X(x)",
            "title": "Independet Random Variable"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#exercise-independent-random-variables",
            "text": "In this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.  Consider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.   In Python:  prob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])  Note that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.  We can get the marginal distributions $p_W$ and $p_I$:  prob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)  Then if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:   p_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.   Note that variables  prob_W  and  prob_I  at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.  We could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of  prob_W  and  prob_I  yields what the joint distribution would be if $W$ and $I$ were independent:   \\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}   The left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.  To compute and print the right-hand side, we do:  print(np.outer(prob_W, prob_I))  Question:  Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?  Answer:  FALSE  from numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)  0.5  Question:  Are X and Y independent?  Answer:  TRUE  prob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)  0.0",
            "title": "Exercise: Independent Random Variables"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#mutual-and-pairwise-independence",
            "text": "Three random variable $X, Y$ and $Z$ are  mutually independent  if    p_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z)    Three random variable $X,Y$ and $Z$ are  pairwise independence  if    p_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J   Throughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.",
            "title": "Mutual and Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01/02 Independence Structure/#exercise-mutual-vs-pairwise-independence",
            "text": "Suppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise.  In this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution  (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).  Suppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.  Select all of the following that are true:  [$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same.  \n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same.  \n[$\\times$] $X, Y$, and $Z$ are pairwise independent. \n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "Exercise: Mutual vs Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/",
            "text": "Conditioning for Random Variables\n\n\nWhen we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.\n\n\nWhen we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:\n\n\n\n\nSecond, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:\n\n\n\n\nNotation:\n The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.\n\n\n\n\nIn general:\n\n\nConditioning:\n Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y)\n0$, the \nconditional probability\n of event $X=x$ given event $Y=y$ has happened is\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.\n\n\n\n\nFor example,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.\n\n\n\n\nComputational interpretation:\n To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.\n\n\nExercise: Conditioning for Random Variables\n\n\nConsider the following two joint probability tables.\n\n\n\n\nQuestion 1:\n What is $p_{W|I}(\\text {sunny}|1)$?\n\n\np_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]\n\n\n\n\n1.0\n\n\n\nQuestion 2:\n What is $p_{X|Y}(\\text {sunny}|1)$?\n\n\np_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y     \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]\n\n\n\n\n0.5\n\n\n\nQuestion 3:\n What is $p_{I|W}(1|\\text {snowy})$? \n\n\n# p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)\n\n\n\n\n0\n\n\n\nQuestion 4:\n What is $p_{Y|X}(1|\\text {snowy})$?\n\n\n# p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']\n\n\n\n\n0.5\n\n\n\nExercise: Simpson's Paradox\n\n\nThis problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.\n\n\nWe have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).\n\n\nThus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.\n\n\nThe joint probability table is provided in the file \nsimpsons_paradox_data.py\n. Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).\n\n\nNow let's load in everything from \nsimpsons_paradox_data.py\n:\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nPlease open up \nsimpsons_paradox_data.py\n to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.\n\n\nFor example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:\n\n\njoint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]\n\n\n\n\nSome terminology:\n In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.\n\n\nLet's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).\n\n\nMarginalization is easy to do with NumPy:\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\n\n\n\n\nOn the right-hand side,\n.sum(axis=1)\n says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable \njoint_prob_gender_admission\n stores a 2D joint probability table for random variables $G$ and $A$.\n\n\nNow, for example, the probability that a woman applies and is admitted is given by:\n\n\njoint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\n\n\n\n\nNow let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:\n\n\n\n\n\\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}\n\n\n\n\nLet's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:\n\n\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\n\n\n\n\nNow this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!\n\n\nprob_admission_given_female = female_only / np.sum(female_only)\n\n\n\n\nThis is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:\n\n\nprob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)\n\n\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint(\nProbability of admitted female: {0:.5f}\n.format(prob_admitted_given_female))\n\n\n\n\nProbability of admitted female: 0.30334\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nmale_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint(\nProbability of admitted male: {0:.5f}\n.format(prob_admitted_given_male))\n\n\n\n\nProbability of admitted male: 0.44520\n\n\n\nSo it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.\n\n\nBut before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:\n\n\nadmitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]\n\n\n\n\nNotice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.\n\n\nThus, the conditional probability table of gender given admitted is:\n\n\nprob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)\n\n\n\n\nAll right, now let's look at which departments are favoring men over women.\n\n\nFor the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:\n\n\nfemale_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]\n\n\n\n\nNow let's determine the probability of getting admitted given each gender and each department.\n\n\nDepartment A:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\n# A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint(\nProbability of admitted female in department A: {0:.3f}\n.format(p_A_given_F_and_A))\n\n\n\n\nProbability of admitted female in department A: 0.820\n\n\n\nGeneral Approach\n\n\n# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep\n\n\n\n\narray([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])\n\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]\n\n\n\n\n0.8200000000000004\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]\n\n\n\n\n0.62000000000000011\n\n\n\nDepartment B:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]\n\n\n\n\n0.67999999999999705\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]\n\n\n\n\n0.63000000000000034\n\n\n\nDepartment C\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]\n\n\n\n\n0.34000000000000008\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]\n\n\n\n\n0.37000000000000005\n\n\n\nDepartment D:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]\n\n\n\n\n0.34999999999999998\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment E\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]\n\n\n\n\n0.23999999999999955\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment F\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]\n\n\n\n\n0.069999999999999701\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nQuestion:\n How many of these departments have a higher probability of admitting women than of admitting men?\n\n\nfor index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value \n 0:\n        print(department_labels[index])\n\n\n\n\nA\nB\nD\nF\n\n\n\nSomehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!\n\n\nTake-away message:\n We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!\n\n\nMore General Story for Conditioning\n\n\nJointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.\n\n\nWe just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "03+Conditioning+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#conditioning-for-random-variables",
            "text": "When we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.  When we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:   Second, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:   Notation:  The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.   In general:  Conditioning:  Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y) 0$, the  conditional probability  of event $X=x$ given event $Y=y$ has happened is   p_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.   For example,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.   Computational interpretation:  To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.",
            "title": "Conditioning for Random Variables"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#exercise-conditioning-for-random-variables",
            "text": "Consider the following two joint probability tables.   Question 1:  What is $p_{W|I}(\\text {sunny}|1)$?  p_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I   {0: 0.5, 1: 0.5}  # p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]  1.0  Question 2:  What is $p_{X|Y}(\\text {sunny}|1)$?  p_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y       {0: 0.5, 1: 0.5}  # p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]  0.5  Question 3:  What is $p_{I|W}(1|\\text {snowy})$?   # p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)  0  Question 4:  What is $p_{Y|X}(1|\\text {snowy})$?  # p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']  0.5",
            "title": "Exercise: Conditioning for Random Variables"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#exercise-simpsons-paradox",
            "text": "This problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.  We have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).  Thus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.  The joint probability table is provided in the file  simpsons_paradox_data.py . Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).  Now let's load in everything from  simpsons_paradox_data.py :  from simpsons_paradox_data import *  Please open up  simpsons_paradox_data.py  to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.  For example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:  joint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]  Some terminology:  In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.  Let's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).  Marginalization is easy to do with NumPy:  joint_prob_gender_admission = joint_prob_table.sum(axis=1)  On the right-hand side, .sum(axis=1)  says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable  joint_prob_gender_admission  stores a 2D joint probability table for random variables $G$ and $A$.  Now, for example, the probability that a woman applies and is admitted is given by:  joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]  Now let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:   \\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}   Let's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:  female_only = joint_prob_gender_admission[gender_mapping['female']]  Now this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!  prob_admission_given_female = female_only / np.sum(female_only)  This is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:  prob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)  from simpsons_paradox_data import *  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  joint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint( Probability of admitted female: {0:.5f} .format(prob_admitted_given_female))  Probability of admitted female: 0.30334  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  male_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint( Probability of admitted male: {0:.5f} .format(prob_admitted_given_male))  Probability of admitted male: 0.44520  So it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.  But before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:  admitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]  Notice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.  Thus, the conditional probability table of gender given admitted is:  prob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)  All right, now let's look at which departments are favoring men over women.  For the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:  female_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]  Now let's determine the probability of getting admitted given each gender and each department.",
            "title": "Exercise: Simpson's Paradox"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#department-a",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  # A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint( Probability of admitted female in department A: {0:.3f} .format(p_A_given_F_and_A))  Probability of admitted female in department A: 0.820",
            "title": "Department A:"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#general-approach",
            "text": "# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep  array([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]  0.8200000000000004  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]  0.62000000000000011",
            "title": "General Approach"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#department-b",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]  0.67999999999999705  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]  0.63000000000000034",
            "title": "Department B:"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#department-c",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]  0.34000000000000008  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]  0.37000000000000005",
            "title": "Department C"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#department-d",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]  0.34999999999999998  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department D:"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#department-e",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]  0.23999999999999955  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department E"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#department-f",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]  0.069999999999999701  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004  Question:  How many of these departments have a higher probability of admitting women than of admitting men?  for index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value   0:\n        print(department_labels[index])  A\nB\nD\nF  Somehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!  Take-away message:  We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!",
            "title": "Department F"
        },
        {
            "location": "/Computational Probability/week01/03+Conditioning+for+Random+Variables/#more-general-story-for-conditioning",
            "text": "Jointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.  We just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "More General Story for Conditioning"
        },
        {
            "location": "/Computational Probability/week01/04+Homework/",
            "text": "Homework Problem: Ice Cream Sales in Inferenceville\n\n\nYou have been hired to investigate a disturbing connection between ice cream sales and crime in Inferenceville. You are given a report that describes the joint distribution over random variable $S$, representing ice cream sales, and random variable $C$, representing crime. Each variable takes on a value of \u201clow\" or \u201chigh\", which we'll represent with $0$ and $1$ respectively. The joint distribution (estimated from data) is as follows:\n\n\n\n\n(a) Are random variables $S$ and $C$ independent?\n\n\nYes\nNo\nunanswered\n\n\n(b) After further investigation, you discover information about the temperature, represented by $T$. This random variable also takes on values $0$ or $1$ corresponding again to \u201clow\" and \u201chigh\". You are able to obtain the conditional distribution $p_{S,C \\mid T}(s,c \\mid t)$, shown below.\n\n\n\n\nAre random variables $S$ and $C$ conditionally independent given $T$?\n\n\nYes\nNo\nunanswered\n\n\n(c) Determine the distribution $p_T$ from the tables above. Express your answer as a Python dictionary. The keys should be the Python integers $0$ and $1$.\n\n\n\n\nNote\n\n\nThis is a good theme. the $A$ adn $\\sum$\n\n\n\n\n\n\noptional explicit title within double quotes\n\n\nAny number of other indented markdown elements.\n\n\nThis is the second paragraph.\n\n\n\n\n\n\nDon't try this at home\n\n\n...\n\n\n\n\n\n\nThis is a admonition box without a title.",
            "title": "04+Homework"
        },
        {
            "location": "/Computational Probability/week01/Linear Algebra December 2015 Questions(Part B)/",
            "text": "Question 1:\n If $A$ is a $5\\times 5$ real matrix with trace $15$ and if $2$ and $3$ are eigenvalues of $A$ , each with algebraic multiplicity $2$, then the determinant of $A$ is equal to\n\n\n\n\n$0$\n\n\n$24$\n\n\n$120$\n\n\n$180$\n\n\n\n\nAnswer:\n Out of five eigenvalues of matrix $A$ we know four $2,~2,~3,~3$. Let the last eigenvalue is $\\lambda$. Then,\n\n\\begin{align}& \\text{ trace }A = 2+2+3+3+\\lambda\\\\ \\Rightarrow& \\lambda = 15-10=5.\\end{align}\n\nHence the determinant, will be product of eigenvalues\n\n\\text{ det }A = 2\\cdot 2\\cdot 3\\cdot 3\\cdot 5=180.\n\n\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 2:\n For a positive integer $n$, let $P_n$ denote the vector space of polynomials in one variable $x$ with real coefficients and with degree $\\leq n$. Consider the map $T:P_2 \\rightarrow P_4$ defined by $T(p(x)) = p(x^2)$. Then\n\n\n\n\n$T$ is a linear transformation and dim range $(T) = 5$.\n\n\n$T$ is a linear transformation and dim range $(T) = 3$.\n\n\n$T$ is a linear transformation and dim range $(T) = 2$.\n\n\n$T$ is not a linear transformation.\n\n\n\n\nAnswer:\n This transformation replaces each $x$ by $x^2$. Also,\n\n\\begin{align}T(\\alpha p(x) + \\beta q(x)) &= \\alpha p(x^2) + \\beta q(x^2)\\\\ &=\\alpha T(p(x)) + \\beta T(q(x))\\end{align}\n\nHence this is a linear transformation, and\n\n\\text{ ker }T = \\{p(x) : p(x^2) = 0\\} = \\{0\\}.\n\nHence the nullity will be zero. By rank nullity theorem, we get\n\n\n\\begin{align}\n\\text{ rank }T &= \\text{ dim }P_2 - \\text{ nullity }\\\\\n&= 3 - 0 \\\\\n&= 3\n\\end{align}\n\n\n\n\n\nHence \n2\n is correct choice.\n\n\n\n\nQuestion 3:\n Let $A$ be a real $3\\times 4$ matrix of rank $2$. Then the rank of $A^tA$, where $A^t$ deonotes the transpose $A$, is\n\n\n\n\nexactly $2$\n\n\nexactly $3$\n\n\nexactly $4$\n\n\nat most $2$ but not necessarily $2$\n\n\n\n\nAnswer:\n Given that the rank of $A$ is $2$, hence the nullity of $A$ is $4-2 = 2$. Using theorem,\n\n\n\n\nNullspace of $A$ and $A^TA$ are the same.\n\n\n\n\nWe get the nullity of $A^TA$ is 2. Since $A^TA$ is $4\\times 4$ matrix. Hence rank of $A^TA$ will be $4-2 = 2$.\n\n\nHence \n1\n is correct choice.\n\n\n\n\nQuestion 4:\n  Let $S$ denote the set of all the prime numbers $p$ with the property that the matrix\n\n\n\\pmatrix{91& 31 & 0\\\\ 29 & 31 & 0\\\\ 79 & 23 & 59}\n\n\nhas an inverse in the field $\\newcommand{\\Z}{\\mathbb{Z}}\\Z/p\\Z$. Then\n\n\n\n\n$S = {31}$\n\n\n$S = {31, ~59}$\n\n\n$S = {7, ~13, ~59}$\n\n\n$S$ is infinite\n\n\n\n\nAnswer:\n The matrix is singular, if it's determinant will be zero in that field.\n\n\\begin{align}\\text{ det } &= 59\\times 31(91-29) \\\\ &= 59\\times 31 \\times  31 \\times 2.\\end{align}\n\nThe determinant will be zero in $\\Z_p$ only for $p=59, ~31, ~2$. For all other primes it will be non-zero.\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 5:\n Consider the quadratic form $Q(v) = v^tAv$, where\n\nA = \\pmatrix{1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&0&1}, v= (x,y,z,w)\n\nThen\n\n\n\n\n$Q$ has rank $3$.\n\n\n$xy+z^2 = Q(P(v))$ for some invertible $4 \\times 4$ real matrix $P$.\n\n\n$xy+y^2+ z^2 = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n$x^2+y^2-zw = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n\n\nAnswer:\n Here,\n\nQ(Pv) = (Pv)^tAPv = v^t(P^tAP)v \n\nWhen $P$ is orthogonal, $A$ and $P^tAP$ both represent equivalent quadratic forms. Then the problem is reduced to find equivalent bilinear forms.\n\n\n\n\nQuestion 6:\n Let $A \\neq I_n$ be an $n \\times n$ matrix such that $A^2 = A$, where $I_n$ is the identity matrix of order $n$. Which of the following statements is false?\n\n\n\n\n$(I_n - A)^2 = I_n - A$.\n\n\ntrace $(A)=$ rank $(A)$.\n\n\nrank $(A)$ + rank $(I_n-A)$ = n.\n\n\nThe eigenvalus of $A$ are each equal to $1$.\n\n\n\n\nAnswer:\n The matrix satisfies the polynomial $x^2 - x = 0\\Rightarrow x(x-1) = 0$. Hence minimal polynomial is a factor of $x(x-1)$. Since this polynomial has linear factors, hence matrix is diagonalizable with eigenvalues $0$ or $1$ or both. Since $A \\neq I$ then at least eigenvalues will be $0$.\n\n\nHence \n4\n is correct choice.",
            "title": "Linear Algebra December 2015 Questions(Part B)"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01/chess/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01_2/01 Simulating Coin Filps/",
            "text": "Simulating Coin Filps\n\n\n%matplotlib inline\n\n\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nflip_fair_coin()\n\n\n\n\n'heads'\n\n\n\nflips = flip_fair_coins(100)\nplot_discrete_histogram(flips)\n\n\n\n\n\n\nplot_discrete_histogram(flip_fair_coins(10), frequency=True)\n\n\n\n\n\n\nn = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "01 Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01_2/01 Simulating Coin Filps/#simulating-coin-filps",
            "text": "%matplotlib inline  import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *  flip_fair_coin()  'heads'  flips = flip_fair_coins(100)\nplot_discrete_histogram(flips)   plot_discrete_histogram(flip_fair_coins(10), frequency=True)   n = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))  import matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/",
            "text": "Introduction to Independence\n\n\nWith a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?\n\n\nThe answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".\n\n\nWe have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.\n\n\nNot only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.\n\n\nIndependet Events\n\n\nTwo events $A$ and $B$ are independet denoted by $A \\perp B$ if\n\n\n\n\n\\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)\n\n\n\n\nExample:\n If we toss two coin then probability of heads up is multiple of probability of heads for each coin. \n\n\nIn terms of conditional probability we \n\n \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align}\n\nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.\n\n\nExercise: Bernoulli and Bin\n\n\nThis problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.\n\n\nThese two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!\n\n\nAs mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.\n\n\nA Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.\n\n\n(a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.\n\n\nAnswer:\n FALSE\n\n\n(b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nsequence = \nHTHTTTTTHH\n\nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob    \n\n\n\n\n0.0005308416000000001\n\n\n\n(c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?\n\n\n[$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails. \n\n[$\\times    $] The probability is different depending on the ordering of heads and tails.\n\n\n(d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)\n\n\ndef ncr(n, r):\n    \n\n    If calculates the n choose r for n \n= r.\n\n    \n ncr(10, 4)\n    210.0\n\n    \n ncr(4, 4)\n    1.0\n    \n\n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ == \n__main__\n:\n    import doctest \n    doctest.testmod()\n\n\n\n\nncr(10, 4)\n\n\n\n\n210.0\n\n\n\n(e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprob * ncr(10, 4)\n\n\n\n\n0.11147673600000002\n\n\n\nIn general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.\n\n\nPlease be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.\n\n\nExercise: The Soda Machine\n\n\n3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.\n\n\n(a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/7)**14\n\n\n\n\n2.4157243620710218e-08\n\n\n\n(b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/5)**14\n\n\n\n\n2.684354560000002e-06\n\n\n\n(c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/3)**14\n\n\n\n\n0.003425487390781748\n\n\n\nExercise: Gambler's Fallacy\n\n\nSuppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.\n\n\n(a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**100\n\n\n\n\n0.9770407138326136\n\n\n\n(b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**99\n\n\n\n\n0.9761576643646371\n\n\n\n(c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\n1 - (26/27)**(100-n)\n\n\n\n\n0.9770407138326136\n\n\n\n(d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?\n\n\n[$\\checkmark$] Probability decreases as $n$ increases \n\n[$\\times    $] Probability increases as $n$ increases\n\n\nimport matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel(\nNo of trials without getting $27$\n)\nplt.ylabel(\nProb of getting $27$\n)\nplt.show()\n\n\n\n\n\n\nIndependet Random Variable\n\n\nTwo random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by \n\n\n\n\np_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y\n\n\n\n\nIndepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability \n\n\n\n\np_{X\\mid Y}(x\\mid y) = p_X(x) \n\n\n\n\nExercise: Independent Random Variables\n\n\nIn this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.\n\n\nConsider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.\n\n\n\n\nIn Python:\n\n\nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\n\n\n\n\nNote that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.\n\n\nWe can get the marginal distributions $p_W$ and $p_I$:\n\n\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n\n\n\n\nThen if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:\n\n\n\n\np_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.\n\n\n\n\nNote that variables \nprob_W\n and \nprob_I\n at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.\n\n\nWe could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of \nprob_W\n and \nprob_I\n yields what the joint distribution would be if $W$ and $I$ were independent:\n\n\n\n\n\\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}\n\n\n\n\nThe left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.\n\n\nTo compute and print the right-hand side, we do:\n\n\nprint(np.outer(prob_W, prob_I))\n\n\n\n\nQuestion:\n Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?\n\n\nAnswer:\n FALSE\n\n\nfrom numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)\n\n\n\n\n0.5\n\n\n\nQuestion:\n Are X and Y independent?\n\n\nAnswer:\n TRUE\n\n\nprob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)\n\n\n\n\n0.0\n\n\n\nMutual and Pairwise Independence\n\n\nThree random variable $X, Y$ and $Z$ are \nmutually independent\n if \n\n\n\n\np_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z) \n\n\n\n\nThree random variable $X,Y$ and $Z$ are \npairwise independence\n if \n\n\n\n\np_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J\n\n\n\n\nThroughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.\n\n\nExercise: Mutual vs Pairwise Independence\n\n\nSuppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise. \nIn this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution\n (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).\n\n\nSuppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.\n\n\nSelect all of the following that are true:\n\n\n[$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same. \n\n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same. \n\n[$\\times$] $X, Y$, and $Z$ are pairwise independent.\n\n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "02 Independence Structure"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#introduction-to-independence",
            "text": "With a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?  The answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".  We have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.  Not only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.",
            "title": "Introduction to Independence"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#independet-events",
            "text": "Two events $A$ and $B$ are independet denoted by $A \\perp B$ if   \\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)   Example:  If we toss two coin then probability of heads up is multiple of probability of heads for each coin.   In terms of conditional probability we   \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align} \nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.",
            "title": "Independet Events"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#exercise-bernoulli-and-bin",
            "text": "This problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.  These two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!  As mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.  A Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.  (a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.  Answer:  FALSE  (b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  sequence =  HTHTTTTTHH \nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob      0.0005308416000000001  (c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?  [$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails.  \n[$\\times    $] The probability is different depending on the ordering of heads and tails.  (d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)  def ncr(n, r):\n     \n    If calculates the n choose r for n  = r.\n\n      ncr(10, 4)\n    210.0\n\n      ncr(4, 4)\n    1.0\n     \n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ ==  __main__ :\n    import doctest \n    doctest.testmod()  ncr(10, 4)  210.0  (e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prob * ncr(10, 4)  0.11147673600000002  In general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.  Please be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.",
            "title": "Exercise: Bernoulli and Bin"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#exercise-the-soda-machine",
            "text": "3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.  (a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/7)**14  2.4157243620710218e-08  (b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/5)**14  2.684354560000002e-06  (c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/3)**14  0.003425487390781748",
            "title": "Exercise: The Soda Machine"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#exercise-gamblers-fallacy",
            "text": "Suppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.  (a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**100  0.9770407138326136  (b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**99  0.9761576643646371  (c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  1 - (26/27)**(100-n)  0.9770407138326136  (d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?  [$\\checkmark$] Probability decreases as $n$ increases  \n[$\\times    $] Probability increases as $n$ increases  import matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel( No of trials without getting $27$ )\nplt.ylabel( Prob of getting $27$ )\nplt.show()",
            "title": "Exercise: Gambler's Fallacy"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#independet-random-variable",
            "text": "Two random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by    p_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y   Indepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability    p_{X\\mid Y}(x\\mid y) = p_X(x)",
            "title": "Independet Random Variable"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#exercise-independent-random-variables",
            "text": "In this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.  Consider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.   In Python:  prob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])  Note that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.  We can get the marginal distributions $p_W$ and $p_I$:  prob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)  Then if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:   p_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.   Note that variables  prob_W  and  prob_I  at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.  We could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of  prob_W  and  prob_I  yields what the joint distribution would be if $W$ and $I$ were independent:   \\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}   The left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.  To compute and print the right-hand side, we do:  print(np.outer(prob_W, prob_I))  Question:  Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?  Answer:  FALSE  from numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)  0.5  Question:  Are X and Y independent?  Answer:  TRUE  prob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)  0.0",
            "title": "Exercise: Independent Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#mutual-and-pairwise-independence",
            "text": "Three random variable $X, Y$ and $Z$ are  mutually independent  if    p_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z)    Three random variable $X,Y$ and $Z$ are  pairwise independence  if    p_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J   Throughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.",
            "title": "Mutual and Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01_2/02 Independence Structure/#exercise-mutual-vs-pairwise-independence",
            "text": "Suppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise.  In this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution  (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).  Suppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.  Select all of the following that are true:  [$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same.  \n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same.  \n[$\\times$] $X, Y$, and $Z$ are pairwise independent. \n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "Exercise: Mutual vs Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/",
            "text": "Conditioning for Random Variables\n\n\nWhen we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.\n\n\nWhen we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:\n\n\n\n\nSecond, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:\n\n\n\n\nNotation:\n The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.\n\n\n\n\nIn general:\n\n\nConditioning:\n Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y)\n0$, the \nconditional probability\n of event $X=x$ given event $Y=y$ has happened is\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.\n\n\n\n\nFor example,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.\n\n\n\n\nComputational interpretation:\n To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.\n\n\nExercise: Conditioning for Random Variables\n\n\nConsider the following two joint probability tables.\n\n\n\n\nQuestion 1:\n What is $p_{W|I}(\\text {sunny}|1)$?\n\n\np_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]\n\n\n\n\n1.0\n\n\n\nQuestion 2:\n What is $p_{X|Y}(\\text {sunny}|1)$?\n\n\np_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y     \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]\n\n\n\n\n0.5\n\n\n\nQuestion 3:\n What is $p_{I|W}(1|\\text {snowy})$? \n\n\n# p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)\n\n\n\n\n0\n\n\n\nQuestion 4:\n What is $p_{Y|X}(1|\\text {snowy})$?\n\n\n# p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']\n\n\n\n\n0.5\n\n\n\nExercise: Simpson's Paradox\n\n\nThis problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.\n\n\nWe have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).\n\n\nThus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.\n\n\nThe joint probability table is provided in the file \nsimpsons_paradox_data.py\n. Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).\n\n\nNow let's load in everything from \nsimpsons_paradox_data.py\n:\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nPlease open up \nsimpsons_paradox_data.py\n to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.\n\n\nFor example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:\n\n\njoint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]\n\n\n\n\nSome terminology:\n In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.\n\n\nLet's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).\n\n\nMarginalization is easy to do with NumPy:\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\n\n\n\n\nOn the right-hand side,\n.sum(axis=1)\n says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable \njoint_prob_gender_admission\n stores a 2D joint probability table for random variables $G$ and $A$.\n\n\nNow, for example, the probability that a woman applies and is admitted is given by:\n\n\njoint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\n\n\n\n\nNow let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:\n\n\n\n\n\\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}\n\n\n\n\nLet's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:\n\n\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\n\n\n\n\nNow this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!\n\n\nprob_admission_given_female = female_only / np.sum(female_only)\n\n\n\n\nThis is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:\n\n\nprob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)\n\n\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint(\nProbability of admitted female: {0:.5f}\n.format(prob_admitted_given_female))\n\n\n\n\nProbability of admitted female: 0.30334\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nmale_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint(\nProbability of admitted male: {0:.5f}\n.format(prob_admitted_given_male))\n\n\n\n\nProbability of admitted male: 0.44520\n\n\n\nSo it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.\n\n\nBut before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:\n\n\nadmitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]\n\n\n\n\nNotice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.\n\n\nThus, the conditional probability table of gender given admitted is:\n\n\nprob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)\n\n\n\n\nAll right, now let's look at which departments are favoring men over women.\n\n\nFor the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:\n\n\nfemale_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]\n\n\n\n\nNow let's determine the probability of getting admitted given each gender and each department.\n\n\nDepartment A:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\n# A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint(\nProbability of admitted female in department A: {0:.3f}\n.format(p_A_given_F_and_A))\n\n\n\n\nProbability of admitted female in department A: 0.820\n\n\n\nGeneral Approach\n\n\n# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep\n\n\n\n\narray([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])\n\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]\n\n\n\n\n0.8200000000000004\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]\n\n\n\n\n0.62000000000000011\n\n\n\nDepartment B:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]\n\n\n\n\n0.67999999999999705\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]\n\n\n\n\n0.63000000000000034\n\n\n\nDepartment C\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]\n\n\n\n\n0.34000000000000008\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]\n\n\n\n\n0.37000000000000005\n\n\n\nDepartment D:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]\n\n\n\n\n0.34999999999999998\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment E\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]\n\n\n\n\n0.23999999999999955\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment F\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]\n\n\n\n\n0.069999999999999701\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nQuestion:\n How many of these departments have a higher probability of admitting women than of admitting men?\n\n\nfor index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value \n 0:\n        print(department_labels[index])\n\n\n\n\nA\nB\nD\nF\n\n\n\nSomehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!\n\n\nTake-away message:\n We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!\n\n\nMore General Story for Conditioning\n\n\nJointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.\n\n\nWe just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "03+Conditioning+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#conditioning-for-random-variables",
            "text": "When we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.  When we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:   Second, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:   Notation:  The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.   In general:  Conditioning:  Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y) 0$, the  conditional probability  of event $X=x$ given event $Y=y$ has happened is   p_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.   For example,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.   Computational interpretation:  To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.",
            "title": "Conditioning for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#exercise-conditioning-for-random-variables",
            "text": "Consider the following two joint probability tables.   Question 1:  What is $p_{W|I}(\\text {sunny}|1)$?  p_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I   {0: 0.5, 1: 0.5}  # p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]  1.0  Question 2:  What is $p_{X|Y}(\\text {sunny}|1)$?  p_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y       {0: 0.5, 1: 0.5}  # p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]  0.5  Question 3:  What is $p_{I|W}(1|\\text {snowy})$?   # p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)  0  Question 4:  What is $p_{Y|X}(1|\\text {snowy})$?  # p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']  0.5",
            "title": "Exercise: Conditioning for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#exercise-simpsons-paradox",
            "text": "This problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.  We have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).  Thus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.  The joint probability table is provided in the file  simpsons_paradox_data.py . Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).  Now let's load in everything from  simpsons_paradox_data.py :  from simpsons_paradox_data import *  Please open up  simpsons_paradox_data.py  to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.  For example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:  joint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]  Some terminology:  In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.  Let's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).  Marginalization is easy to do with NumPy:  joint_prob_gender_admission = joint_prob_table.sum(axis=1)  On the right-hand side, .sum(axis=1)  says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable  joint_prob_gender_admission  stores a 2D joint probability table for random variables $G$ and $A$.  Now, for example, the probability that a woman applies and is admitted is given by:  joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]  Now let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:   \\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}   Let's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:  female_only = joint_prob_gender_admission[gender_mapping['female']]  Now this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!  prob_admission_given_female = female_only / np.sum(female_only)  This is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:  prob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)  from simpsons_paradox_data import *  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  joint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint( Probability of admitted female: {0:.5f} .format(prob_admitted_given_female))  Probability of admitted female: 0.30334  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  male_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint( Probability of admitted male: {0:.5f} .format(prob_admitted_given_male))  Probability of admitted male: 0.44520  So it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.  But before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:  admitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]  Notice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.  Thus, the conditional probability table of gender given admitted is:  prob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)  All right, now let's look at which departments are favoring men over women.  For the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:  female_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]  Now let's determine the probability of getting admitted given each gender and each department.",
            "title": "Exercise: Simpson's Paradox"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#department-a",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  # A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint( Probability of admitted female in department A: {0:.3f} .format(p_A_given_F_and_A))  Probability of admitted female in department A: 0.820",
            "title": "Department A:"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#general-approach",
            "text": "# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep  array([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]  0.8200000000000004  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]  0.62000000000000011",
            "title": "General Approach"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#department-b",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]  0.67999999999999705  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]  0.63000000000000034",
            "title": "Department B:"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#department-c",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]  0.34000000000000008  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]  0.37000000000000005",
            "title": "Department C"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#department-d",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]  0.34999999999999998  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department D:"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#department-e",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]  0.23999999999999955  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department E"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#department-f",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]  0.069999999999999701  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004  Question:  How many of these departments have a higher probability of admitting women than of admitting men?  for index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value   0:\n        print(department_labels[index])  A\nB\nD\nF  Somehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!  Take-away message:  We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!",
            "title": "Department F"
        },
        {
            "location": "/Computational Probability/week01_2/03+Conditioning+for+Random+Variables/#more-general-story-for-conditioning",
            "text": "Jointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.  We just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "More General Story for Conditioning"
        },
        {
            "location": "/Computational Probability/week01_2/04+Homework/",
            "text": "Homework Problem: Ice Cream Sales in Inferenceville\n\n\nYou have been hired to investigate a disturbing connection between ice cream sales and crime in Inferenceville. You are given a report that describes the joint distribution over random variable $S$, representing ice cream sales, and random variable $C$, representing crime. Each variable takes on a value of \u201clow\" or \u201chigh\", which we'll represent with $0$ and $1$ respectively. The joint distribution (estimated from data) is as follows:\n\n\n\n\n(a) Are random variables $S$ and $C$ independent?\n\n\nYes\nNo\nunanswered\n\n\n(b) After further investigation, you discover information about the temperature, represented by $T$. This random variable also takes on values $0$ or $1$ corresponding again to \u201clow\" and \u201chigh\". You are able to obtain the conditional distribution $p_{S,C \\mid T}(s,c \\mid t)$, shown below.\n\n\n\n\nAre random variables $S$ and $C$ conditionally independent given $T$?\n\n\nYes\nNo\nunanswered\n\n\n(c) Determine the distribution $p_T$ from the tables above. Express your answer as a Python dictionary. The keys should be the Python integers $0$ and $1$.\n\n\n\n\nNote\n\n\nThis is a good theme. the $A$ adn $\\sum$",
            "title": "04+Homework"
        },
        {
            "location": "/Computational Probability/week01_2/Linear Algebra December 2015 Questions(Part B)/",
            "text": "Question 1:\n If $A$ is a $5\\times 5$ real matrix with trace $15$ and if $2$ and $3$ are eigenvalues of $A$ , each with algebraic multiplicity $2$, then the determinant of $A$ is equal to\n\n\n\n\n$0$\n\n\n$24$\n\n\n$120$\n\n\n$180$\n\n\n\n\nAnswer:\n Out of five eigenvalues of matrix $A$ we know four $2,~2,~3,~3$. Let the last eigenvalue is $\\lambda$. Then,\n\n\\begin{align}& \\text{ trace }A = 2+2+3+3+\\lambda\\\\ \\Rightarrow& \\lambda = 15-10=5.\\end{align}\n\nHence the determinant, will be product of eigenvalues\n\n\\text{ det }A = 2\\cdot 2\\cdot 3\\cdot 3\\cdot 5=180.\n\n\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 2:\n For a positive integer $n$, let $P_n$ denote the vector space of polynomials in one variable $x$ with real coefficients and with degree $\\leq n$. Consider the map $T:P_2 \\rightarrow P_4$ defined by $T(p(x)) = p(x^2)$. Then\n\n\n\n\n$T$ is a linear transformation and dim range $(T) = 5$.\n\n\n$T$ is a linear transformation and dim range $(T) = 3$.\n\n\n$T$ is a linear transformation and dim range $(T) = 2$.\n\n\n$T$ is not a linear transformation.\n\n\n\n\nAnswer:\n This transformation replaces each $x$ by $x^2$. Also,\n\n\\begin{align}T(\\alpha p(x) + \\beta q(x)) &= \\alpha p(x^2) + \\beta q(x^2)\\\\ &=\\alpha T(p(x)) + \\beta T(q(x))\\end{align}\n\nHence this is a linear transformation, and\n\n\\text{ ker }T = \\{p(x) : p(x^2) = 0\\} = \\{0\\}.\n\nHence the nullity will be zero. By rank nullity theorem, we get\n\n\n\\begin{align}\n\\text{ rank }T &= \\text{ dim }P_2 - \\text{ nullity }\\\\\n&= 3 - 0 \\\\\n&= 3\n\\end{align}\n\n\n\n\n\nHence \n2\n is correct choice.\n\n\n\n\nQuestion 3:\n Let $A$ be a real $3\\times 4$ matrix of rank $2$. Then the rank of $A^tA$, where $A^t$ deonotes the transpose $A$, is\n\n\n\n\nexactly $2$\n\n\nexactly $3$\n\n\nexactly $4$\n\n\nat most $2$ but not necessarily $2$\n\n\n\n\nAnswer:\n Given that the rank of $A$ is $2$, hence the nullity of $A$ is $4-2 = 2$. Using theorem,\n\n\n\n\nNullspace of $A$ and $A^TA$ are the same.\n\n\n\n\nWe get the nullity of $A^TA$ is 2. Since $A^TA$ is $4\\times 4$ matrix. Hence rank of $A^TA$ will be $4-2 = 2$.\n\n\nHence \n1\n is correct choice.\n\n\n\n\nQuestion 4:\n  Let $S$ denote the set of all the prime numbers $p$ with the property that the matrix\n\n\n\\pmatrix{91& 31 & 0\\\\ 29 & 31 & 0\\\\ 79 & 23 & 59}\n\n\nhas an inverse in the field $\\newcommand{\\Z}{\\mathbb{Z}}\\Z/p\\Z$. Then\n\n\n\n\n$S = {31}$\n\n\n$S = {31, ~59}$\n\n\n$S = {7, ~13, ~59}$\n\n\n$S$ is infinite\n\n\n\n\nAnswer:\n The matrix is singular, if it's determinant will be zero in that field.\n\n\\begin{align}\\text{ det } &= 59\\times 31(91-29) \\\\ &= 59\\times 31 \\times  31 \\times 2.\\end{align}\n\nThe determinant will be zero in $\\Z_p$ only for $p=59, ~31, ~2$. For all other primes it will be non-zero.\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 5:\n Consider the quadratic form $Q(v) = v^tAv$, where\n\nA = \\pmatrix{1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&0&1}, v= (x,y,z,w)\n\nThen\n\n\n\n\n$Q$ has rank $3$.\n\n\n$xy+z^2 = Q(P(v))$ for some invertible $4 \\times 4$ real matrix $P$.\n\n\n$xy+y^2+ z^2 = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n$x^2+y^2-zw = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n\n\nAnswer:\n Here,\n\nQ(Pv) = (Pv)^tAPv = v^t(P^tAP)v \n\nWhen $P$ is orthogonal, $A$ and $P^tAP$ both represent equivalent quadratic forms. Then the problem is reduced to find equivalent bilinear forms.\n\n\n\n\nQuestion 6:\n Let $A \\neq I_n$ be an $n \\times n$ matrix such that $A^2 = A$, where $I_n$ is the identity matrix of order $n$. Which of the following statements is false?\n\n\n\n\n$(I_n - A)^2 = I_n - A$.\n\n\ntrace $(A)=$ rank $(A)$.\n\n\nrank $(A)$ + rank $(I_n-A)$ = n.\n\n\nThe eigenvalus of $A$ are each equal to $1$.\n\n\n\n\nAnswer:\n The matrix satisfies the polynomial $x^2 - x = 0\\Rightarrow x(x-1) = 0$. Hence minimal polynomial is a factor of $x(x-1)$. Since this polynomial has linear factors, hence matrix is diagonalizable with eigenvalues $0$ or $1$ or both. Since $A \\neq I$ then at least eigenvalues will be $0$.\n\n\nHence \n4\n is correct choice.",
            "title": "Linear Algebra December 2015 Questions(Part B)"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01_2/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01_2/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01_2/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01_2/chess/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01_3/01 Simulating Coin Filps/",
            "text": "Simulating Coin Filps\n\n\n%matplotlib inline\n\n\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nflip_fair_coin()\n\n\n\n\n'heads'\n\n\n\nflips = flip_fair_coins(100)\nplot_discrete_histogram(flips)\n\n\n\n\n\n\nplot_discrete_histogram(flip_fair_coins(10), frequency=True)\n\n\n\n\n\n\nn = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "01 Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01_3/01 Simulating Coin Filps/#simulating-coin-filps",
            "text": "%matplotlib inline  import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *  flip_fair_coin()  'heads'  flips = flip_fair_coins(100)\nplot_discrete_histogram(flips)   plot_discrete_histogram(flip_fair_coins(10), frequency=True)   n = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))  import matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/",
            "text": "Introduction to Independence\n\n\nWith a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?\n\n\nThe answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".\n\n\nWe have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.\n\n\nNot only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.\n\n\nIndependet Events\n\n\nTwo events $A$ and $B$ are independet denoted by $A \\perp B$ if\n\n\n\n\n\\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)\n\n\n\n\nExample:\n If we toss two coin then probability of heads up is multiple of probability of heads for each coin. \n\n\nIn terms of conditional probability we \n\n \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align}\n\nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.\n\n\nExercise: Bernoulli and Bin\n\n\nThis problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.\n\n\nThese two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!\n\n\nAs mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.\n\n\nA Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.\n\n\n(a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.\n\n\nAnswer:\n FALSE\n\n\n(b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nsequence = \nHTHTTTTTHH\n\nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob    \n\n\n\n\n0.0005308416000000001\n\n\n\n(c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?\n\n\n[$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails. \n\n[$\\times    $] The probability is different depending on the ordering of heads and tails.\n\n\n(d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)\n\n\ndef ncr(n, r):\n    \n\n    If calculates the n choose r for n \n= r.\n\n    \n ncr(10, 4)\n    210.0\n\n    \n ncr(4, 4)\n    1.0\n    \n\n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ == \n__main__\n:\n    import doctest \n    doctest.testmod()\n\n\n\n\nncr(10, 4)\n\n\n\n\n210.0\n\n\n\n(e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprob * ncr(10, 4)\n\n\n\n\n0.11147673600000002\n\n\n\nIn general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.\n\n\nPlease be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.\n\n\nExercise: The Soda Machine\n\n\n3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.\n\n\n(a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/7)**14\n\n\n\n\n2.4157243620710218e-08\n\n\n\n(b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/5)**14\n\n\n\n\n2.684354560000002e-06\n\n\n\n(c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/3)**14\n\n\n\n\n0.003425487390781748\n\n\n\nExercise: Gambler's Fallacy\n\n\nSuppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.\n\n\n(a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**100\n\n\n\n\n0.9770407138326136\n\n\n\n(b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**99\n\n\n\n\n0.9761576643646371\n\n\n\n(c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\n1 - (26/27)**(100-n)\n\n\n\n\n0.9770407138326136\n\n\n\n(d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?\n\n\n[$\\checkmark$] Probability decreases as $n$ increases \n\n[$\\times    $] Probability increases as $n$ increases\n\n\nimport matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel(\nNo of trials without getting $27$\n)\nplt.ylabel(\nProb of getting $27$\n)\nplt.show()\n\n\n\n\n\n\nIndependet Random Variable\n\n\nTwo random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by \n\n\n\n\np_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y\n\n\n\n\nIndepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability \n\n\n\n\np_{X\\mid Y}(x\\mid y) = p_X(x) \n\n\n\n\nExercise: Independent Random Variables\n\n\nIn this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.\n\n\nConsider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.\n\n\n\n\nIn Python:\n\n\nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\n\n\n\n\nNote that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.\n\n\nWe can get the marginal distributions $p_W$ and $p_I$:\n\n\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n\n\n\n\nThen if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:\n\n\n\n\np_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.\n\n\n\n\nNote that variables \nprob_W\n and \nprob_I\n at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.\n\n\nWe could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of \nprob_W\n and \nprob_I\n yields what the joint distribution would be if $W$ and $I$ were independent:\n\n\n\n\n\\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}\n\n\n\n\nThe left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.\n\n\nTo compute and print the right-hand side, we do:\n\n\nprint(np.outer(prob_W, prob_I))\n\n\n\n\nQuestion:\n Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?\n\n\nAnswer:\n FALSE\n\n\nfrom numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)\n\n\n\n\n0.5\n\n\n\nQuestion:\n Are X and Y independent?\n\n\nAnswer:\n TRUE\n\n\nprob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)\n\n\n\n\n0.0\n\n\n\nMutual and Pairwise Independence\n\n\nThree random variable $X, Y$ and $Z$ are \nmutually independent\n if \n\n\n\n\np_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z) \n\n\n\n\nThree random variable $X,Y$ and $Z$ are \npairwise independence\n if \n\n\n\n\np_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J\n\n\n\n\nThroughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.\n\n\nExercise: Mutual vs Pairwise Independence\n\n\nSuppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise. \nIn this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution\n (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).\n\n\nSuppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.\n\n\nSelect all of the following that are true:\n\n\n[$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same. \n\n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same. \n\n[$\\times$] $X, Y$, and $Z$ are pairwise independent.\n\n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "02 Independence Structure"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#introduction-to-independence",
            "text": "With a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?  The answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".  We have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.  Not only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.",
            "title": "Introduction to Independence"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#independet-events",
            "text": "Two events $A$ and $B$ are independet denoted by $A \\perp B$ if   \\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)   Example:  If we toss two coin then probability of heads up is multiple of probability of heads for each coin.   In terms of conditional probability we   \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align} \nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.",
            "title": "Independet Events"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#exercise-bernoulli-and-bin",
            "text": "This problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.  These two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!  As mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.  A Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.  (a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.  Answer:  FALSE  (b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  sequence =  HTHTTTTTHH \nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob      0.0005308416000000001  (c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?  [$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails.  \n[$\\times    $] The probability is different depending on the ordering of heads and tails.  (d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)  def ncr(n, r):\n     \n    If calculates the n choose r for n  = r.\n\n      ncr(10, 4)\n    210.0\n\n      ncr(4, 4)\n    1.0\n     \n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ ==  __main__ :\n    import doctest \n    doctest.testmod()  ncr(10, 4)  210.0  (e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prob * ncr(10, 4)  0.11147673600000002  In general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.  Please be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.",
            "title": "Exercise: Bernoulli and Bin"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#exercise-the-soda-machine",
            "text": "3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.  (a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/7)**14  2.4157243620710218e-08  (b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/5)**14  2.684354560000002e-06  (c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/3)**14  0.003425487390781748",
            "title": "Exercise: The Soda Machine"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#exercise-gamblers-fallacy",
            "text": "Suppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.  (a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**100  0.9770407138326136  (b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**99  0.9761576643646371  (c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  1 - (26/27)**(100-n)  0.9770407138326136  (d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?  [$\\checkmark$] Probability decreases as $n$ increases  \n[$\\times    $] Probability increases as $n$ increases  import matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel( No of trials without getting $27$ )\nplt.ylabel( Prob of getting $27$ )\nplt.show()",
            "title": "Exercise: Gambler's Fallacy"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#independet-random-variable",
            "text": "Two random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by    p_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y   Indepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability    p_{X\\mid Y}(x\\mid y) = p_X(x)",
            "title": "Independet Random Variable"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#exercise-independent-random-variables",
            "text": "In this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.  Consider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.   In Python:  prob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])  Note that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.  We can get the marginal distributions $p_W$ and $p_I$:  prob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)  Then if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:   p_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.   Note that variables  prob_W  and  prob_I  at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.  We could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of  prob_W  and  prob_I  yields what the joint distribution would be if $W$ and $I$ were independent:   \\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}   The left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.  To compute and print the right-hand side, we do:  print(np.outer(prob_W, prob_I))  Question:  Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?  Answer:  FALSE  from numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)  0.5  Question:  Are X and Y independent?  Answer:  TRUE  prob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)  0.0",
            "title": "Exercise: Independent Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#mutual-and-pairwise-independence",
            "text": "Three random variable $X, Y$ and $Z$ are  mutually independent  if    p_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z)    Three random variable $X,Y$ and $Z$ are  pairwise independence  if    p_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J   Throughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.",
            "title": "Mutual and Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01_3/02 Independence Structure/#exercise-mutual-vs-pairwise-independence",
            "text": "Suppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise.  In this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution  (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).  Suppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.  Select all of the following that are true:  [$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same.  \n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same.  \n[$\\times$] $X, Y$, and $Z$ are pairwise independent. \n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "Exercise: Mutual vs Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/",
            "text": "Conditioning for Random Variables\n\n\nWhen we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.\n\n\nWhen we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:\n\n\n\n\nSecond, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:\n\n\n\n\nNotation:\n The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.\n\n\n\n\nIn general:\n\n\nConditioning:\n Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y)\n0$, the \nconditional probability\n of event $X=x$ given event $Y=y$ has happened is\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.\n\n\n\n\nFor example,\n\n\n\n\np_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.\n\n\n\n\nComputational interpretation:\n To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.\n\n\nExercise: Conditioning for Random Variables\n\n\nConsider the following two joint probability tables.\n\n\n\n\nQuestion 1:\n What is $p_{W|I}(\\text {sunny}|1)$?\n\n\np_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]\n\n\n\n\n1.0\n\n\n\nQuestion 2:\n What is $p_{X|Y}(\\text {sunny}|1)$?\n\n\np_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y     \n\n\n\n\n{0: 0.5, 1: 0.5}\n\n\n\n# p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]\n\n\n\n\n0.5\n\n\n\nQuestion 3:\n What is $p_{I|W}(1|\\text {snowy})$? \n\n\n# p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)\n\n\n\n\n0\n\n\n\nQuestion 4:\n What is $p_{Y|X}(1|\\text {snowy})$?\n\n\n# p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']\n\n\n\n\n0.5\n\n\n\nExercise: Simpson's Paradox\n\n\nThis problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.\n\n\nWe have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).\n\n\nThus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.\n\n\nThe joint probability table is provided in the file \nsimpsons_paradox_data.py\n. Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).\n\n\nNow let's load in everything from \nsimpsons_paradox_data.py\n:\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nPlease open up \nsimpsons_paradox_data.py\n to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.\n\n\nFor example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:\n\n\njoint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]\n\n\n\n\nSome terminology:\n In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.\n\n\nLet's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).\n\n\nMarginalization is easy to do with NumPy:\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\n\n\n\n\nOn the right-hand side,\n.sum(axis=1)\n says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable \njoint_prob_gender_admission\n stores a 2D joint probability table for random variables $G$ and $A$.\n\n\nNow, for example, the probability that a woman applies and is admitted is given by:\n\n\njoint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\n\n\n\n\nNow let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:\n\n\n\n\n\\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}\n\n\n\n\nLet's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:\n\n\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\n\n\n\n\nNow this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!\n\n\nprob_admission_given_female = female_only / np.sum(female_only)\n\n\n\n\nThis is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:\n\n\nprob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)\n\n\n\n\nfrom simpsons_paradox_data import *\n\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\njoint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint(\nProbability of admitted female: {0:.5f}\n.format(prob_admitted_given_female))\n\n\n\n\nProbability of admitted female: 0.30334\n\n\n\nQuestion:\n What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nmale_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint(\nProbability of admitted male: {0:.5f}\n.format(prob_admitted_given_male))\n\n\n\n\nProbability of admitted male: 0.44520\n\n\n\nSo it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.\n\n\nBut before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:\n\n\nadmitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]\n\n\n\n\nNotice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.\n\n\nThus, the conditional probability table of gender given admitted is:\n\n\nprob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)\n\n\n\n\nAll right, now let's look at which departments are favoring men over women.\n\n\nFor the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:\n\n\nfemale_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]\n\n\n\n\nNow let's determine the probability of getting admitted given each gender and each department.\n\n\nDepartment A:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\n# A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint(\nProbability of admitted female in department A: {0:.3f}\n.format(p_A_given_F_and_A))\n\n\n\n\nProbability of admitted female in department A: 0.820\n\n\n\nGeneral Approach\n\n\n# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep\n\n\n\n\narray([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])\n\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]\n\n\n\n\n0.8200000000000004\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]\n\n\n\n\n0.62000000000000011\n\n\n\nDepartment B:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]\n\n\n\n\n0.67999999999999705\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]\n\n\n\n\n0.63000000000000034\n\n\n\nDepartment C\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]\n\n\n\n\n0.34000000000000008\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]\n\n\n\n\n0.37000000000000005\n\n\n\nDepartment D:\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]\n\n\n\n\n0.34999999999999998\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment E\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]\n\n\n\n\n0.23999999999999955\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nDepartment F\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]\n\n\n\n\n0.069999999999999701\n\n\n\nWhat is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)\n\n\nprob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]\n\n\n\n\n0.3300000000000004\n\n\n\nQuestion:\n How many of these departments have a higher probability of admitting women than of admitting men?\n\n\nfor index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value \n 0:\n        print(department_labels[index])\n\n\n\n\nA\nB\nD\nF\n\n\n\nSomehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!\n\n\nTake-away message:\n We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!\n\n\nMore General Story for Conditioning\n\n\nJointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.\n\n\nWe just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "03+Conditioning+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#conditioning-for-random-variables",
            "text": "When we observe that a random variable takes on a specific value (such as $W=\\text {rainy}$ from earlier for which we say that we condition on random variable $W$ taking on the value \u201crainy\"), this observation can affect what we think are likely or unlikely values for another random variable.  When we condition on $W=\\text {rainy}$, we do a two-step procedure; first, we only keep the row for $W$ corresponding to the observed value:   Second, we \u201cnormalize\" the table so that its entries add up to $1$, which corresponds to dividing it by the sum of the entries, which is equal to $p_{W}(\\text {rainy})$ in this case:   Notation:  The resulting probability table $p_{T\\mid W}(\\cdot \\mid \\text {rainy})$ is associated with the random variable denoted $(T\\mid W=\\text {rainy})$; we use \u201c\u2223\" to denote that we're conditioning on things to the right of \u201c\u2223\" happening (these are things that we have observed or that we are given as having happened). We read $\"T\\mid W=\\text {rainy}\"$ as either \u201c$T$ given $W$ is rainy\" or \u201c$T$ conditioned on $W$ being rainy\". To refer to specific entries of the table, we write, for instance,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\mathbb {P}(T=\\text {cold}\\mid W=\\text {rainy})=\\frac{4}{5}.   In general:  Conditioning:  Consider two random variables $X$ and $Y$ (that take on values in the sets $\\mathcal{X}$ and $\\mathcal{Y}$ respectively) with joint probability table $p_{X,Y}$ (from which by marginalization we can readily compute the marginal probability table pY). For any $x\u2208\\mathcal{X}$ and $y\u2208\\mathcal{Y}$ such that $p_{Y}(y) 0$, the  conditional probability  of event $X=x$ given event $Y=y$ has happened is   p_{X\\mid Y}(x\\mid y)\\triangleq \\frac{p_{X,Y}(x,y)}{p_{Y}(y)}.   For example,   p_{T\\mid W}(\\text {cold}\\mid \\text {rainy})=\\frac{p_{W,T}(\\text {rainy},\\text {cold})}{p_{W}(\\text {rainy})}=\\frac{\\frac{2}{15}}{\\frac{1}{6}}=\\frac{4}{5}.   Computational interpretation:  To compute $p_{X\\mid Y}(x\\mid y)$, take the entry $p_{X,Y}(x,y)$ in the joint probability table corresponding to $X=x$ and $Y=y$, and then divide the entry by $p_{Y}(y)$, which is an entry in the marginal probability table $p_Y$ for random variable $Y$.",
            "title": "Conditioning for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#exercise-conditioning-for-random-variables",
            "text": "Consider the following two joint probability tables.   Question 1:  What is $p_{W|I}(\\text {sunny}|1)$?  p_W_I = {\n    'sunny': {1: 1/2},\n    'rainy': {0: 1/6},\n    'snowy': {0: 1/3}\n}\n\np_W = {}\nfor key, value in p_W_I.items():\n    p_W_value = 0\n    for key1, value1 in value.items():\n        p_W_value += value1\n\n    p_W[key] = p_W_value\n\np_W\n\np_I = {}\nfor key, value in p_W_I.items():\n    for key1, value1 in value.items():\n        if key1 not in p_I:\n#             print(key1, value1)\n            p_I[key1] = value1\n\n        else:\n            p_I[key1] += value1\n\np_I   {0: 0.5, 1: 0.5}  # p_W_I('sunny', 1)\np_W_I['sunny'][1]/p_I[1]  1.0  Question 2:  What is $p_{X|Y}(\\text {sunny}|1)$?  p_X_Y = {\n    'sunny': {1: 1/4 ,  0: 1/4 },\n    'rainy': {1: 1/12,  0: 1/12},\n    'snowy': {1: 1/6 ,  0: 1/6 }\n}\n\np_X = {}\nfor key, value in p_X_Y.items():\n    p_X_value = 0\n    for key1, value1 in value.items():\n        p_X_value += value1\n\n    p_X[key] = p_X_value\n\np_X\n\np_Y = {}\nfor key, value in p_X_Y.items():\n    for key1, value1 in value.items():\n        if key1 not in p_Y:\n            p_Y[key1] = value1\n\n        else:\n            p_Y[key1] += value1\n\np_Y       {0: 0.5, 1: 0.5}  # p_X_Y('sunny', 1)\np_X_Y['sunny'][1]/p_Y[1]  0.5  Question 3:  What is $p_{I|W}(1|\\text {snowy})$?   # p_W_I['snowy'][1] = p_Y_X[1]['snowy]\ntry:\n    p_W_I['snowy'][1]/p_I['snowy']\nexcept:\n    print(0)  0  Question 4:  What is $p_{Y|X}(1|\\text {snowy})$?  # p_X_Y['snowy'][1] = p_Y_X[1]['snowy]\np_X_Y['snowy'][1]/p_X['snowy']  0.5",
            "title": "Exercise: Conditioning for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#exercise-simpsons-paradox",
            "text": "This problem looks at a real-life situation in which a school was accused of gender bias, seemingly admitting more men than women.  We have the data encoded as a 3D joint probability table across three random variables $G$ for gender (either female or male), $D$ for the department someone applied to (there are 6 departments in consideration, which for simplicity we will just call A, B, C, D, E, and F), and $A$ for the admissions decision (either admitted or rejected).  Thus, $p_{G,D,A}(\\text {female},\\text {C},\\text {admitted})$ refers to the probability that a woman applying to department C gets admitted.  The joint probability table is provided in the file  simpsons_paradox_data.py . Please download that and make sure it's in the same working directory as your IPython prompt (the same way you were able to import comp_prob_inference.py).  Now let's load in everything from  simpsons_paradox_data.py :  from simpsons_paradox_data import *  Please open up  simpsons_paradox_data.py  to see what Python variables you have access to, and to see how we can turn a probability space represented as a Python dictionary into the 3D joint probability table, stored as a 3D NumPy array.  For example, to get the probability that a woman applied to department C and got admitted, you can enter into the prompt:  joint_prob_table[gender_mapping['female'], department_mapping['C'], admission_mapping['admitted']]  Some terminology:  In Python, especially when we deal with NumPy arrays higher than 2 dimensions, talking about rows and columns is confusing. We will instead refer to the different axes as axis 0 (in this case: gender), axis 1 (in this case: department), axis 2 (in this case: admission decision), and so forth if we have more than 3 dimensions.  Let's see what the paradox is. First, let's look at the probability that women were admitted vs the probability that men were admitted. This means that we are going to marginalize out the department (again, this is axis 1).  Marginalization is easy to do with NumPy:  joint_prob_gender_admission = joint_prob_table.sum(axis=1)  On the right-hand side, .sum(axis=1)  says to sum across axis 1 so that we no longer have axis 1! This means that the Python variable  joint_prob_gender_admission  stores a 2D joint probability table for random variables $G$ and $A$.  Now, for example, the probability that a woman applies and is admitted is given by:  joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]  Now let's do some conditioning to see what the probability is of being admitted given that the applicant is female. Remember, what we want is the following conditional probability:   \\begin{eqnarray}\np_{A|G}(\\text{admitted} | \\text{female})\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_G(\\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {\\sum_a p_{A,G}(a, \\text{female})} \\\\\n&=&\n\\frac{p_{A,G}(\\text{admitted}, \\text{female})}\n     {p_{A,G}(\\text{admitted}, \\text{female})\n      + p_{A,G}(\\text{rejected}, \\text{female})}.\n\\end{eqnarray}   Let's restrict the joint probability table of $G$ and $A$ so that we only look at when $G = \\text {female}$:  female_only = joint_prob_gender_admission[gender_mapping['female']]  Now this corresponds to a vector that we have to normalize to be 1 to get a valid conditional probability table!  prob_admission_given_female = female_only / np.sum(female_only)  This is the right conditional probability table, represented as an array. To get it into the dictionary format we've been dealing with earlier in the course, we do:  prob_admission_given_female_dict = dict(zip(admission_labels, prob_admission_given_female))\nprint(prob_admission_given_female_dict)  from simpsons_paradox_data import *  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  joint_prob_gender_admission = joint_prob_table.sum(axis=1)\nfemale_only = joint_prob_gender_admission[gender_mapping['female']]\nprob_admission_and_female = joint_prob_gender_admission[gender_mapping['female'], admission_mapping['admitted']]\nprob_admitted_given_female = prob_admission_and_female/ np.sum(female_only)\nprint( Probability of admitted female: {0:.5f} .format(prob_admitted_given_female))  Probability of admitted female: 0.30334  Question:  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  male_only = joint_prob_gender_admission[gender_mapping['male']]\nprob_admission_and_male = joint_prob_gender_admission[gender_mapping['male'], admission_mapping['admitted']]\nprob_admitted_given_male = prob_admission_and_male / np.sum(male_only)\nprint( Probability of admitted male: {0:.5f} .format(prob_admitted_given_male))  Probability of admitted male: 0.44520  So it looks like there's some gender bias going on! Let's investigate by looking at how things differ by each department.  But before we do so, we just wanted to say a word about conditioning in code on axes aside from axis 0. With our 2D joint probability table of $G$ and $A$, to condition on admission decision instead, there's a slightly different syntax in Python. Let's condition on being admitted:  admitted_only = joint_prob_gender_admission[:, admission_mapping['admitted']]  Notice that we had to put a \u201c:,\". That is to indicate that we want to keep everything in the 0-th axis. We didn't have to do this when we conditioned on a value in the 0-th axis, since it is implied that you want everything in the axis 1 in that case.  Thus, the conditional probability table of gender given admitted is:  prob_gender_given_admitted = admitted_only / np.sum(admitted_only)\nprob_gender_given_admitted_dict = dict(zip(gender_labels, prob_gender_given_admitted))\nprint(prob_gender_given_admitted_dict)  All right, now let's look at which departments are favoring men over women.  For the following part, we will condition on both $G$ and $D$ taking on specific values together. For example, to only look at the entries in the 3D joint probability table for when $G=\\text{female}$ and, at the same time, $D=A$, then we can do the following:  female_and_A_only = joint_prob_table[gender_mapping['female'], department_mapping['A']]  Now let's determine the probability of getting admitted given each gender and each department.",
            "title": "Exercise: Simpson's Paradox"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#department-a",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  # A: admitted, F: female\np_A_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['admitted']]\np_R_F_A = joint_prob_table[gender_mapping['female'], department_mapping['A'], \\\n                           admission_mapping['rejected']]\np_A_given_F_and_A = p_A_F_A / (p_A_F_A + p_R_F_A)\nprint( Probability of admitted female in department A: {0:.3f} .format(p_A_given_F_and_A))  Probability of admitted female in department A: 0.820",
            "title": "Department A:"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#general-approach",
            "text": "# Taking admitted only\njoint_prob_admitted = joint_prob_table[:,:,admission_mapping['admitted']]\n# Summing admitted and rejected\njoint_prob_gen_and_dep = joint_prob_table.sum(axis=2)\n# Taking admitted only / Summing admitted and rejected\nprob_admitted_given_gen_and_dep = joint_prob_admitted/joint_prob_gen_and_dep\nprob_admitted_given_gen_and_dep  array([[ 0.82,  0.68,  0.34,  0.35,  0.24,  0.07],\n       [ 0.62,  0.63,  0.37,  0.33,  0.28,  0.06]])  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['A']]  0.8200000000000004  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {A})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['A']]  0.62000000000000011",
            "title": "General Approach"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#department-b",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['B']]  0.67999999999999705  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {B})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['B']]  0.63000000000000034",
            "title": "Department B:"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#department-c",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['C']]  0.34000000000000008  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {C})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['C']]  0.37000000000000005",
            "title": "Department C"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#department-d",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['D']]  0.34999999999999998  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {D})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department D:"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#department-e",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['E']]  0.23999999999999955  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {E})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004",
            "title": "Department E"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#department-f",
            "text": "What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {female}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['female'], department_mapping['F']]  0.069999999999999701  What is $\\mathbb {P}(A = \\text {admitted} | G = \\text {male}, D = \\text {F})$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places.)  prob_admitted_given_gen_and_dep[gender_mapping['male'], department_mapping['D']]  0.3300000000000004  Question:  How many of these departments have a higher probability of admitting women than of admitting men?  for index, value in enumerate(np.diff(prob_admitted_given_gen_and_dep, axis=0)[0]):\n    if value   0:\n        print(department_labels[index])  A\nB\nD\nF  Somehow, it seems that when we marginalized out the department, the gender bias is going one direction, yet when looking at the specific departments, most departments seem to be having the bias go the other direction!  Take-away message:  We have to be very careful when interpreting conditional probabilities! Also, marginalization (which lumps different groups of data together, where here the groups are departments) can reverse trends that appear in specific groups!",
            "title": "Department F"
        },
        {
            "location": "/Computational Probability/week01_3/03+Conditioning+for+Random+Variables/#more-general-story-for-conditioning",
            "text": "Jointly distributed random variables play a central role in this course. Remember that we will model observations as random variables and the quantities we want to infer also as random variables. When these random variables are jointly distributed so that we have a probabilistic way to describe how they relate (through their joint probability table), then we can systematically and quantitatively produce inferences.  We just saw how to condition on a random variable taking on a specific value. What about if we wanted to condition on a random variable taking on any one of of many values rather just one specific value? To answer this question, we look at a more general story of conditioning which is in terms of events.",
            "title": "More General Story for Conditioning"
        },
        {
            "location": "/Computational Probability/week01_3/04+Homework/",
            "text": "Homework Problem: Ice Cream Sales in Inferenceville\n\n\nYou have been hired to investigate a disturbing connection between ice cream sales and crime in Inferenceville. You are given a report that describes the joint distribution over random variable $S$, representing ice cream sales, and random variable $C$, representing crime. Each variable takes on a value of \u201clow\" or \u201chigh\", which we'll represent with $0$ and $1$ respectively. The joint distribution (estimated from data) is as follows:\n\n\n\n\n(a) Are random variables $S$ and $C$ independent?\n\n\nYes\nNo\nunanswered\n\n\n(b) After further investigation, you discover information about the temperature, represented by $T$. This random variable also takes on values $0$ or $1$ corresponding again to \u201clow\" and \u201chigh\". You are able to obtain the conditional distribution $p_{S,C \\mid T}(s,c \\mid t)$, shown below.\n\n\n\n\nAre random variables $S$ and $C$ conditionally independent given $T$?\n\n\nYes\nNo\nunanswered\n\n\n(c) Determine the distribution $p_T$ from the tables above. Express your answer as a Python dictionary. The keys should be the Python integers $0$ and $1$.\n\n\n\n\nNote\n\n\nThis is a good theme. the $A$ adn $\\sum$\n\n\n\n\n\n\noptional explicit title within double quotes\n\n\nAny number of other indented markdown elements.\n\n\nThis is the second paragraph.\n\n\n\n\n\n\nDon't try this at home\n\n\n...\n\n\n\n\n\n\ngjhgjhgh\n\n\nThis is a admonition box without a title.",
            "title": "04+Homework"
        },
        {
            "location": "/Computational Probability/week01_3/Linear Algebra December 2015 Questions(Part B)/",
            "text": "Question 1:\n If $A$ is a $5\\times 5$ real matrix with trace $15$ and if $2$ and $3$ are eigenvalues of $A$ , each with algebraic multiplicity $2$, then the determinant of $A$ is equal to\n\n\n\n\n$0$\n\n\n$24$\n\n\n$120$\n\n\n$180$\n\n\n\n\nAnswer:\n Out of five eigenvalues of matrix $A$ we know four $2,~2,~3,~3$. Let the last eigenvalue is $\\lambda$. Then,\n\n\\begin{align}& \\text{ trace }A = 2+2+3+3+\\lambda\\\\ \\Rightarrow& \\lambda = 15-10=5.\\end{align}\n\nHence the determinant, will be product of eigenvalues\n\n\\text{ det }A = 2\\cdot 2\\cdot 3\\cdot 3\\cdot 5=180.\n\n\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 2:\n For a positive integer $n$, let $P_n$ denote the vector space of polynomials in one variable $x$ with real coefficients and with degree $\\leq n$. Consider the map $T:P_2 \\rightarrow P_4$ defined by $T(p(x)) = p(x^2)$. Then\n\n\n\n\n$T$ is a linear transformation and dim range $(T) = 5$.\n\n\n$T$ is a linear transformation and dim range $(T) = 3$.\n\n\n$T$ is a linear transformation and dim range $(T) = 2$.\n\n\n$T$ is not a linear transformation.\n\n\n\n\nAnswer:\n This transformation replaces each $x$ by $x^2$. Also,\n\n\\begin{align}T(\\alpha p(x) + \\beta q(x)) &= \\alpha p(x^2) + \\beta q(x^2)\\\\ &=\\alpha T(p(x)) + \\beta T(q(x))\\end{align}\n\nHence this is a linear transformation, and\n\n\\text{ ker }T = \\{p(x) : p(x^2) = 0\\} = \\{0\\}.\n\nHence the nullity will be zero. By rank nullity theorem, we get\n\n\n\\begin{align}\n\\text{ rank }T &= \\text{ dim }P_2 - \\text{ nullity }\\\\\n&= 3 - 0 \\\\\n&= 3\n\\end{align}\n\n\n\n\n\nHence \n2\n is correct choice.\n\n\n\n\nQuestion 3:\n Let $A$ be a real $3\\times 4$ matrix of rank $2$. Then the rank of $A^tA$, where $A^t$ deonotes the transpose $A$, is\n\n\n\n\nexactly $2$\n\n\nexactly $3$\n\n\nexactly $4$\n\n\nat most $2$ but not necessarily $2$\n\n\n\n\nAnswer:\n Given that the rank of $A$ is $2$, hence the nullity of $A$ is $4-2 = 2$. Using theorem,\n\n\n\n\nNullspace of $A$ and $A^TA$ are the same.\n\n\n\n\nWe get the nullity of $A^TA$ is 2. Since $A^TA$ is $4\\times 4$ matrix. Hence rank of $A^TA$ will be $4-2 = 2$.\n\n\nHence \n1\n is correct choice.\n\n\n\n\nQuestion 4:\n  Let $S$ denote the set of all the prime numbers $p$ with the property that the matrix\n\n\n\\pmatrix{91& 31 & 0\\\\ 29 & 31 & 0\\\\ 79 & 23 & 59}\n\n\nhas an inverse in the field $\\newcommand{\\Z}{\\mathbb{Z}}\\Z/p\\Z$. Then\n\n\n\n\n$S = {31}$\n\n\n$S = {31, ~59}$\n\n\n$S = {7, ~13, ~59}$\n\n\n$S$ is infinite\n\n\n\n\nAnswer:\n The matrix is singular, if it's determinant will be zero in that field.\n\n\\begin{align}\\text{ det } &= 59\\times 31(91-29) \\\\ &= 59\\times 31 \\times  31 \\times 2.\\end{align}\n\nThe determinant will be zero in $\\Z_p$ only for $p=59, ~31, ~2$. For all other primes it will be non-zero.\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 5:\n Consider the quadratic form $Q(v) = v^tAv$, where\n\nA = \\pmatrix{1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&0&1}, v= (x,y,z,w)\n\nThen\n\n\n\n\n$Q$ has rank $3$.\n\n\n$xy+z^2 = Q(P(v))$ for some invertible $4 \\times 4$ real matrix $P$.\n\n\n$xy+y^2+ z^2 = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n$x^2+y^2-zw = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n\n\nAnswer:\n Here,\n\nQ(Pv) = (Pv)^tAPv = v^t(P^tAP)v \n\nWhen $P$ is orthogonal, $A$ and $P^tAP$ both represent equivalent quadratic forms. Then the problem is reduced to find equivalent bilinear forms.\n\n\n\n\nQuestion 6:\n Let $A \\neq I_n$ be an $n \\times n$ matrix such that $A^2 = A$, where $I_n$ is the identity matrix of order $n$. Which of the following statements is false?\n\n\n\n\n$(I_n - A)^2 = I_n - A$.\n\n\ntrace $(A)=$ rank $(A)$.\n\n\nrank $(A)$ + rank $(I_n-A)$ = n.\n\n\nThe eigenvalus of $A$ are each equal to $1$.\n\n\n\n\nAnswer:\n The matrix satisfies the polynomial $x^2 - x = 0\\Rightarrow x(x-1) = 0$. Hence minimal polynomial is a factor of $x(x-1)$. Since this polynomial has linear factors, hence matrix is diagonalizable with eigenvalues $0$ or $1$ or both. Since $A \\neq I$ then at least eigenvalues will be $0$.\n\n\nHence \n4\n is correct choice.",
            "title": "Linear Algebra December 2015 Questions(Part B)"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01_3/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01_3/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01_3/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01_3/chess/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01_4/01 Simulating Coin Filps/",
            "text": "Simulating Coin Filps\n\n\n%matplotlib inline\n\n\n\n\nimport sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *\n\n\n\n\nflip_fair_coin()\n\n\n\n\n'heads'\n\n\n\nflips = flip_fair_coins(100)\nplot_discrete_histogram(flips)\n\n\n\n\n\n\nplot_discrete_histogram(flip_fair_coins(10), frequency=True)\n\n\n\n\n\n\nn = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "01 Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01_4/01 Simulating Coin Filps/#simulating-coin-filps",
            "text": "%matplotlib inline  import sys\nsys.path.append('../comp_prob_inference')\nfrom comp_prob_inference import *  flip_fair_coin()  'heads'  flips = flip_fair_coins(100)\nplot_discrete_histogram(flips)   plot_discrete_histogram(flip_fair_coins(10), frequency=True)   n = 100000\nheads_so_far = 0\nfraction_of_heads = []\nfor i in range(n):\n    if flip_fair_coin() == 'heads':\n        heads_so_far += 1\n    fraction_of_heads.append(heads_so_far / (i+1))  import matplotlib.pyplot as plt\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, n+1), fraction_of_heads)\nplt.xlabel('Number of flips')\nplt.ylabel('Fraction of heads')\nplt.show()",
            "title": "Simulating Coin Filps"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/",
            "text": "Introduction to Independence\n\n\nWith a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?\n\n\nThe answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".\n\n\nWe have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.\n\n\nNot only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.\n\n\nIndependet Events\n\n\nTwo events $A$ and $B$ are independet denoted by $A \\perp B$ if\n\n\n\n\n\\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)\n\n\n\n\nExample:\n If we toss two coin then probability of heads up is multiple of probability of heads for each coin. \n\n\nIn terms of conditional probability we \n\n \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align}\n\nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.\n\n\nExercise: Bernoulli and Bin\n\n\nThis problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.\n\n\nThese two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!\n\n\nAs mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.\n\n\nA Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.\n\n\n(a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.\n\n\nAnswer:\n FALSE\n\n\n(b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nsequence = \nHTHTTTTTHH\n\nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob    \n\n\n\n\n0.0005308416000000001\n\n\n\n(c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?\n\n\n[$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails. \n\n[$\\times    $] The probability is different depending on the ordering of heads and tails.\n\n\n(d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)\n\n\ndef ncr(n, r):\n    \n\n    If calculates the n choose r for n \n= r.\n\n    \n ncr(10, 4)\n    210.0\n\n    \n ncr(4, 4)\n    1.0\n    \n\n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ == \n__main__\n:\n    import doctest \n    doctest.testmod()\n\n\n\n\nncr(10, 4)\n\n\n\n\n210.0\n\n\n\n(e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprob * ncr(10, 4)\n\n\n\n\n0.11147673600000002\n\n\n\nIn general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.\n\n\nPlease be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.\n\n\nExercise: The Soda Machine\n\n\n3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.\n\n\n(a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/7)**14\n\n\n\n\n2.4157243620710218e-08\n\n\n\n(b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/5)**14\n\n\n\n\n2.684354560000002e-06\n\n\n\n(c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n(2/3)**14\n\n\n\n\n0.003425487390781748\n\n\n\nExercise: Gambler's Fallacy\n\n\nSuppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.\n\n\n(a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**100\n\n\n\n\n0.9770407138326136\n\n\n\n(b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\n1 - (26/27)**99\n\n\n\n\n0.9761576643646371\n\n\n\n(c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\n1 - (26/27)**(100-n)\n\n\n\n\n0.9770407138326136\n\n\n\n(d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?\n\n\n[$\\checkmark$] Probability decreases as $n$ increases \n\n[$\\times    $] Probability increases as $n$ increases\n\n\nimport matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel(\nNo of trials without getting $27$\n)\nplt.ylabel(\nProb of getting $27$\n)\nplt.show()\n\n\n\n\n\n\nIndependet Random Variable\n\n\nTwo random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by \n\n\n\n\np_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y\n\n\n\n\nIndepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability \n\n\n\n\np_{X\\mid Y}(x\\mid y) = p_X(x) \n\n\n\n\nExercise: Independent Random Variables\n\n\nIn this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.\n\n\nConsider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.\n\n\n\n\nIn Python:\n\n\nprob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])\n\n\n\n\nNote that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.\n\n\nWe can get the marginal distributions $p_W$ and $p_I$:\n\n\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n\n\n\n\nThen if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:\n\n\n\n\np_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.\n\n\n\n\nNote that variables \nprob_W\n and \nprob_I\n at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.\n\n\nWe could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of \nprob_W\n and \nprob_I\n yields what the joint distribution would be if $W$ and $I$ were independent:\n\n\n\n\n\\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}\n\n\n\n\nThe left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.\n\n\nTo compute and print the right-hand side, we do:\n\n\nprint(np.outer(prob_W, prob_I))\n\n\n\n\nQuestion:\n Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?\n\n\nAnswer:\n FALSE\n\n\nfrom numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)\n\n\n\n\n0.5\n\n\n\nQuestion:\n Are X and Y independent?\n\n\nAnswer:\n TRUE\n\n\nprob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)\n\n\n\n\n0.0\n\n\n\nMutual and Pairwise Independence\n\n\nThree random variable $X, Y$ and $Z$ are \nmutually independent\n if \n\n\n\n\np_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z) \n\n\n\n\nThree random variable $X,Y$ and $Z$ are \npairwise independence\n if \n\n\n\n\np_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J\n\n\n\n\nThroughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.\n\n\nExercise: Mutual vs Pairwise Independence\n\n\nSuppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise. \nIn this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution\n (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).\n\n\nSuppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.\n\n\nSelect all of the following that are true:\n\n\n[$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same. \n\n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same. \n\n[$\\times$] $X, Y$, and $Z$ are pairwise independent.\n\n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "02 Independence Structure"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#introduction-to-independence",
            "text": "With a fair coin, let's say that we just tossed it five times and tails turned up all five times. Is it more likely now that we'll see heads?  The answer is no because the outcome of the tosses don't tell us anything about the outcome of a new toss. This concept is referred to as \u201cindependence\".  We have actually already encountered independent events already, for example when we talked about two coin flips or two dice rolls. Now we develop theory for independent events and then, very soon, independent random variables. We build up to a concept called conditional independence, where two random variables become independent only after we condition on the value of a third random variable.  Not only is independence an important phenomenon to understand and to help us reason about many scenarios, it will also play a pivotal role in how we can represent very large probabilistic models with very little space on a computer.",
            "title": "Introduction to Independence"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#independet-events",
            "text": "Two events $A$ and $B$ are independet denoted by $A \\perp B$ if   \\mathbb{P}(A \\cap B) = \\mathbb{p}(A) \\cdot \\mathbb{P}(B)   Example:  If we toss two coin then probability of heads up is multiple of probability of heads for each coin.   In terms of conditional probability we   \\begin{align} \n\\require{cancel}\\mathbb{P}(A \\cap B) &= \\mathbb{p}(A) \\cdot \\mathbb{P}(B) \\\\\n\\cancel{\\mathbb{P}(A)} \\mathbb{P}(B\\mid A) &= \\cancel{\\mathbb{P}(A)} \\cdot \\mathbb{P}(B) \\\\\n\\mathbb{P}(B\\mid A) &= \\mathbb{P}(B)\n\\end{align} \nThus if $A \\perp B$ then probabiliy if $B$ given $A$ is euqal to probability of $B$.",
            "title": "Independet Events"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#exercise-bernoulli-and-bin",
            "text": "This problem introduces two of the most common random variables that people use in probabilistic models: the Bernoulli random variable, and the Binomial random variable. We have actually already encountered these albeit with a disguise! A Bernoulli random variable is like a biased coin flip. A Binomial random variable is like counting the number of heads for $n$ of these biased coin flips.  These two distributions appear all the time in many, many application domains that use inference! We introduce them now to equip you with some vocabulary and also to let you see our first example of a random variable whose probability table can be described by only a few numbers even if the number of entries in the table can be much larger!  As mentioned, a Bernoulli random variable is like a biased coin flip where probability of heads is $p$. In particular, a Bernoulli random variables is $1$ with probability $p$, and $0$ with probability $1\u2212p$. If a random variable $X$ has this particular distribution, then we write $X\\sim \\text{Bernoulli}(p)$, where \u201c$\\sim$\" can be read as \u201cis distributed as\" or \u201chas distribution\". Some people like to abbreviate $\\text{Bernoulli}(p)$ by writing $\\text{Bern}(p)$, $\\text{Ber}(p)$, or even just $B(p)$.  A Binomial random variable can be thought of as n independent coin flips, each with probability $p$ of heads. For a random variable $S$ that has this Binomial distribution with parameters $n$ and $p$, we denote it as $S \\sim \\text{Binomial}(n,p)$, read as \u201c$S$ is distributed as Binomial with parameters $n$ and $p$\". Some people might also abbreviate and instead of writing $\\text{Binomial}(n,p)$, they write $\\text{Binom}(n,p)$ or $\\text{Bin}(n,p)$.  (a) True or false: If $Y \\sim \\text{Binomial}(n,p)$, then $Y$ is a Bernoulli random variable.  Answer:  FALSE  (b) Let's say we have a coin that turns up heads with probability $0.6$. We flip this coin $10$ times. What is the probability of seeing the sequence HTHTTTTTHH, where H denotes heads and T denotes tails (so we have heads in the first toss, tails in the second, heads in the third, etc)? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  sequence =  HTHTTTTTHH \nmodel = {'H': 0.6, 'T': 0.4}\nprob = 1\nfor char in sequence:\n    prob *= model[char]\n\nprob      0.0005308416000000001  (c) In the previous part, there were 4 heads and 6 tails. Did the ordering of them matter? In other words, would your answer to the previous part be the same if, for example, instead we saw the sequence HHHHTTTTTT (or any other permutation of 4 heads and 6 tails)?  [$\\checkmark$] The probability stays the same so long as we have 4 heads and 6 tails.  \n[$\\times    $] The probability is different depending on the ordering of heads and tails.  (d) From the previous two parts, what we were analyzing was the same as the random variable $S \\sim \\text{Binomial}(10,0.6)$. Note that $S=4$ refers to the event that we see exactly 4 heads. Note that HTHTTTTTHH and HHHHTTTTTT are different outcomes of the underlying experiment of coin flipping. How many ways are there to see 4 heads in 10 tosses? (Please provide the exact answer.)  def ncr(n, r):\n     \n    If calculates the n choose r for n  = r.\n\n      ncr(10, 4)\n    210.0\n\n      ncr(4, 4)\n    1.0\n     \n    from scipy.misc import factorial \n    return factorial(n) // (factorial(r) * factorial(n-r))\n\nif __name__ ==  __main__ :\n    import doctest \n    doctest.testmod()  ncr(10, 4)  210.0  (e) Using your answers to parts (b) through (d), what is the probability that $S=4$? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prob * ncr(10, 4)  0.11147673600000002  In general, for a random variable $S \\sim \\text{Binomial}(n,p)$, the reasoning used in parts (b) through (e) could be used to obtain the probability that $S=s$ for any $s\u2208{0,1,2,\u2026,n}$. Importantly, what this means is that by just specifying two numbers $n$ and $p$, we know the full probability table for random variable $S$, which has $n+1$ entries! This is an example of where we could have many probability table entries yet we can fully specify the entire table using fewer numbers than the number of entries in the table.  Please be sure to look at the solution to this problem after you have finished it to see the general equation for what the probability table entry $p_S(s)$ is, and also why the probability table entries sum to $1$.",
            "title": "Exercise: Bernoulli and Bin"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#exercise-the-soda-machine",
            "text": "3 points possible (graded)\nA soda machine advertises 7 different flavors of soda. However, there is only one button for buying soda, which dispenses a flavor of the machine's choosing. Adam buys 14 sodas today, and notices that they are all either grape or root beer flavored.  (a) Assuming that the soda machine actually dispenses each of its 7 flavors randomly, with equal probability, and independently each time, what is the probability that all 14 of Adam's sodas are either grape or root beer flavored? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/7)**14  2.4157243620710218e-08  (b) How would your answer to the (a) change if the machine were out of diet cola, ginger ale, so it randomly chooses one of only 5 flavors? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/5)**14  2.684354560000002e-06  (c) What if the machine only had 3 flavors: grape, root beer, and cherry? (Please be precise with at least 10 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  (2/3)**14  0.003425487390781748",
            "title": "Exercise: The Soda Machine"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#exercise-gamblers-fallacy",
            "text": "Suppose you have a 27-sided fair die (with faces numbered $1,2,\\ldots ,27$) that you get to roll 100 times. You win a prize if you roll 27 at least once. In this problem we look at what happens if you don't roll 27 for a while and see whether or not you're more likely to roll a 27 in your remaining rolls.  (a) What is the probability that you roll 27 at least once out of the 100 rolls? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**100  0.9770407138326136  (b) Suppose you roll the die once and don't get 27. What is the probability that of the remaining 99 rolls, you will roll 27 at least once? (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  1 - (26/27)**99  0.9761576643646371  (c) Suppose you roll the die n times and don't get 27 any of those times. What is the probability that of the remaining $100\u2212n$ rolls, you will roll 27 at least once? Express your answer in terms of n.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use $\\hat{}$ for exponentiation, e.g., $x\\hat{} 2$ denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  1 - (26/27)**(100-n)  0.9770407138326136  (d) Plot the probability in part (c) as a function of $n$ for $n=1,2,\u2026,99$. Does this probability increase or decrease as n increases?  [$\\checkmark$] Probability decreases as $n$ increases  \n[$\\times    $] Probability increases as $n$ increases  import matplotlib.pyplot as plt\n\nx = [i for i in range(1, 100)]\ny = [1 - (26/27)**(100-n) for n in x]\n\nplt.plot(x, y, 'g')\nplt.xlabel( No of trials without getting $27$ )\nplt.ylabel( Prob of getting $27$ )\nplt.show()",
            "title": "Exercise: Gambler's Fallacy"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#independet-random-variable",
            "text": "Two random variable $X$ and $Y$ are independent denoted by $X \\perp Y$, if the joint probability distribution $p_{X,Y}$ is given by    p_{X,Y} (x,y) = p_X(x)\\, p_Y(y) \\quad \\forall x,y   Indepence roughly means \"knowing one we have no information about other\". Also in terms of conditioanl probability    p_{X\\mid Y}(x\\mid y) = p_X(x)",
            "title": "Independet Random Variable"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#exercise-independent-random-variables",
            "text": "In this exercise, we look at how to check if two random variables are independent in Python. Please make sure that you can follow the math for what's going on and be able to do this by hand as well.  Consider random variables $W, I, X$, and $Y$, where we have shown the joint probability tables $p_{W,I}$ and $p_{X,Y}$.   In Python:  prob_W_I = np.array([[1/2, 0], [0, 1/6], [0, 1/3]])  Note that here, we are not explicitly storing the labels, but we'll keep track of them in our heads. The labels for the rows (in order of row index): sunny, rainy, snowy. The labels for the columns (in order of column index): 1, 0.  We can get the marginal distributions $p_W$ and $p_I$:  prob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)  Then if $W$ and $I$ were actually independent, then just from their marginal distributions $p_W$ and $p_I$, we would be able to compute the joint distribution with the formula:   p_{W,I}(w,i)=p_W(w)\\, p_I(i) \\quad \\forall ~w,i.   Note that variables  prob_W  and  prob_I  at this point store the probability tables $p_W$ and $p_I$ as 1D NumPy arrays, for which NumPy does not store whether each of these should be represented as a row or as a column.  We could however ask NumPy to treat them as column vectors, and in particular, taking the outer product of  prob_W  and  prob_I  yields what the joint distribution would be if $W$ and $I$ were independent:   \\begin{eqnarray}\n\\begin{bmatrix}\np_W(\\text{sunny}) \\\\\np_W(\\text{rainy}) \\\\\np_W(\\text{snowy})\n\\end{bmatrix}\n\\begin{bmatrix}\np_I(1) & p_I(0)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\np_W(\\text{sunny})p_I(1) & p_W(\\text{sunny})p_I(0) \\\\\np_W(\\text{rainy})p_I(1) & p_W(\\text{rainy})p_I(0) \\\\\np_W(\\text{snowy})p_I(1) & p_W(\\text{snowy})p_I(0)\n\\end{bmatrix}.\n\\end{eqnarray}   The left-hand side is an outer product, and the right-hand side is precisely the joint probability table that would result if $W$ and $I$ were independent.  To compute and print the right-hand side, we do:  print(np.outer(prob_W, prob_I))  Question:  Are $W$ and $I$ independent (compare the joint probability table we would get if they were independent with their actual joint probability table)?  Answer:  FALSE  from numpy import array, inf\nfrom numpy.linalg import norm\nprob_W_I = array([[1/2, 0], [0, 1/6], [0, 1/3]])\nprob_W = prob_W_I.sum(axis=1)\nprob_I = prob_W_I.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_W, prob_I) - prob_W_I\nnorm(\u0394, inf)  0.5  Question:  Are X and Y independent?  Answer:  TRUE  prob_X_Y = array([[1/4, 1/4], [1/12, 1/12], [1/6, 1/6]])\nprob_X = prob_X_Y.sum(axis=1)\nprob_Y = prob_X_Y.sum(axis=0)\n# Difference between two matrix\n\u0394 = np.outer(prob_X, prob_Y) - prob_X_Y\nnorm(\u0394, inf)  0.0",
            "title": "Exercise: Independent Random Variables"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#mutual-and-pairwise-independence",
            "text": "Three random variable $X, Y$ and $Z$ are  mutually independent  if    p_{X,Y,Z} = p_X(x) \\, p_Y(y) \\, p_Z(z)    Three random variable $X,Y$ and $Z$ are  pairwise independence  if    p_{I,J} = p_I \\, p_J \\quad \\forall ~I,J \\in \\{X,Y,Z\\}, I\\neq J   Throughout this course, if we say that many random variables are independent (without saying which specific kind of independence), then we mean mutual independence, which we often also call marginal independence.",
            "title": "Mutual and Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01_4/02 Independence Structure/#exercise-mutual-vs-pairwise-independence",
            "text": "Suppose random variables $X$ and $Y$ are independent, where $X$ is $1$ with probability $1/2$, and $-1$ otherwise. Similarly, $Y$ is also $1$ with probability $1/2$, and $-1$ otherwise.  In this case, we say that $X$ and $Y$ are identically distributed since they have the same distribution  (remember, just because they have the same distribution doesn't mean that they are the same random variable \u2014 here $X$ and $Y$ are independent!). Note that often in this course, we'll be seeing random variables that are independent and identically distributed (i.i.d.).  Suppose we have another random variable $Z$ that is the product of $X$ and $Y$, i.e., $Z=XY$.  Select all of the following that are true:  [$\\times$] The distributions $p_X, p_Y$, and $p_Z$ are the same.  \n[$\\times$] The joint distributions $p_{X,Y}, p_{X,Z}$, and $p_{Y,Z}$ are the same.  \n[$\\times$] $X, Y$, and $Z$ are pairwise independent. \n[$\\times$] $X, Y$, and $Z$ are mutually independent.",
            "title": "Exercise: Mutual vs Pairwise Independence"
        },
        {
            "location": "/Computational Probability/week01_4/04+Homework/",
            "text": "Homework Problem: Ice Cream Sales in Inferenceville\n\n\nYou have been hired to investigate a disturbing connection between ice cream sales and crime in Inferenceville. You are given a report that describes the joint distribution over random variable $S$, representing ice cream sales, and random variable $C$, representing crime. Each variable takes on a value of \u201clow\" or \u201chigh\", which we'll represent with $0$ and $1$ respectively. The joint distribution (estimated from data) is as follows:\n\n\n\n\n(a) Are random variables $S$ and $C$ independent?\n\n\nYes\nNo\nunanswered\n\n\n(b) After further investigation, you discover information about the temperature, represented by $T$. This random variable also takes on values $0$ or $1$ corresponding again to \u201clow\" and \u201chigh\". You are able to obtain the conditional distribution $p_{S,C \\mid T}(s,c \\mid t)$, shown below.\n\n\n\n\nAre random variables $S$ and $C$ conditionally independent given $T$?\n\n\nYes\nNo\nunanswered\n\n\n(c) Determine the distribution $p_T$ from the tables above. Express your answer as a Python dictionary. The keys should be the Python integers $0$ and $1$.\n\n\n\n\nNote\n\n\nThis is a good theme. the $A$ adn $\\sum$\n\n\n\n\n\n\noptional explicit title within double quotes\n\n\nAny number of other indented markdown elements.\n\n\nThis is the second paragraph.\n\n\n\n\n\n\nDon't try this at home\n\n\n...\n\n\n\n\n!!! important \"gjhgjhgh\"\n    This is a admonition box without a title. \n\n\n\n\ngjhgjhgh\n\n\nThis is a admonition box without a title. \n\n\n\n\n\n\ngjhgjhgh\n\n\nThis is a admonition box without a title. \n\n\n\n\n\n\ngjhgjhgh\n\n\nThis is a admonition box without a title.",
            "title": "04+Homework"
        },
        {
            "location": "/Computational Probability/week01_4/Linear Algebra December 2015 Questions(Part B)/",
            "text": "Question 1:\n If $A$ is a $5\\times 5$ real matrix with trace $15$ and if $2$ and $3$ are eigenvalues of $A$ , each with algebraic multiplicity $2$, then the determinant of $A$ is equal to\n\n\n\n\n$0$\n\n\n$24$\n\n\n$120$\n\n\n$180$\n\n\n\n\nAnswer:\n Out of five eigenvalues of matrix $A$ we know four $2,~2,~3,~3$. Let the last eigenvalue is $\\lambda$. Then,\n\n\\begin{align}& \\text{ trace }A = 2+2+3+3+\\lambda\\\\ \\Rightarrow& \\lambda = 15-10=5.\\end{align}\n\nHence the determinant, will be product of eigenvalues\n\n\\text{ det }A = 2\\cdot 2\\cdot 3\\cdot 3\\cdot 5=180.\n\n\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 2:\n For a positive integer $n$, let $P_n$ denote the vector space of polynomials in one variable $x$ with real coefficients and with degree $\\leq n$. Consider the map $T:P_2 \\rightarrow P_4$ defined by $T(p(x)) = p(x^2)$. Then\n\n\n\n\n$T$ is a linear transformation and dim range $(T) = 5$.\n\n\n$T$ is a linear transformation and dim range $(T) = 3$.\n\n\n$T$ is a linear transformation and dim range $(T) = 2$.\n\n\n$T$ is not a linear transformation.\n\n\n\n\nAnswer:\n This transformation replaces each $x$ by $x^2$. Also,\n\n\\begin{align}T(\\alpha p(x) + \\beta q(x)) &= \\alpha p(x^2) + \\beta q(x^2)\\\\ &=\\alpha T(p(x)) + \\beta T(q(x))\\end{align}\n\nHence this is a linear transformation, and\n\n\\text{ ker }T = \\{p(x) : p(x^2) = 0\\} = \\{0\\}.\n\nHence the nullity will be zero. By rank nullity theorem, we get\n\n\n\\begin{align}\n\\text{ rank }T &= \\text{ dim }P_2 - \\text{ nullity }\\\\\n&= 3 - 0 \\\\\n&= 3\n\\end{align}\n\n\n\n\n\nHence \n2\n is correct choice.\n\n\n\n\nQuestion 3:\n Let $A$ be a real $3\\times 4$ matrix of rank $2$. Then the rank of $A^tA$, where $A^t$ deonotes the transpose $A$, is\n\n\n\n\nexactly $2$\n\n\nexactly $3$\n\n\nexactly $4$\n\n\nat most $2$ but not necessarily $2$\n\n\n\n\nAnswer:\n Given that the rank of $A$ is $2$, hence the nullity of $A$ is $4-2 = 2$. Using theorem,\n\n\n\n\nNullspace of $A$ and $A^TA$ are the same.\n\n\n\n\nWe get the nullity of $A^TA$ is 2. Since $A^TA$ is $4\\times 4$ matrix. Hence rank of $A^TA$ will be $4-2 = 2$.\n\n\nHence \n1\n is correct choice.\n\n\n\n\nQuestion 4:\n  Let $S$ denote the set of all the prime numbers $p$ with the property that the matrix\n\n\n\\pmatrix{91& 31 & 0\\\\ 29 & 31 & 0\\\\ 79 & 23 & 59}\n\n\nhas an inverse in the field $\\newcommand{\\Z}{\\mathbb{Z}}\\Z/p\\Z$. Then\n\n\n\n\n$S = {31}$\n\n\n$S = {31, ~59}$\n\n\n$S = {7, ~13, ~59}$\n\n\n$S$ is infinite\n\n\n\n\nAnswer:\n The matrix is singular, if it's determinant will be zero in that field.\n\n\\begin{align}\\text{ det } &= 59\\times 31(91-29) \\\\ &= 59\\times 31 \\times  31 \\times 2.\\end{align}\n\nThe determinant will be zero in $\\Z_p$ only for $p=59, ~31, ~2$. For all other primes it will be non-zero.\n\n\nHence \n4\n is correct choice.\n\n\n\n\nQuestion 5:\n Consider the quadratic form $Q(v) = v^tAv$, where\n\nA = \\pmatrix{1&0&0&0 \\\\ 0&1&0&0 \\\\ 0&0&0&1 \\\\ 0&0&0&1}, v= (x,y,z,w)\n\nThen\n\n\n\n\n$Q$ has rank $3$.\n\n\n$xy+z^2 = Q(P(v))$ for some invertible $4 \\times 4$ real matrix $P$.\n\n\n$xy+y^2+ z^2 = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n$x^2+y^2-zw = Q(P(v))$ for some invertible $4\\times 4$ real matrix $P$.\n\n\n\n\nAnswer:\n Here,\n\nQ(Pv) = (Pv)^tAPv = v^t(P^tAP)v \n\nWhen $P$ is orthogonal, $A$ and $P^tAP$ both represent equivalent quadratic forms. Then the problem is reduced to find equivalent bilinear forms.\n\n\n\n\nQuestion 6:\n Let $A \\neq I_n$ be an $n \\times n$ matrix such that $A^2 = A$, where $I_n$ is the identity matrix of order $n$. Which of the following statements is false?\n\n\n\n\n$(I_n - A)^2 = I_n - A$.\n\n\ntrace $(A)=$ rank $(A)$.\n\n\nrank $(A)$ + rank $(I_n-A)$ = n.\n\n\nThe eigenvalus of $A$ are each equal to $1$.\n\n\n\n\nAnswer:\n The matrix satisfies the polynomial $x^2 - x = 0\\Rightarrow x(x-1) = 0$. Hence minimal polynomial is a factor of $x(x-1)$. Since this polynomial has linear factors, hence matrix is diagonalizable with eigenvalues $0$ or $1$ or both. Since $A \\neq I$ then at least eigenvalues will be $0$.\n\n\nHence \n4\n is correct choice.",
            "title": "Linear Algebra December 2015 Questions(Part B)"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01_4/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_4/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01_4/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01_4/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/",
            "text": "We introduced inference in the context of random variables, where there was a simple way to visualize what was going on in terms of joint probability tables. Marginalization referred to summing out rows or columns. Conditioning referred to taking a slice of the table and renormalizing so entries within that slice summed to 1. We then saw a more general story in terms of events. In fact, we saw that for many inference problems, using random variables to solve the problem is not necessary \u2013 reasoning with events was enough! A powerful tool we saw was Bayes' theorem.\n\n\nWe now return to random variables and build up to Bayes' theorem for random variables. This machinery will be extremely important as it will be how we automate inference for much larger problems in the later sections of the course, where we can have a large number of random variables at play, and a large amount of observations that we need to incorporate into our inference.\n\n\nProduct Rule for Random Variables\n\n\nWe know that product rule for event is \n\n\\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B})\n\nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$.\n\n\\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}\n\n\n\n\n\n\n\nIn general the formula for joint probabiliy distribution is given by\n\n\n\n\n  p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}\n\n\n\n\n\nMore than 2 random variable\n\n\nSuppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get \n\n\n\n\n\\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}\n\n\n\n\nWe can genrealize the formula as follows,\n\n\n\n\n\\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}\n\n\n\n\nExercise: The Product Rule for Random Variables - Medical Diagnosis Revisited\n\n\nLet's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.\n\n\nLet random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:\n\n\n\n\nMeanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):\n\n\n\n\nUsing the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.\n\n\n$p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$  \n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y    \n\n\n\n\n{('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}\n\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'positive')]))\n\n\n\n\n0.00999\n\n\n\n$p_{X,Y}(\\text {healthy}, \\text {negative}) = $\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('healthy', 'negative')]))\n\n\n\n\n0.98901\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {positive}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'positive')]))\n\n\n\n\n0.00099\n\n\n\n$p_{X,Y}(\\text {infected}, \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_Y[('infected', 'negative')]))\n\n\n\n\n0.00001\n\n\n\nBaye's Rule for Random Variable\n\n\nIn inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.\n\n\n\n\nAfter observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.\n\n\nThus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.\n\n\nBayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.\n\n\nBayes' theorem:\n Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y)\n0$. Then\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nImportant: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.\n\n\nProof: We have\n\n\n\n\np_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},\n\n\n\n\nwhere step (a) uses the definition of conditional probability (this step requires $p_Y(y)\n0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$. \n\n\nBAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW\n\n\nComputationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:\n\n\n\n\n\n\nFor each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so\n\n\n\n\n\\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),\n\n\n\n\nwhere we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!\n\n\nAlso, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.\n\n\nTo make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:\n\n\n\n\n\n\n\n\nWe fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.\n\n\n\n\n\n\n\n\nAn important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!\n\n\nMAXIMUM A POSTERIORI (MAP) ESTIMATION\n\n\nFor a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule. \nThe posterior is a distribution for what we are inferring\n. Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.\n\n\nThe value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write\n\n\n\n\n\\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).\n\n\n\n\nNote that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.\n\n\nIn general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.\n\n\nExercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued\n\n\nRecall the medical diagnosis setup from before, summarized in these tables:\n\n\n\n\n\n\nRecall that Bayes' theorem is given by\n\n\n\n\np_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}\n\n\n\n\nfor all values $x$ that random variable $X$ can take on.\n\n\nUse Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n\n\nprior_or_p_X = {\n    \nhealthy\n : 0.999,\n    \ninfected\n: 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y      \n\n\n\n\n{('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $  \n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'positive')]))\n\n\n\n\n0.90984\n\n\n\n$p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$\n\n\nprint(\n{0:.5f}\n.format(p_X_given_Y [('healthy', 'negative')]))\n\n\n\n\n0.99999\n\n\n\nWhat is the MAP estimate for $X$ given $Y = \\text{positive}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nWhat is the MAP estimate for $X$ given $Y=\\text{negative}$?\n\n\ncomp = 0\nMAP = \n\nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp \n val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)\n\n\n\n\nhealthy\n\n\n\nExercise: Complexity of Computing Bayes' Theorem for Random Variables\n\n\nThis exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.\n\n\nConsider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get\n\n\n\n\n\\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}\n\n\n\n\nSuppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.\n\n\nIn this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $\n$, e.g. $x\ny$ is $xy$.\n\n\nAnswer:\n Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "01+Product+Rule+for+Random+Variables"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#product-rule-for-random-variables",
            "text": "We know that product rule for event is  \\mathbb {P}(\\mathcal{A} \\cap \\mathcal{B}) = \\mathbb {P}(\\mathcal{A}) \\mathbb {P}(\\mathcal{A} | \\mathcal{B}) \nSimilarly the product rule for random varaible will be given by follwing formula if $p_Y(y) \\neq 0$. \\begin{align}\n    p_{x|y}(x|y) &= \\frac{p_{X,Y}(x,y)}{p_Y(y)} \\\\\n                 &\\Downarrow      \\\\\n    p_{X,Y}(x,y) &= p_Y(y) \\, p_{x|y}(x|y)\n\\end{align}    In general the formula for joint probabiliy distribution is given by     p_{X,Y}(x,y) = \\begin{cases}\n       p_Y(y) \\, p_{x|y}(x|y) & \\mbox{if } p_Y(y) > 0 \\\\\n       0                      & \\mbox{if } p_Y(y) = 0\n       \\end{cases}",
            "title": "Product Rule for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#more-than-2-random-variable",
            "text": "Suppose we have have three random variable then we can think any two as one random variable, for example trat last two as one random variable we get    \\begin{align}p_{X_1, X_2, X_3}(x_1, x_2, x_3) \n&= p_{X_1}(x_1)p_{X_2, X_3|X_1}(x_2, x_3|x_1) \\\\ \n&= p_{X_1}(x_1)p_{X2|X_1}(x_2|x_1)p_{X_3|X_1,X_2}(x_3|x_1,x_2) \n\\end{align}   We can genrealize the formula as follows,   \\begin{align}p_{X_1, X_2,\\ldots, X_N}(x_1, x_2,\\ldots,x_N) \n&= p_{X_1}(x_1) p_{X_2,\\ldots, X_N|X_1}(x_2,\\ldots, x_N|x_1) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) p_{X_3,\\ldots,X_N|X_1, X_2}(x_3,\\ldots,x_N|x_1,x_2) \\\\\n&= p_{X_1}(x_1) p_{X_2|X_1}(x_2|x_1) \\cdots p_{X_N|X_1, \\ldots, X_{N-1}}(x_n|x_1,\\ldots,x_{N-1})\n\\end{align}",
            "title": "More than 2 random variable"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-the-product-rule-for-random-variables-medical-diagnosis-revisited",
            "text": "Let's revisit the medical diagnosis problem we saw earlier. We now use random variables to construct a joint probability table.  Let random variable $X$ represent the patient's condition \u2014 whether \u201chealthy\" or \u201cinfected\", with the following distribution for $X$:   Meanwhile, the test outcome $Y$ for whether the patient is infected is either \u201cpositive\" (for the disease) or \u201cnegative\". As before, the test is $99\\%$ accurate, which means that the conditional probability table for $Y$ given $X$ is as follows (note that we also show how to write things out as a single table):   Using the product rule for random variables, what are the four entries for the joint probability table? Please provide the exact answer for these four quantities.  $p_{X,Y}(\\text {healthy}, \\text {positive}) = p_X(\\text {healthy})~p_{Y|X}(\\text {positive}~|~\\text {healthy})$    prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probability dist. of X and Y\np_X_Y = {} \nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\np_X_Y      {('healthy', 'negative'): 0.98901,\n ('healthy', 'positive'): 0.00999,\n ('infected', 'negative'): 1e-05,\n ('infected', 'positive'): 0.00099}  print( {0:.5f} .format(p_X_Y[('healthy', 'positive')]))  0.00999  $p_{X,Y}(\\text {healthy}, \\text {negative}) = $  print( {0:.5f} .format(p_X_Y[('healthy', 'negative')]))  0.98901  $p_{X,Y}(\\text {infected}, \\text {positive}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'positive')]))  0.00099  $p_{X,Y}(\\text {infected}, \\text {negative}) =$  print( {0:.5f} .format(p_X_Y[('infected', 'negative')]))  0.00001",
            "title": "Exercise: The Product Rule for Random Variables - Medical Diagnosis Revisited"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-rule-for-random-variable",
            "text": "In inference, what we want to reason about is some unknown random variable $X$, where we get to observe some other random variable $Y$, and we have some model for how $X$ and $Y$ relate. Specifically, suppose that we have some \u201cprior\" distribution $p_X$ for $X$; this prior distribution encodes what we believe to be likely or unlikely values that $X$ takes on, before we actually have any observations. We also suppose we have a \u201clikelihood\" distribution $p_{Y\u2223X}$.   After observing that $Y$ takes on a specific value $y$, our \u201cbelief\" of what $X$ given $Y=y$ is now given by what's called the \u201cposterior\" distribution $p_{X\u2223Y}(\u22c5\u2223y)$. Put another way, we keep track of a probability distribution that tells us how plausible we think different values $X$ can take on are. When we observe data $Y$ that can help us reason about $X$, we proceed to either upweight or downweight how plausible we think different values $X$ can take on are, making sure that we end up with a probability distribution giving us our updated belief of what $X$ can be.  Thus, once we have observed $Y=y$, our belief of what $X$ is changes from the prior $p_X$ to the posterior $p_{X\u2223Y}(\u22c5\u2223y)$.  Bayes' theorem (also called Bayes' rule or Bayes' law) for random variables explicitly tells us how to compute the posterior distribution $p_{X\u2223Y}(\u22c5\u2223y)$, i.e., how to weight each possible value that random variable $X$ can take on, once we've observed $Y=y$. Bayes' theorem is the main workhorse of numerous inference algorithms and will show up many times throughout the course.  Bayes' theorem:  Suppose that $y$ is a value that random variable $Y$ can take on, and $p_Y(y) 0$. Then   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Important: Remember that $p_{Y\u2223X}(\u22c5\u2223x)$ could be undefined but this isn't an issue since this happens precisely when $p_X(x)=0$, and we know that $p_{X,Y}(x,y)=0$ (for every $y$) whenever $p_X(x)=0$.  Proof: We have   p_{X\\mid Y}(x\\mid y)\\overset {(a)}{=}\\frac{p_{X,Y}(x,y)}{p_{Y}(y)}\\overset {(b)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{p_{Y}(y)}\\overset {(c)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X,Y}( x',y)}\\overset {(d)}{=}\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')},   where step (a) uses the definition of conditional probability (this step requires $p_Y(y) 0$, step (b) uses the product rule (recall that for notational convenience we're not separately writing out the case when $p_X(x)=0$, step (c) uses the formula for marginalization, and step (d) uses the product rule (again, for notational convenience, we're not separately writing out the case when $p_X(x\u2032)=0$.",
            "title": "Baye's Rule for Random Variable"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#bayes-theorem-for-random-variables-a-computational-view",
            "text": "Computationally, Bayes' theorem can be thought of as a two-step procedure. Once we have observed $Y=y$:    For each value $x$ that random variable $X$ can take on, initially we believed that $X=x$ with a score of $p_X(x)$, which could be thought of as how plausible we thought ahead of time that $X=x$. However now that we have observed $Y=y$, we weight the score $p_X(x)$ by a factor $p_{Y\u2223X}(y\u2223x)$, so   \\text {new belief for how plausible }X=x\\text { is:}\\quad \\alpha (x\\mid y)\\triangleq p_{X}(x)p_{Y\\mid X}(y\\mid x),   where we have defined a new table $\u03b1(\u22c5\u2223y)$ which is not a probability table, since when we put in the weights, the new beliefs are no longer guaranteed to sum to $1$ (i.e., $\\sum _{x}\\alpha (x\\mid y)$ might not equal $1$)! $\u03b1(\u22c5\u2223y)$ is an unnormalized posterior distribution!  Also, if $p_X(x)$ is already $0$, then as we already mentioned a few times, $p_{Y\u2223X}(y\u2223x)$ is undefined, but this case isn't a problem: no weighting is needed since an impossible outcome stays impossible.  To make things concrete, here is an example from the medical diagnosis problem where we observe $Y = \\text {positive}$:     We fix the fact that the unnormalized posterior table $\u03b1(\u22c5\u2223y)$ isn't guaranteed to sum to $1$ by renormalizing:   p_{X\\mid Y}(x\\mid y)=\\frac{\\alpha (x\\mid y)}{\\sum _{ x'}\\alpha ( x'\\mid y)}=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}.     An important note: Some times we won't actually care about doing this second renormalization step because we will only be interested in what value that $X$ takes on is more plausible relative to others; while we could always do the renormalization, if we just want to see which value of $x$ yields the highest entry in the unnormalized table $\u03b1(\u22c5\u2223y)$, we could find this value of x without renormalizing!",
            "title": "BAYES' THEOREM FOR RANDOM VARIABLES: A COMPUTATIONAL VIEW"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#maximum-a-posteriori-map-estimation",
            "text": "For a hidden random variable $X$ that we are inferring, and given observation $Y=y$, we have been talking about computing the posterior distribution $p_{X\u2223Y}(\u22c5|y)$ using Bayes' rule.  The posterior is a distribution for what we are inferring . Often times, we want to report which particular value of $X$ actually achieves the highest posterior probability, i.e., the most probable value $x$ that $X$ can take on given that we have observed $Y=y$.  The value that $X$ can take on that maximizes the posterior distribution is called the maximum a posteriori (MAP) estimate of $X$ given $Y=y$. We denote the MAP estimate by $\\widehat{x}_{\\text {MAP}}(y)$, where we make it clear that it depends on what the observed $y$ is. Mathematically, we write   \\widehat{x}_{\\text {MAP}}(y) = \\arg \\max _ x p_{X \\mid Y}(x | y).   Note that if we didn't include the \u201carg\" before the \u201cmax\", then we would just be finding the highest posterior probability rather than which value\u2013or \u201cargument\"\u2013x actually achieves the highest posterior probability.  In general, there could be ties, i.e., multiple values that $X$ can take on are able to achieve the best possible posterior probability.",
            "title": "MAXIMUM A POSTERIORI (MAP) ESTIMATION"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-bayes-theorem-for-random-variables-medical-diagnosis-continued",
            "text": "Recall the medical diagnosis setup from before, summarized in these tables:    Recall that Bayes' theorem is given by   p_{X\\mid Y}(x\\mid y)=\\frac{p_{X}(x)p_{Y\\mid X}(y\\mid x)}{\\sum _{ x'}p_{X}( x')p_{Y\\mid X}(y\\mid x')}   for all values $x$ that random variable $X$ can take on.  Use Bayes' theorem to compute the following probabilities: (Please be precise with at least 3 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)  prior_or_p_X = {\n     healthy  : 0.999,\n     infected : 0.001\n}\n\np_Y_given_X = {\n    ('positive', 'healthy' ): 0.01,\n    ('positive', 'infected'): 0.99,\n    ('negative', 'healthy' ): 0.99,\n    ('negative', 'infected'): 0.01\n}\n\n# p_X_Y stores the joint probabilty distribution of X and Y\np_X_Y = {}\nfor key, values in p_Y_given_X.items():\n    p_X_Y[key[::-1]] = values * prior_or_p_X[key[1]]\n\n\n# p_Y stores the marginal probabilty distribution Y    \np_Y = {}\nfor key, values in p_X_Y.items():\n    if key[1] in p_Y:\n        p_Y[key[1]] += values\n\n    else:\n        p_Y[key[1]] = values\n\n# p_X_given_Y stores the conditional probability dist. of X given Y.         \np_X_given_Y = {}\nfor key, values in p_X_Y.items():\n    p_X_given_Y[key] = values / p_Y[key[1]]\n\np_X_given_Y        {('healthy', 'negative'): 0.9999898889810116,\n ('healthy', 'positive'): 0.9098360655737705,\n ('infected', 'negative'): 1.0111018988493663e-05,\n ('infected', 'positive'): 0.09016393442622951}  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {positive}) = $    print( {0:.5f} .format(p_X_given_Y [('healthy', 'positive')]))  0.90984  $p_{X\\mid Y}(\\text {healthy}\\mid \\text {negative}) =$  print( {0:.5f} .format(p_X_given_Y [('healthy', 'negative')]))  0.99999  What is the MAP estimate for $X$ given $Y = \\text{positive}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'positive' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy  What is the MAP estimate for $X$ given $Y=\\text{negative}$?  comp = 0\nMAP =  \nfor key, val in p_X_given_Y.items():\n    if 'negative' in key and comp   val:\n        comp = val\n        MAP = key[0]\n\nprint(MAP)  healthy",
            "title": "Exercise: Bayes' Theorem for Random Variables - Medical Diagnosis, Continued"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/01+Product+Rule+for+Random+Variables/#exercise-complexity-of-computing-bayes-theorem-for-random-variables",
            "text": "This exercise is extremely important and gets at how expensive it is to compute a posterior distribution when we have many quantities we want to infer.  Consider when we have $N$ random variables $X_1, \\dots , X_ N$ with joint probability distribution $p_{X_1, \\dots , X_ N}$, and where we have an observation $Y$ related to $X_1, \\dots , X_ N$ through the known conditional probability table $p_{Y\\mid X_1, \\dots , X_ N}$. Treating $X=(X_1, \\dots , X_ N)$ as one big random variable, we can apply Bayes' theorem to get   \\begin{eqnarray}\n&& p_{X_1, X_2, \\dots, X_N \\mid Y}(x_1, x_2, \\dots, x_N \\mid y) \\\\\n&&\n= \\frac{p_{X_1, X_2, \\dots, X_N}(x_1, x_2, \\dots, x_N)\n        p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1, x_2, \\dots, x_N)}\n       {\\sum_{x_1'}\n        \\sum_{x_2'}\n        \\cdots\n        \\sum_{x_N'}\n          p_{X}(x_1',\n                x_2',\n                \\dots,\n                x_N')\n          p_{Y\\mid X_1, X_2, \\dots, X_N}(y\\mid x_1',\n                x_2',\n                \\dots,\n                x_N')}.\n\\end{eqnarray}   Suppose each $X_i$ takes on one of $k$ values. In the denominator, how many terms are we summing together? Express your answer in terms of $k$ and $N$.  In this part, please provide your answer as a mathematical formula (and not as Python code). Use \"$\\hat{}$\" for exponentiation, e.g., $x\\hat{}2$ to denotes $x^2$. Explicitly include multiplication using $ $, e.g. $x y$ is $xy$.  Answer:  Here we have $k$ choices of $X_1$ and $k$ choices of $X_2$ and so on. Hence total number of terms will be $k^N$.",
            "title": "Exercise: Complexity of Computing Bayes' Theorem for Random Variables"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/03+Probabilities+with+Events+and+Code/",
            "text": "Probabilities with Events and Code\n\n\nFrom the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!\n\n\nThe probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:\n\n\n\n\n\\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),\n\n\n\n\nwhere \u201c$\u225c$\" means \u201cdefined as\".\n\n\nWe can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:\n\n\ndef prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\n\n\n\nHere's an example of how to use the above function:\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\ndef prob_of_event(event, prob_space):\n    \n\n    Gives the probability of event for a given sample space.\n\n    \n prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n    \n prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n    \n\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ == \n__main__\n:\n    import doctest\n    doctest.testmod()\n\n\n\n\nprob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))\n\n\n\n\n0.5\n\n\n\nmodel = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)\n\n\n\n\n0.8\n\n\n\nExercise: Some Dice Rolling and Coin Flipping Events\n\n\nConsider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.\n\n\ntwo_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36\n\n\n\n\n\u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE\n\n\n\n\n{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}\n\n\n\nWe flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A}\n{\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A}\n{\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.\n\n\ncoin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()\n\n\n\n\nprint(prob_of_event(E1, coin), prob_of_event(E2, coin))\n\n\n\n\n1.0 0",
            "title": "03+Probabilities+with+Events+and+Code"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/03+Probabilities+with+Events+and+Code/#probabilities-with-events-and-code",
            "text": "From the videos, we see that an event is a subset of the sample space $\u03a9$. If you remember our table representation for a probability space, then an event could be thought of as a subset of the rows, and the probability of the event is just the sum of the probability values in those rows!  The probability of an event $A\u2286\u03a9$ is the sum of the probabilities of the possible outcomes in $A$:   \\mathbb {P}(\\mathcal{A})\\triangleq \\sum _{\\omega \\in \\mathcal{A}}\\mathbb {P}(\\text {outcome }\\omega ),   where \u201c$\u225c$\" means \u201cdefined as\".  We can translate the above equation into Python code. In particular, we can compute the probability of an event encoded as a Python set event, where the probability space is encoded as a Python dictionary prob_space:  def prob_of_event(event, prob_space):\n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total  Here's an example of how to use the above function:  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  def prob_of_event(event, prob_space):\n     \n    Gives the probability of event for a given sample space.\n\n      prob_of_event({'rainy', 'snowy'}, {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3})\n    0.5\n      prob_of_event({'benign', 'malignant'}, {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2})\n    0.8\n     \n    total = 0\n    for outcome in event:\n        total += prob_space[outcome]\n    return total\n\nif __name__ ==  __main__ :\n    import doctest\n    doctest.testmod()  prob_space = {'sunny': 1/2, 'rainy': 1/6, 'snowy': 1/3}\nrainy_or_snowy_event = {'rainy', 'snowy'}\nprint(prob_of_event(rainy_or_snowy_event, prob_space))  0.5  model = {'benign': 0.3, 'malignant': 0.5, 'not sure': 0.2}\nevent = {'benign', 'malignant'}\nprob_of_event(event, model)  0.8",
            "title": "Probabilities with Events and Code"
        },
        {
            "location": "/Computational Probability/week01_4/chess/chess/03+Probabilities+with+Events+and+Code/#exercise-some-dice-rolling-and-coin-flipping-events",
            "text": "Consider rolling two six-sided dice with faces numbered $1$ through $6$. Again, we use the sample space from earlier $\u03a9={(1,1),(1,2),\u2026,(6,5),(6,6)}$. What is the event that the sum of the faces is $7$? Enter your answer as a Python set.  two_dice = {}\nfor i in range(6):\n    for j in range(6):\n        two_dice[(i+1, j+1)] = 1/36  \u03a9 = set(two_dice.keys())\nE = {x for x in \u03a9 if sum(x) == 7}\n\nE  {(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}  We flip a coin. The coin landing with any face shown means that it's okay if either heads shows or tails shows. This corresponds to the event $\\mathcal{A} {\\text {any-face-shows}}={ \\text {heads},\\text {tails}}$, which happens to be equal to the whole sample space. Meanwhile, simultaneously having both heads and tails show face up refers to an impossible situation, corresponding to the \u201cempty set\" event $\\mathcal{A} {\\text {both-faces-simultaneously-show}}={ }$, also denoted as $\\phi$.  coin = {'heads': 1/2, 'tails': 1/2}\n\u03a9 = set(coin.keys())\nE1 = set(['heads', 'tails'])\nE2 = set()  print(prob_of_event(E1, coin), prob_of_event(E2, coin))  1.0 0",
            "title": "Exercise: Some Dice Rolling and Coin Flipping Events"
        }
    ]
}