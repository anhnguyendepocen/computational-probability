{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "**Mutual information:** For two discrete random variables $X$ and $Y,$ the mutual information between $X$ and $Y,$ denoted as $I(X;Y),$ measures how many much information they share. Specifically,\n",
    "\n",
    "$$I(X;Y)\\triangleq D(p_{X,Y}\\parallel p_{X}p_{Y}),$$\n",
    " \n",
    "where $p_{X}p_{Y}$ is the distribution we get if $X$ and $Y$ were actually independent (i.e., if $X$ and $Y$ were actually independent, then we know that the joint probability table would satisfy $\\mathbb {P}(X=x,Y=y)=p_{X}(x)p_{Y}(y)).$\n",
    "\n",
    "The mutual information could be thought of as how far $X$ and $Y$ are from being independent, since if indeed they were independent, then $I(X;Y)=0.$\n",
    "\n",
    "On the opposite extreme, consider when $X=Y.$ Then we would expect $X$ and $Y$ to share the most possible amount of information. In this scenario, we can write $p_{X,Y}(x,y)=p_{X}(x)\\mathbf{1}\\{ x=y\\},$ and so\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "I(X;Y)\n",
    "&=&D(p_{X,Y}\\parallel p_{X}p_{Y})\\\\\n",
    "&=&\\sum_{x}\\sum_{y}p_{X,Y}(x,y)\\log_{2}\\frac{1}{p_{X}(x)p_{Y}(y)}\\\\\n",
    "&&\\quad\n",
    "  -\\sum_{x}\\sum_{y}p_{X,Y}(x,y)\\log_{2}\\frac{1}{p_{X,Y}(x,y)}\\\\\n",
    "&=&\\sum_{x}\\sum_{y}p_{X}(x)\\mathbf{1}\\{x=y\\}\\log_{2}\\frac{1}{p_{X}(x)p_{Y}(y)}\\\\\n",
    "&&\\quad\n",
    "  -\\sum_{x}\\sum_{y}p_{X}(x)\\mathbf{1}\\{x=y\\}\\log_{2}\\frac{1}{p_{X}(x)\\mathbf{1}\\{x=y\\}}\\\\\n",
    "&=&\\sum_{x}p_{X}(x)\\log_{2}\\Big(\\frac{1}{p_{X}(x)}\\Big)^{2}-\\sum_{x}p_{X}(x)\\log_{2}\\frac{1}{p_{X}(x)}\\\\\n",
    "&=&2\\sum_{x}p_{X}(x)\\log_{2}\\frac{1}{p_{X}(x)}-\\sum_{x}p_{X}(x)\\log_{2}\\frac{1}{p_{X}(x)}\\\\\n",
    "&=&\\sum_{x}p_{X}(x)\\log_{2}\\frac{1}{p_{X}(x)}\\\\\n",
    "&=&H(X).\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "This is not surprising: if $X$ and $Y$ are the same, then the number of bits they share is exactly the average number of bits needed to store $X$ (or $Y$), namely $H(X)$ bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "ans": "0.00226"
    }
   },
   "source": [
    "### Exercise: Mutual Information\n",
    "\n",
    "Consider the following joint probability table for random variables $X$ and $Y.$ We'll compute the mutual information $I(X; Y)$ of random variables $X$ and $Y$ step-by-step.\n",
    "\n",
    "| $X$ vs $Y$ | 0       | 1      | 2      |\n",
    "|:----------:|:-------:|:------:|:------:|\n",
    "| 0          | $0.10$  | $0.09$ | $0.11$ |\n",
    "| 1          | $0.08$  | $0.07$ | $0.07$ |\n",
    "| 2          | $0.18$  | $0.13$ | $0.17$ |\n",
    "\n",
    "Mutual information is about comparing the joint distribution of $X$ and $Y$ with what the joint distribution would be if $X$ and $Y$ were actually independent.\n",
    "\n",
    "In Python (where we won't explicitly store the labels of the rows and columns):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "joint_prob_XY = np.array([[0.10, 0.09, 0.11], \n",
    "                          [0.08, 0.07, 0.07], \n",
    "                          [0.18, 0.13, 0.17]])\n",
    "```\n",
    "\n",
    "The marginal distributions $p_X$ and $p_Y$ are given by:\n",
    "\n",
    "```python\n",
    "prob_X = joint_prob_XY.sum(axis=1)\n",
    "prob_Y = joint_prob_XY.sum(axis=0)\n",
    "```\n",
    "\n",
    "Next, we produce what the joint probability table would be if $X$ and $Y$ were actually independent:\n",
    "\n",
    "```python\n",
    "joint_prob_XY_indep = np.outer(prob_X, prob_Y)\n",
    "```\n",
    "\n",
    "At this point, we have the joint distribution of $X$ and $Y$ (denoted $p_{X,Y}$) stored in code as `joint_prob_XY`, and also what the joint distribution would be if $X$ and $Y$ were independent (denoted $p_ X p_ Y$) stored in code as `joint_prob_XY_indep`. The mutual information of $X$ and $Y$ is precisely given by the KL divergence between $p_{X,Y}$ and $p_ X p_ Y:$\n",
    "\n",
    "$$I(X;Y) = D(p_{X,Y}\\parallel p_{X}p_{Y}) = \\sum _ x \\sum _ y p_{X, Y}(x, y) \\log _2 \\frac{p_{X, Y}(x, y)}{p_ X(x) p_ Y(y)}.$$\n",
    " \n",
    "**Question:** What is $I(X; Y)?$ Provide just the number and don't write â€œbits\" at the end. We suggest that you code a Python function that computes the information divergence between any two distributions, and then you can just plug in `joint_prob_XY` and `joint_prob_XY_indep`.\n",
    "\n",
    "(Please be precise with at least 5 decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n",
    "\n",
    "**Answer:** {{ans}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "joint_prob_XY = np.array([[0.10, 0.09, 0.11], \n",
    "                          [0.08, 0.07, 0.07], \n",
    "                          [0.18, 0.13, 0.17]])\n",
    "prob_X = joint_prob_XY.sum(axis=1)\n",
    "prob_Y = joint_prob_XY.sum(axis=0)\n",
    "joint_prob_XY_indep = np.outer(prob_X, prob_Y)\n",
    "\n",
    "f = lambda x, y: joint_prob_XY[x][y] * \\\n",
    "    np.log2(joint_prob_XY[x][y] / (prob_X[x] * prob_Y[y]))\n",
    "ans = sum([f(x,y) for x in range(3) for y in range(3)])\n",
    "ans = \"{0:.5f}\".format(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Are X and Y independent?\n",
    "\n",
    "**Ans:** No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018400000000000055"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "norm(joint_prob_XY - joint_prob_XY_indep, np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
