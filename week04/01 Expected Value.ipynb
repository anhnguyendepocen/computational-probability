{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 13,
        "hidden": false,
        "row": 0,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Expected value of random variable\n",
    "\n",
    "Expected value of random variable is generalization of taking average of numbers. It is similar to taking weighted average, where each value of random variable is multiplied by it's probability. \n",
    "\n",
    "$$\\mathbb{E}[X] = \\sum_{x \\in \\mathcal{X}} x \\cdot p_X(x) $$\n",
    "\n",
    "Also in terms of conditional probability,\n",
    "$$\\mathbb{E}[X \\mid Y=y] = \\sum_{x \\in \\mathcal{X}} x \\cdot p_{X\\mid Y}(x\\mid y)$$\n",
    "\n",
    "In general, let $f$ any function from $\\mathbb{R}$ to $\\mathbb{R}$, then \n",
    "\n",
    "$$ \\mathbb{E}[f(X)] = \\sum_{x \\in \\mathcal{X}} f(x) \\cdot p_X(x) $$\n",
    "\n",
    "Thus expectection gives a single number associated with a probability table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": null,
        "height": 7,
        "hidden": false,
        "row": 13,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Shanon Information Content\n",
    "\n",
    "Shanon information content for event $A$ is defined as $\\log_2 \\frac{1}{\\mathbb{P}(A)}$. \n",
    "\n",
    "!!! Note \n",
    "    The Shanon information content is high for an event which has very low probability. Hence if something of low probabilty happens it gives us a large number of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": null,
        "height": 9,
        "hidden": false,
        "row": 31,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "### Shanon Entropy\n",
    "\n",
    "How many bits to store $n$ `i.i.d.` samples form $p_X$?\n",
    "\n",
    "$$H(X) \\sim n \\left [ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{p_X(x)}\\right ]$$ \n",
    "\n",
    "Also, in terms of expectation, \n",
    "\n",
    "$$H(X) = \\mathbb{E}\\left[ \\log_{2}{\\frac{1}{p_X(x)}}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": null,
        "height": 14,
        "hidden": false,
        "row": 31,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Information divergence \n",
    "\n",
    "$$\\text{Entropy:} \\quad H(X) \\sim n \\left [ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{p_X(x)}\\right ]$$\n",
    "\n",
    "Suppose we want to distribute $X \\sim q$, then the entropy will be \n",
    "\n",
    "$$ \\sum_{x\\in \\mathcal{X}} p_X(x) \\cdot \\log_2\\frac{1}{q(x)}$$\n",
    "\n",
    "This is fundamental theorem of information theory that if we use any distribution other than $p$ then we have to use more bits. \n",
    "\n",
    "Information divergence of $p$ and $q$ is defined as \n",
    "\n",
    "$$\\begin{align} D(p\\parallel q) &= \\sum_{x\\in \\mathcal{X}} p_X(x)  \\log_2\\frac{1}{q(x)} - \\sum_{x\\in \\mathcal{X}} p_X(x)  \\log_2\\frac{1}{p(x)} \\\\[2ex]\n",
    "&= \\sum_{x\\in \\mathcal{X}} p_X(x)\\log_2\\frac{p(x)}{q(x)} \\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": null,
        "height": 11,
        "hidden": false,
        "row": 20,
        "width": null
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## Geometric Distribution\n",
    "\n",
    "Suppose we have a biased coin with $\\mathbb{P}(H) = p$ and we toss it repeatedly, then the sample space will look like \n",
    "\n",
    "$$\\Omega = \\{H, TH, TTH, \\ldots \\}$$\n",
    "Then \n",
    "$$\\mathbb{P}(TH) = \\mathbb{P}(T)\\cdot \\mathbb{P}(H) = (1-p)p$$\n",
    "\n",
    "Let $X$ is the random variable defined as \n",
    "\n",
    "$$X = \\#\\text{ tosses untill first heads }$$\n",
    "Then \n",
    "$$\\begin{align}\\mathbb{P}(X=x) &= \\mathbb{P}(x-1 \\text{ tails followed by heads}) \\\\\n",
    "&= (1-p)^{x-1}p \\end{align}\n",
    "$$\n",
    "This is probability mass function of random variable $X$. If we normalize we will get the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
