{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shanon Information Content\n",
    "\n",
    "First, let's consider storing an integer that isn't random. Let's say we have an integer that is from $0,1,\\dots ,63$. Then the number of bits needed to store this integer is $\\log _{2}(64)=6$ bits: you tell me $6$ bits and I can tell you exactly what the integer is.\n",
    "\n",
    "A different way to think about this result is that we don't a priori know which of the $64$ outcomes is going to be stored, and so each outcome is equally likely with probability $\\frac{1}{64}$. Then the number of bits needed to store an event $\\mathcal{A}$ is given by what's called the “Shannon information content\" (also called self-information):\n",
    "\n",
    "$$\\log _{2}\\frac{1}{\\mathbb {P}(\\mathcal{A})}.$$\n",
    " \n",
    "In particular, for an integer $x\\in \\{ 0,1,\\dots ,63\\}$, the Shannon information content of observing $x$ is\n",
    "\n",
    "$$\\log _{2}\\frac{1}{\\mathbb {P}(\\text {integer is }x)}=\\log _{2}\\frac{1}{1/64}=\\log _{2}64=6\\text { bits}.$$\n",
    " \n",
    "If instead, the integer was deterministically $0$ and never equal to any of the other values $1,2,\\dots ,63,$ then the Shannon information content of observing integer $0$ is\n",
    "\n",
    "$$\\log _{2}\\frac{1}{\\mathbb {P}(\\text {integer is }0)}=\\log _{2}\\frac{1}{1}=0\\text { bits}.$$\n",
    " \n",
    "This is not surprising in that a outcome that we deterministically always observe tells us no new information. Meanwhile, for each integer $x\\in \\{ 1,2,\\dots ,63\\},$\n",
    "\n",
    "$$\\log _{2}\\frac{1}{\\mathbb {P}(\\text {integer is }x)}=\\log _{2}\\frac{1}{0}=\\infty \\text { bits}.$$\n",
    " \n",
    "How could observing one of the integers $\\{ 1,2,\\dots ,63\\}$ tell us infinite bits of information?! Well, this isn't an issue since the event that we observe any of these integers has probability $0$ and is thus impossible. An interpration of Shannon information content is how surprised we would be to observe an event. In this sense, observing an impossible event would be infinitely surprising.\n",
    "\n",
    "It is possible to have the Shannon information content of an event be some fractional number of bits (e.g., $0.7$ bits). The interpretation is that from many repeats of the underlying experiment, the average number of bits needed to store the event is given by the Shannon information content, which can be fractional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Shannon Information Content\n",
    "\n",
    "I have an integer in mind, uniformly distributed between $0$ and $127$. You can keep guessing what my number is until you get it right (and each time you guess, I tell you whether you got it right). Each time you guess a number wrong, you discard that number so as to not guess it again.\n",
    "\n",
    "For example, if you guess wrong the first time, then the Shannon information content of guessing wrong is\n",
    "\n",
    "$$\\log _2\\frac{1}{\\frac{127}{128}}=\\log _{2}\\frac{128}{127}=0.0113\\dots \\text { bits}.$$\n",
    " \n",
    "If you guess wrong the second time, then the Shannon information content of the second guess is\n",
    "\n",
    "$$\\log _2\\frac{1}{\\frac{126}{127}}=\\log _{2}\\frac{127}{126}=0.0114\\dots \\text { bits}.$$\n",
    " \n",
    "If you guess right on the third time, then the Shannon information content of the third guess is\n",
    "\n",
    "$$\\log _2\\frac{1}{\\frac{1}{126}}=\\log _{2}\\frac{126}{1}=6.9772\\dots \\text { bits}.$$\n",
    " \n",
    "**Question:** For the above example where the third guess is right, what is the sum of the Shannon information content of the three guesses? \n",
    "\n",
    "**Solution:**\n",
    "$$\\begin{eqnarray}\n",
    "&&\n",
    "\\log_2 \\frac1{\\frac{127}{128}}\n",
    "+\\log_2 \\frac1{\\frac{126}{127}}\n",
    "+\\log_2 \\frac1{\\frac{1}{126}} \\\\\n",
    "&&=\n",
    "\\log_2 \\frac{128}{127}\n",
    "+\\log_2 \\frac{127}{126}\n",
    "+\\log_2 126 \\\\\n",
    "&&=\n",
    "\\log_2 128 - \\log_2 127\n",
    "+ \\log_2 127 - \\log_2 126\n",
    "+ \\log_2 126 \\\\\n",
    "&&=\n",
    "\\log_2 128 \\\\\n",
    "&&=\n",
    "\\boxed{7\\text{ bits}}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "**Question:** Suppose you guessed right only after 5 tries. What is the sum of the Shannon information content of these 5 guesses? \n",
    "\n",
    "**Solution:**\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "&&\\log_2 \\frac1{\\frac{127}{128}}\n",
    "+\\log_2 \\frac1{\\frac{126}{127}}\n",
    "+\\log_2 \\frac1{\\frac{125}{126}}\n",
    "+\\log_2 \\frac1{\\frac{124}{125}}\n",
    "+\\log_2 \\frac1{\\frac{1}{124}} \\\\\n",
    "&&=\n",
    "\\log_2 \\frac{128}{127}\n",
    "+\\log_2 \\frac{127}{126}\n",
    "+\\log_2 \\frac{126}{125}\n",
    "+\\log_2 \\frac{125}{124}\n",
    "+\\log_2 124 \\\\\n",
    "&&=\n",
    "\\log_2 128 - \\log_2 127\n",
    "+ \\log_2 127 - \\log_2 126\n",
    "+ \\log_2 126 \\\\\n",
    "&&\\quad\n",
    "- \\log_2 125\n",
    "+ \\log_2 125 - \\log_2 124\n",
    "+ \\log_2 124 \\\\\n",
    "&&=\n",
    "\\log_2 128 \\\\\n",
    "&&=\n",
    "\\boxed{7\\text{ bits}}.\n",
    "\\end{eqnarray}$$ \n",
    "\n",
    "**Question:** Suppose you guess right after $k$ tries $(k \\in \\{ 1, 2, \\dots , 128\\})$. If you sum up the Shannon information content for all the guesses up and including the one in which you guess right, does this total number of bits depend on $k$? (While we aren't asking for you to justify your answer, we encourage you to be able to do so! For example, if your answer is \"Yes\" then you should be able to come up with two specific cases for two different number of tries before guessing right that yield different number of total bits, and if your answer is \"No\" then you should be able to show why the total number of bits gained is always the same.)\n",
    "\n",
    "**Solution:** The answer is no. The previous two parts provide a clue: the sum we're computing is a telescoping sum where all the terms cancel out except for the first one: $\\log _2 128 = 7.$\n",
    "\n",
    "In general, if we guess right after $k$ tries, then the total amount of information “learned\" is:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "&&\n",
    "\\left[\n",
    "\\sum_{i=1}^{k-1}\n",
    "  \\log_2 \\frac1{\\frac{128 - i}{128 - (i - 1)}}\n",
    "\\right]\n",
    "+ \\log_2 (128 - (k - 1)) \\\\\n",
    "&&=\n",
    "\\left[\n",
    "\\sum_{i=1}^{k-1}\n",
    "  \\log_2 \\frac{128 - (i - 1)}{128 - i}\n",
    "\\right]\n",
    "+ \\log_2 (128 - (k - 1)) \\\\\n",
    "&&=\n",
    "\\left[\n",
    "\\sum_{i=1}^{k-1}\n",
    "  \\log_2 (128 - (i - 1)) - \\log_2(128 - i)\n",
    "\\right]\n",
    "+ \\log_2 (128 - (k - 1)) \\\\\n",
    "&&= \\log_2 128 - \\log_2 127 + \\log_2 127 - \\cdots \\\\\n",
    "&&\\quad\n",
    "- \\log_2 (128 - (k - 1)) + \\log_2 (128 - (k - 1)) \\\\\n",
    "&&= \\log_2 128 \\\\\n",
    "&&= 7\\text{ bits}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Put another way, $7$ bits of information are needed before you know the number, and with wrong guesses, you learn very few bits of information (although as the number of possibilities shrinks, wrong guesses provide more and more bits of information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shanon Entropy\n",
    "\n",
    "To go from the number of bits contained in an event to the number of bits contained in a random variable, we simply take the expectation of the Shannon information content across the possible outcomes. The resulting quantity is called the entropy of a random variable:\n",
    "\n",
    "$$H(X)=\\sum _{x}p_{X}(x)\\underbrace{\\log _{2}\\frac{1}{p_{X}(x)}}_{\\text {Shannon information content of event }X=x}.$$\n",
    " \n",
    "The interpretation is that on average, the number of bits needed to encode each i.i.d. sample of a random variable X is $H(X)$. In fact, if we sample n times i.i.d. from $p_{X}$, then two fundamental results in information theory that are beyond the scope of this course state that: (a) there's an algorithm that is able to store these n samples in $nH(X)$ bits, and (b) we can't possibly store the sequence in fewer than $nH(X)$ bits!\n",
    "\n",
    "**Example:** If $X$ is a fair coin toss “heads\" or “tails\" each with probability $1/2$, then\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H(X)\n",
    "&=& p_X(\\text{heads}) \\log_2 \\frac1{p_X(\\text{heads})}\n",
    "+ p_X(\\text{tails}) \\log_2 \\frac1{p_X(\\text{tails})} \\\\\n",
    "&=& \\frac12 \\cdot \\underbrace{\\log_2 \\frac1{\\frac{1}{2}}}_1\n",
    "+ \\frac12 \\cdot \\underbrace{\\log_2 \\frac1{\\frac{1}{2}}}_1 \\\\\n",
    "&=& 1 \\text{ bit}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "**Example:** If $X$ is a biased coin toss where heads occurs with probability $1$ then\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "H(X)\n",
    "&=& p_X(\\text{heads}) \\log_2 \\frac1{p_X(\\text{heads})}\n",
    "+ p_X(\\text{tails}) \\log_2 \\frac1{p_X(\\text{tails})} \\\\\n",
    "&=& 1 \\cdot \\underbrace{\\log_2 \\frac11}_0\n",
    "+ 0 \\cdot \\cdot \\underbrace{\\log_2 \\frac10}_1 \\\\\n",
    "&=& 0 \\text{ bits},\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "where $0 \\log _2 \\frac10 = 0 \\log _2 1 - 0 \\log _2 0 = 0$ using the convention that $0 \\log _2 0 \\triangleq 0$. (Note: You can use l'Hopital's rule from calculus to show that $\\lim _{x\\rightarrow 0} x \\log x = 0 and \\lim _{x\\rightarrow 0} x \\log \\frac1x = 0$.)\n",
    "\n",
    "Notation: Note that entropy $H(X) = \\sum _ x p_ X(x) \\log _2 \\frac{1}{p_ X(x)}$ is in the form of an expectation! So in fact, we can write an expectation:\n",
    "\n",
    "$$H(X) = \\mathbb {E}\\left[\\log _2 \\frac{1}{p_ X(X)}\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def H(pX):\n",
    "    \"\"\"\n",
    "    Retrun Shanon entropy of given probability.\n",
    "    >>> pX = {-1: 999999/1000000, 999   : 1/1000000}\n",
    "    >>> H(pX)\n",
    "    2.137426288890686e-05\n",
    "    \n",
    "    >>> coin = {'H': 1/2, 'T': 1/2}\n",
    "    >>> H(coin)\n",
    "    1.0\n",
    "    \"\"\"\n",
    "    from math import log\n",
    "    return - sum([value * log(value,2) for key, value in pX.items()]) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "H_1": "0.0000214",
     "H_2": "0.0000214",
     "H_3": "0.4689956"
    }
   },
   "source": [
    "### Exercise: Shannon Entropy\n",
    "\n",
    "Entropy gives a very different measure for uncertainty than variance, which we saw previously. Whereas variance $\\text {var}(X) = \\mathbb {E}[(X-\\mathbb {E}[X])^2]$ measures how far a random variable is expected to deviate from its expected value $\\mathbb {E}[X]$, entropy measures how many bits are needed on average to store each i.i.d. sample of a random variable $X$.\n",
    "\n",
    "Let's return to the three lotteries from earlier. Here, random variables $L_1, L_2,$ and $L_3$ represent the amount won (accounting for having to pay \\$1):\n",
    "\n",
    "|$L_1$      |            $p$           |        $L_2$ |            $p$           |   $L_3$ |       $p$      |\n",
    "|----------:|:------------------------:|-------------:|:------------------------:|--------:|:--------------:|\n",
    "|        -1 | $\\frac{999999}{1000000}$ |           -1 | $\\frac{999999}{1000000}$ |      -1 | $\\frac{9}{10}$ |\n",
    "|   -1+1000 |    $\\frac{1}{1000000}$   |   -1+1000000 |    $\\frac{1}{1000000}$   |   -1+10 | $\\frac{1}{10}$ |\n",
    "\n",
    "Compute the following entropies in bits. Please input just the number and do not include the text \"bits\" at the end. (Please be precise with at least $7$ decimal places, unless of course the answer doesn't need that many decimal places. You could also put a fraction.)\n",
    "\n",
    "- $H(L_1) =$ {{H_1}}    \n",
    "- $H(L_2) =$ {{H_2}}\n",
    "- $H(L_3) =$ {{H_3}}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_L1 = {-1: 999999/1000000, 999   : 1/1000000}\n",
    "p_L2 = {-1: 999999/1000000, 999999: 1/1000000}\n",
    "p_L3 = {-1:      9/10     , 9     : 1/10     }\n",
    "\n",
    "H_1 = '{0:0.7f}'.format(H(p_L1))\n",
    "H_2 = '{0:0.7f}'.format(H(p_L2))\n",
    "H_3 = '{0:0.7f}'.format(H(p_L3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've computed some entropies, let's answer some general questions about entropy.\n",
    "\n",
    "**Question:** In computing the entropy of a random variable $X$, we look at the probabilities in the probability table of $X.$ Did we have to look at the labels in the probability table (note that the labels correspond to the alphabet of $X$)?\n",
    "\n",
    "[$\\times    $] Yes<br>\n",
    "[$\\checkmark$] No\n",
    "\n",
    "**Solution:** The answer is no. You can see this in the Python code: we don't need to look at the alphabet $\\mathcal{X}$.\n",
    "\n",
    "Suppose for a random variable $X$, we take its probability table and shuffle the ordering of the probabilities (but otherwise keep the labels the same). For example, if $X$ is a biased coin flip with probability of heads $3/4$, suppose we took its table and instead have the probability of tails be $3/4$ and the probability of heads be $1/4$.\n",
    "\n",
    "**Question:** By shuffling the ordering of the probabilities, does the entropy of the random variable change?\n",
    "\n",
    "[$\\times    $] Yes<br>\n",
    "[$\\checkmark$] No\n",
    "\n",
    "**Solution:** The answer is no. Since what the labels are does not actually matter (of course, we cannot have the same label appear twice in the table though), we can permute the labels (or the probabilities) and get the same entropy.\n",
    "\n",
    "Finally, let's think about how small and large entropy could be.\n",
    "\n",
    "**Question:** We've seen an example where the entropy is $0$ bits. Can entropy be negative? (While there is an intuitive answer for this, see if you can show it mathematically.)\n",
    "\n",
    "[$\\times    $] Yes<br>\n",
    "[$\\checkmark$] No\n",
    "\n",
    "**Solution:** The answer is no. First, note that for any probability $p\\in [0,1]$, the Shannon information content corresponding to this probability is $\\log_2 (1/p)$, which has a value from $0$ to infinity, i.e., Shannon information content is always nonnegative. Shannon entropy is just a weighted average of Shannon information content, where the weights are nonnegative. A nonnegative weighted sum of a collection of nonnegative numbers remains nonnegative.\n",
    "\n",
    "**Question:** For a random variable $X$ that takes on one of two values, one with probability $p$ and the other with probability $1-p$, plot the entropy $H(p)$ as a function of $p$ in Python. For what value of $p$ is the entropy maximized (i.e., what value of $p$ yields the largest $H(X))$? Please provide an exact answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAF5CAYAAACm4JG+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3Xd4lGXaxuHfTS8qqCjoClZU7CRrQayLAbsCtoiiYMNu\nrLura11lXRUrCq4FEIi4LioqLgJ2WSwJdkDXgh2xoQhIu78/nslnyCaQMjPPlOs8jjlg3rzvzJUh\nZO55qrk7IiIiIunUKHYAERERyT8qQERERCTtVICIiIhI2qkAERERkbRTASIiIiJppwJERERE0k4F\niIiIiKSdChARERFJOxUgIiIiknYqQERERCTtMqIAMbM9zWyCmX1hZivM7NBaXLOPmZWZ2WIze9/M\nTkhHVhEREWm4jChAgNbAG8AZwGo3pzGzTYAngKnAjsCtwD1mVpS6iCIiIpIslmmb0ZnZCuBwd5+w\ninOuBw5w9x0qHSsF2rj7gWmIKSIiIg2QKS0gdbUbMKXKsUlAtwhZREREpI6ytQDpAMytcmwusJaZ\nNY+QR0REROqgSewA6WJm6wK9gE+AxXHTiIiIZJUWwCbAJHf/LhkPmK0FyNdA+yrH2gM/ufuvNVzT\nCxiT0lQiIiK5rR8wNhkPlK0FyH+AA6oc65k4XpNPAEaPHk2XLl1SFEuqKikp4eabb44dI68k+zX/\n4gt49VWYMQM++AA+/hiWLg1fW3dd2Hxz2GIL6Nw5/P13v4O11oJGKe7gdYeFC2HuXPjvf8Ptww/D\nn59//tt5HTuGfNtvD7vsAlttlfxs+jlPP73m6TVz5kyOO+44SLyXJkNGFCBm1hrYArDEoc3MbEfg\ne3f/zMwGAxu6e8VaH8OAMxOzYe4DegBHAKuaAbMYoEuXLhQUFKTi25BqtGnTRq93mjX0NZ83D555\nBqZMgalTQ8HRqBEUFMDee8OZZ4Y38+23h3btkhg8iX75Bd59F955B95+G956C+65B267LRRN++4L\nPXqE2xZbgNnqH3NV9HOefnrNo0naEIaMKECA3wPPEtYAceCmxPGRwEDCoNOOFSe7+ydmdhBwM3AO\n8DlwkrtXnRkjIquxYAG88EIoNqZMCW/WAF26wMEHhzfpvfeGtm3j5qyL1q1Da8cuu/x2bMkSmD79\nt8LqrLNg+XLo1Om3YqRHD+jQIV5ukXySEQWIuz/PKmbkuPuAao69ABSmMpdIrlqyBP79bxg7FiZM\ngEWLYKONwhvwRRfBH/4AG24YO2VyNWsGe+0VbldfDT/9tHLhdf/9oSVkn33g2GOhb19Ye+3YqUVy\nV0YUICKSeitWwIsvhqLjn/+EH34I3SiXXw69e8OWWza8KyKbrLVWaOE5+OBwf+5cePLJ8Pqceiqc\ncQYceCD06xfOadkybl6RXKMCRFKquLg4doS8U/k1d4c334QxY+DBB8PgzI03hkGDoLg4FCAStG8P\nAweG21dfwbhxoRg56ihYc81QpPXrF1qHmlT5zamf8/TTa579Mm4p9lQxswKgrKysTAOXJOctWBC6\nFO66C2bODINFjz46dC1065ZfLR0N9cEHUFoaipHZs2H99eHEE+Hss0O3lUg+KC8vp7CwEKDQ3cuT\n8ZjZuhKqiFTjs8/g4ovDG2NJCeywAzz1FHz5JdxxB+y+u4qPuurcOXRTzZwJZWWhiBs+HDbdNPz9\ntddiJxTJTipARHLAK6/AMceEN8V//ANOOy1Mn33wQdh/f2jaNHbC7GcWpiLffHMo9IYMCa/7LrvA\nHnvAv/4VZtWISO2oABHJUsuWwcMPh1aN3XYLn85vuSW8OV5/fViAS1JjzTVDF8z778Mjj0DjxnDE\nEWFNkVtuCTNsRGTVVICIZJlff4Xbbw9vdkceGaaXPvoozJoV1rZYY43YCfNH48Zw+OHw/PPw+uvQ\nvXuYxrzRRnDhhWFRNxGpngoQkSyxfDk88ABsvTWcd15o9i8rg+eeg8MOC2+GEk9hIYweDZ98EgrB\nf/wDNtsMrroKfv45djqRzKMCRCTDucMTT0DXrtC/f/jznXfCm50mdGWe3/0OrrsOPvoojMUZPDjs\nkXP77aH1SkQCFSAiGWzatLBy5yGHhD1Mpk+H8ePDMumS2dZdF268MYwTOfjg0Gq19dahcFyxInY6\nkfhUgIhkoHffDd0q3buH5vunngobxO26a+xkUledOsF994VN8XbaCY4/PrRiTZwYWrdE8pUKEJEM\n8tVXMGBAWKH0nXfCCqbl5WEqrdbvyG7bbBNmzEybFjb2O+igsO9MWVnsZCJxqAARyQArVsCwYaFr\n5YknwrbxM2eGha4a6X9pTunWLQwcfvLJsB/PLruEReM0UFXyjX61iUT2zjuw555w+ulhLYnZs8Ms\nimbNYieTVDELG92Vl4c1W+6+G7bdNuxMLJIvVICIRLJoEVx6aRgP8N13YS2Je+6BddaJnUzSpUmT\nsF7Iu+/CdtuFcT99+8IXX8ROJpJ6KkBEIpgyJYzzuPFGuOyysGPtXnvFTiWxbLJJ6JJ58EF4+eXQ\nFTd0qJZ2l9ymAkQkjebNC7MgiorCaplvvQVXXAHNm8dOJrGZhR2LZ86E4uLQDde9e/gZEclFKkBE\n0sD9t1VMJ04M0zKffRa22ip2Msk0a68ddtt98cUwMLWgAP74Ry1iJrlHBYhIis2fD/36hVVM998/\n7NkyYICm1cqq7bEHzJgRlnIfMiTMnnn//dipRJJHBYhICk2fHgaZPvkkjB0b1vVYb73YqSRbNGsW\nBipPnw6//BJaQ+6/XwuYSW5QASKSAsuXh/1A9tgD2reHN94I/foi9VFQEBYsO+ooGDgwrA8zf37s\nVCINowJEJMm++CIMMr3sstB3/8ILsOmmsVNJtltjjTB26MEHwziinXaC//wndiqR+lMBIpJEEybA\njjuGxcSmToW//hWaNo2dSnLJ0UeHFrUNNggL2F17rabrSnbKmALEzM40s4/NbJGZTTeznWtx/ntm\nttDMZprZ8enKKlLVokVh2mTFBnJvvgn77hs7leSqTTcNLWt/+hP85S+w337w+eexU4nUTUYUIGZ2\nNHATcAXQFXgTmGRm7Wo4/3TgWuByYBvgSmComR2UlsAilXzwQdil9p57wuJRjz4K7ar9yRVJniZN\n4Jprwi7JH3wQWt6eeip2KpHay4gCBCgBhrv7KHefBQwCFgIDazj/uMT5D7v7J+4+DrgbuCQ9cUWC\nKVNC8fHrr/Daa3DGGZpeK+m1zz6hxW333cMOuzfeqFkykh2iFyBm1hQoBKZWHHN3B6YA3Wq4rDmw\nuMqxxcAuZtY4FTlFKnMPO9buv38oQF55JSytLhLDuuvCY4+FQc8XXQQnngiLq/6GFMkw0QsQoB3Q\nGJhb5fhcoEMN10wCTjazAgAz+z1wEtA08XgiKbNkCZx6Kpx7brg98QS0bRs7leS7Ro3C1O8xY2Dc\nuDAG6auvYqcSqVkmFCD1cQ3wFPAfM1sKPAKMSHxtRaxQkvvmzQsD/kaNClMib7oJGqvNTTLIsceG\nZdznzIGddw7rh4hkoiaxAwDfAsuB9lWOtwe+ru4Cd19MaAE5LXHeV8BpwM/uPm9VT1ZSUkKbNm1W\nOlZcXEyxVomS1XjrLTj00DDj5dlnQ5+7SCbaeWd4/XU4/PAwVff++8P0XZHaKC0tpbS0dKVj81Ow\n8p15BoxWMrPpwCvufm7ivgGfAre5+w21fIzngM/cvdrpuInumrKysjIKCgqSE1zyxiOPhF1sO3cO\nfe2dOsVOJLJ6ixbBySeHbQAuuyzsK9MoW9u9Jary8nIKCwsBCt29PBmPmSk/ikOAU8ysv5ltDQwD\nWpHoVjGzwWY2suJkM+tsZv3MbAsz28XMHgS2BS6NkF1ymHtYTKxPHzjgAHjpJRUfkj1atoTRo2Hw\n4LBg2RFHwIIFsVOJBJnQBYO7P5RY8+NqQpfKG0CvSt0pHYCOlS5pDFwAbAksBZ4Fdnf3T9OXWnLd\nkiVh340xY+DKK8OCT/r0KNnGLMyO2XbbMD5kjz3CeiEbbBA7meS7jChAANz9TuDOGr42oMr9WYD6\nUSRlFi4MnxanTg0zCo46KnYikYY55JCwd8z++4ciZPJk2Gyz2Kkkn+nznEgVP/4IPXuGpa6ffFLF\nh+SO7baDl18OM7e6d4e3346dSPKZChCRSr7+GvbeG2bODK0f++0XO5FIcm28cZim26ED7LUXTJsW\nO5HkKxUgIgkffxyapr/9NrR+7Lpr7EQiqdG+PTz3XFi9t6gIJk2KnUjykQoQEeCdd0KTtFloot52\n29iJRFKrTZtQePzhD2F8yLhxsRNJvlEBInlv+vTQFL3++mGa7SabxE4kkh4tW8L48WGRsuJiGD48\ndiLJJxkzC0YkhsmTw2qRBQXw+OPa00XyT9OmMHIkrL02DBoE338fpu1qV2dJNRUgkrcefjisi1BU\nBP/8J7RqFTuRSByNGsGtt4Zddf/8Z/juO7jhBhUhkloqQCQvjRkD/fvDMcfAiBHhU6BIPjODK66A\nddaBc86BX36BO+9UESKpowJE8s7DD4fio39/uPderW4qUtnZZ0Pr1nDSSdCiBQwZoiJEUkMFiOSV\nxx8Pg+2OPhruuUfFh0h1Bg6ExYvhzDNDEXLddSpCJPlUgEjemDQpLK9+2GEwalRYDVJEqnfGGfDr\nr3D++WG2zOWXx04kuUYFiOSFZ58Ns1169gxbkzfRT77IapWUhJaQP/8ZmjeHSy6JnUhyiX4NS857\n+WU4+GDYc88w26VZs9iJRLLHn/4EixaFqbktWsC558ZOJLlCBYjktFdfhQMOgJ13hkcfDb9ARaRu\nrroqtIScd174P3TaabETSS5QASI56403oFevsN/FE09onQ+R+jKD668PRcigQaEIOeGE2Kkk26kA\nkZz0zjthJ9sttoCJE2GNNWInEsluZmGxssWLwyyZZs3CjDKR+lIBIjnn/fdD8fG734WZL23axE4k\nkhvMYNiwUIQcf3wYmNqnT+xUkq1UgEhO+eqrsLT6OuvAlCnhTxFJnkaN4L77YMmS0ALy9NOw996x\nU0k20jJMkjMWLAizXZYvD78U11svdiKR3NSkSVhLZ889w/T2mTNjJ5JspAJEcsKyZWF10w8+gCef\nhI02ip1IJLc1awb/+lf4v3bAAfD117ETSbZRASJZzx3OOiuM93j4Ydhxx9iJRPJDmzZhkPfSpaH1\nccGC2Ikkm6gAkaz397/D8OFw991hpVMRSZ+OHUOr4+zZYUzIsmWxE0m2UAEiWa20NKzQ+Je/hKmB\nIpJ+O+0UVhl+6ik455zQKimyOipAJGu98AKceGKYDnjVVbHTiOS3/fcPU3TvugtuvDF2GskGGVOA\nmNmZZvaxmS0ys+lmtvNqzu9nZm+Y2S9m9qWZ3WtmmnSZJ2bNCqPvu3eHe+7RVuEimeDkk+HSS+Hi\ni2HcuNhpJNNlRAFiZkcDNwFXAF2BN4FJZtauhvO7AyOBfwDbAEcAuwB3pyWwRDV3bhh1v+GGMH68\nNpcTySTXXAP9+kH//vDii7HTSCbLiAIEKAGGu/sod58FDAIWAjX16u8GfOzuQ919jrtPA4YTihDJ\nYb/8Ekbb//prGH3ftm3sRCJSmRncey/svjscdlgYnCpSnegFiJk1BQqBqRXH3N2BKUC3Gi77D9DR\nzA5IPEZ74EjgydSmlZiWLw+j7GfODKPuO3WKnUhEqtO8eWid7NAhtFZ+803sRJKJohcgQDugMTC3\nyvG5QIfqLki0eBwHjDOzJcBXwA/AWSnMKZFddlkoPB56CLp2jZ1GRFZl7bXDrJiFC+GII8JaISKV\nZeVeMGa2DXArcCXwNLABcCOhG+bkVV1bUlJCmyq7kxUXF1OsbR0z2r/+BX/7W1jz48ADY6cRkdrY\neOOwOOC++8JFF8Ett8ROJLVRWlpKaWnpSsfmz5+f9OcxjzxhO9EFsxDo6+4TKh0fAbRx997VXDMK\naOHuR1U61h14EdjA3au2pmBmBUBZWVkZBQUFyf9GJGXeew923TU05Y4bpxkvItlm6NCwWvEDD8Bx\nx8VOI/VRXl5OYWEhQKG7lyfjMaN3wbj7UqAM6FFxzMwscX9aDZe1Aqqut7cCcEBvTzlk/nzo3Tt8\nkrrvPhUfItnojDPCrJhTT4U33oidRjJF9AIkYQhwipn1N7OtgWGEImMEgJkNNrORlc5/HOhrZoPM\nbNNE68etwCvuri2RcsSKFXDCCWHa7SOPwBprxE4kIvVhFhYp69IlfKD4/vvYiSQTZEQB4u4PARcC\nVwMzgB2AXu4+L3FKB6BjpfNHAucDZwJvA+OAmUDfNMaWFLvuOnjsMRg9Gjp3jp1GRBqiZcswM+bn\nn8NstuXLYyeS2DKiAAFw9zvdfRN3b+nu3dz99UpfG+Duf6hy/lB3397d13D3jdz9BHf/Kv3JJRWe\negouvxyuuCKs+yEi2W/jjeHBB2HKlLB/k+S3jClARCp8+CEceywcdFAoQkQkd+y3HwweHG7jx8dO\nIzGpAJGM8ssvoY+4XbswYr6RfkJFcs5FF4W1QU44ISwsKPlJv94lY7iHzaw++igMOtUy6yK5ySzM\nauvUKXzg+Omn2IkkBhUgkjFuuSX0D993H2y3Xew0IpJKa64ZPmh89VWYortiRexEkm4qQCQjPPdc\naJa98EI46qjVni4iOWDLLUNX62OPhTEhkl9UgEh08+aFaXl77aVfQiL55tBDw4yYyy+HF16InUbS\nSQWIROUOAwbAsmUwZgw0ycrdiUSkIa64Arp3D8u0//BD7DSSLipAJKqhQ8MOt/ffDxtsEDuNiMTQ\nuHFYcPDnn8Ny7ZG3KJM0UQEi0bz9dhjzcdZZWmxMJN916gT/+EfYPfe++2KnkXRQASJRLFoUxn10\n7gw33BA7jYhkgiOOCFPxzzkHZs+OnUZSTQWIRHHhhWHF09JSaNEidhoRyRS33AIbbRQ+oPz6a+w0\nkkoqQCTtJkyAO++Em27Seh8isrLWrcMHk3fegUsvjZ1GUkkFiKTVl1/CwIFh6t3pp8dOIyKZqKAA\n/va38CHl6adjp5FUUQEiabNiRVjxsFkzuPfesByziEh1zjsPevYMvzO++SZ2GkkFFSCSNjfeCM88\nE1Y+bNcudhoRyWSNGsHIkeGDy4ABmpqbi1SASFq8/nroz734YujRI3YaEckGHTrAiBEwcSLccUfs\nNJJsKkAk5RYsCCPad9oJrr46dhoRySYHHhim5V50Ebz1Vuw0kkwqQCTlzj477HhZWhrGf4iI1MX1\n18NWW4UPMgsXxk4jyaICRFLq0UdDE+odd8AWW8ROIyLZqEWL8AHmo4/gz3+OnUaSRQWIpMz334ep\ntocdBiecEDuNiGSzbbaB666D226DadNip5FkUAEiKXP++bB4cVh0TFNuRaShzjkHdt01rCW0eHHs\nNNJQKkAkJZ56Kkyhu/lm2HDD2GlEJBc0bhw2qvv4Y7jyythppKFUgEjSzZ8fttTu1UtdLyKSXF26\nhOLjhhvgtddip5GGyJgCxMzONLOPzWyRmU03s51Xce79ZrbCzJYn/qy4vZ3OzFK9iy+GH3+Eu+9W\n14uIJN+FF4Zp/QMHwpIlsdNIfWVEAWJmRwM3AVcAXYE3gUlmVtN6mecAHYANEn9uBHwPPJT6tLIq\nU6eGwuOGG6BTp9hpRCQXNW0aumJmzQoDUyU7ZUQBApQAw919lLvPAgYBC4GB1Z3s7j+7+zcVN2AX\noC0wIl2B5X8tWACnnAL77BO6YEREUmXHHcOU3Guv1QJl2Sp6AWJmTYFCYGrFMXd3YArQrZYPMxCY\n4u6fJT+h1Nall8LXX8M994R9HEREUunSS2HrrcNeMcuWxU4jdZUJbxPtgMbA3CrH5xK6V1bJzDYA\nDgD+kfxoUlsvvQS33x6aQzffPHYaEckHzZqFrpg33gibXUp2yYQCpKFOBH4AHoucI28tWgQnnQS7\n7RaWXRcRSZeddw6DUq+8EmbOjJ1G6qJJ7ADAt8ByoH2V4+2Br2tx/QBglLvXqgGupKSENm3arHSs\nuLiY4uLi2lwu1bjySpgzBx57LMzTFxFJpyuvDNs+nHQSvPiifg81VGlpKaWlpSsdmz9/ftKfx8Jw\ni7jMbDrwirufm7hvwKfAbe5+wyqu24cwdmQ7d19l7WtmBUBZWVkZBQUFScue7157LbR8XHst/PGP\nsdOISL56+WXYc08YMgTOOy92mtxTXl5OYWEhQKG7lyfjMTOlC2YIcIqZ9TezrYFhQCsSs1rMbLCZ\njazmupMIhYsa3iL49dcw+Ktr19AEKiISS/fuoQv4z3+GDz+MnUZqIyMKEHd/CLgQuBqYAewA9HL3\neYlTOgAdK19jZmsBvYF70hhVKhk8GGbPDoPAmmRCZ56I5LXrroMOHeDkkyEDGvdlNTLmbcPd7wTu\nrOFrA6o59hOwRqpzSfX++99QgFxyCeywQ+w0IiLQujUMHw49e8LYsdCvX+xEsioZ0QIi2cUdzj0X\nNtggNHeKiGSKoiI44ojQLfzTT7HTyKqoAJE6e/xxmDgRbrkFWrWKnUZEZGVDhoTi46qrYieRVVEB\nInWyaFFo/dh/fzjssNhpRET+V8eO8Je/wK23wjvvxE4jNVEBInXyt7/Bl1/Cbbdpp1sRyVznnx9W\nZT7rLA1IzVQqQKTWPvwQrr8eLroIOneOnUZEpGbNmoXtIZ5/Hh58MHYaqY4KEKm1c8+F9u018FRE\nskPPntC3L1xwgQakZiIVIFIrjz8OTz6pgacikl2GDIH58+Hqq2MnkapUgMhqLVoE55wDvXrB4YfH\nTiMiUnudOsFll4UPT+++GzuNVKYCRFbr+uvDwNPbb9fAUxHJPhqQmplUgMgqffRRmPly4YUaeCoi\n2al58/AB6rnnYNy42GmkggoQWaVzz4X119fAUxHJbj17Qp8+YUDqzz/HTiOgAkRW4Yknwu2WW8Ie\nCyIi2ezmm+GHHzQgNVOoAJFqVQw87dkTeveOnUZEpOEqD0h9773YaUQFiFTr73+Hzz/XwFMRyS0X\nXACbbqoBqZlABYj8jzlzfht4uuWWsdOIiCRPxYDUZ5+Fhx+OnSa/qQCR/3HZZdC2rQaeikhu6tUL\nDjkE/vhHWLIkdpr8pQJEVjJjBoweHQZprbFG7DQiIqnxt7/BJ5/AsGGxk+QvFSDy/9zDRnNdusCA\nAbHTiIikzjbbwEknhQ9b8+fHTpOf6lyAmFkjM9vXzC43s3vNrNTMbjOzAWbWMRUhJT2efhqmTg0r\nnzZpEjuNiEhqXXllmPF3/fWxk+SnWhcgZtbSzC4DPgMmAgcAbYHlwBbAVcDHZjbRzHZLRVhJneXL\n4eKLYa+94OCDY6cREUm9DTcMs2JuvjnM+pP0qksLyPvADsApwFru3s3d+7r7ce5+oLt3AjYHXgQe\nNLNTUpBXUmT0aHjrLbjhBk27FZH8cdFFsOaacPnlsZPkn7oUID3d/Sh3n+juS6s7wd3nuPtgoDPw\nTFISSsotWhRmvhx5JOyyS+w0IiLps+aacMUVMGIEvP127DT5pdYFiLvPrMO5S939w/pFknS77Tb4\n+mu47rrYSURE0u/UU2GLLeCSS2InyS/1ngVjZp3MbE8z62VmBWbWPJnBJD2+/TYUHqefHv4Diojk\nm6ZNYfBgeOqpMBBf0qNOBYiZbWJm15vZHOBj4HngKeB1YL6ZTTazI82sPrNrzjSzj81skZlNN7Od\nV3N+MzO71sw+MbPFZvaRmZ1Y1+fNd9deG6bf/uUvsZOIiMTTpw/stlsYjL9iRew0+aEus2BuA94E\nNgUuA7YB2gDNgA7AgcBLwNXAW6srIKo89tHATcAVQNfE80wys3aruOyfwL7AAGBLoBiYXdvnFPjo\nIxg6NKwGuN56sdOIiMRjFgbhl5fDgw/GTpMf6rLawy/AZu7+XTVf+4Yw6PQZ4Coz2x/oCLxWy8cu\nAYa7+ygAMxsEHAQMBP5e9eTE4++ZyPNj4vCndfheBLj00lB4nHde7CQiIvHtsQccfnjYhqJv37Bv\njKROXQah/qmG4qO6c//t7uNrc66ZNQUKgf/veXN3B6YA3Wq47BBCt88lZva5mc02sxvMrEVtnlPg\ntddClX/NNdCqVew0IiKZYfDgsCbI0KGxk+S+Bi3FbmbrJwai7mlm69fzYdoBjYG5VY7PJXTtVGcz\nQgvItsDhwLnAEYB+ZGqhYsn1bbeFE06InUZEJHNsvTWccgr89a/www+x0+S2ei24bWZrAncCxxCK\nB4DlZjYOONPdU72yfiNgBXCsuy9IZDof+KeZneHuv9Z0YUlJCW3atFnpWHFxMcXFxanMm1EmToTn\nn4cnn4TGjVd/vohIPrniCnjggdAa8vf/GQSQ+0pLSyktLV3p2PwUbJhjobejjheFQqMrcDbwn8Th\nbsCtwBvufkwdHqspsBDo6+4TKh0fAbRx997VXDMC2N3dt6x0bGvgXWDL6tYgMbMCoKysrIyCgoLa\nxss5y5bBjjtC+/ZhuplWPRUR+V9XXRUKkNmzYeONY6eJr7y8nMLCQoBCdy9PxmPWtwvmYGCgu09y\n958St0mEZdoPqcsDJVZVLQN6VBwzM0vcn1bDZS8DG5pZ5dELWxFaRbSi/yqMHAnvvReqehUfIiLV\nu+ACaNs2rBItqVHfAuQ7oLr2mPlAfXrNhgCnmFn/REvGMKAVMALAzAab2chK549NZLjfzLqY2V6E\n2TL3rqr7Jd8tWRK2nj7qKPj972OnERHJXGusEbpixoyBmbVeB1zqor4FyF+BIWb2/4NEE3+/Abim\nrg/m7g8BFxLWEJlB2PSul7vPS5zSgTCtt+L8X4Aiwm68rwEPAI8RBqNKDUaMgM8+C/+pRERk1QYO\nhI02CgNSJfnqOwZkBrAF0Jzf1t/oBPwKfFD5XHfPiAEX+T4GZMkS6NwZunXTIjsiIrV1111w5pmh\n63rrrWOniScVY0DqNQsGeDQZTy7pM3JkaP2YODF2EhGR7DFwYNgv669/hdGjY6fJLfUqQNz9qmQH\nkdRZsiTs+XLkkWHtDxERqZ3mzeFPf4Kzzw57Zm21VexEuaMue8FozkSWGjUKPv1UG86JiNTHSSfB\nBhtoLEiYTgJ3AAAgAElEQVSy1WUQ6rtmdoyZNVvVSWbW2czuMrM/NjCbJMHSpaH144gjYLvtYqcR\nEck+Fa0gY8fC++/HTpM76lKAnE2YqfK1mY0zs4vMrJ+Z9TWzk81siJm9CrwB/ATclYrAUjejRsEn\nn8Dll8dOIiKSvdQKknx12Yxuqrv/HjiUsPttP+AOYAxwJdAZGAVs5O6XpGE5dlmNpUvDfxa1foiI\nNEyLFvDHP4Z1QdQKkhx1HoTq7i8BL6UgiyTZAw+E1o8JE1Z7qoiIrMbJJ4fl2a+9NswslIap80Jk\nZtbUzKaaWedUBJLkqGj96NsXtt8+dhoRkexX0QoyejR88MHqz5dVq3MBkti7ZYcUZJEkGj0aPv5Y\nYz9ERJLplFPCZp7XXhs7Sfar71Lso4GTkhlEkqei9aNPH9hBpaKISNJUbgX5739jp8lu9V0JtQkw\n0Mz2I+xk+0vlL7r7+Q0NJvU3Zgx89BGMHx87iYhI7jnlFPjb30IryP33x06TverbArIdUA78DGwJ\ndK102yk50aQ+li0LrR+9e8OOO8ZOIyKSe1q2hEsuCQP9P/wwdprsVd+l2PdNdhBJjjFjwn+Ihx+O\nnUREJHedeupvrSD33Rc7TXaqbwsIAGa2hZn1MrOWiftarj2iZcvgmmvg8MNhJ7VDiYikTEUryKhR\nagWpr3oVIGa2rplNBd4HJgIbJL50r5ndlKxwUjdjx4b/CJr5IiKSeqedBuutF3bLlbqrbwvIzcBS\noBOwsNLxccD+DQ0ldVfR+nHYYdC1a+w0IiK5r2VLuPjisCjZRx/FTpN96luA9AQucffPqxz/ANi4\nYZGkPsaPD1PCtOOtiEj6nHYarLsu3KS2/zqrbwHSmpVbPiqsA/xa/zhSH+5w443QowcUFsZOIyKS\nP1q1grPOCtNxv/02dprsUt8C5EWgf6X7bmaNgIuBZxucSurkxRfhtdfgggtiJxERyT+nnx7+vEt7\nwNdJfQuQi4FTzewpoBnwd+AdYC/gkiRlk1q66SbYZhvYX6NvRETSrl07OPFEuOMOWLw4dprsUa8C\nxN3fISxA9hLwGKFLZjzQ1d01ISmNZs8Ou91eeCFoErSISBwlJTBvXliiXWqnvkux4+7zAW3HE9mQ\nIdChAxx7bOwkIiL5q3PnsAbTTTfBwIHQqEGrbOWHBr9EZtbazAaa2Zlm1jkZoaR2vvkmTP86+2xo\n3jx2GhGR/HbBBTBrFkycGDtJdqhTAWJmnczseTP72cwmm1knwp4w9wC3A2+Y2V71CZIoYD42s0Vm\nNt3Mdl7FuXub2Yoqt+Vmtn59njtb3XknNG4MgwbFTiIiIrvvDrvtpim5tVXXFpAbCYNOBxGm4U4i\nrIbaAWgPPAVcWdcQZnY0cBNwBWFDuzeBSWbWbhWXOdA58dwdgA3c/Zu6Pne2WrgQhg6Fk06CddaJ\nnUZERMzCeLznnoPXX4+dJvPVtQDZCzjX3ccAA4CtgOvc/Rt3nwdcA+xQjxwlwHB3H+Xus/itwBm4\nmuvmJZ77m3wqPiDsP/D993DeebGTiIhIhcMPh802UytIbdS1AFkfmAPg7t8TioS5lb7+NbB2XR7Q\nzJoChcDUimPu7sAUoNuqLiV0+XxpZk+b2e51ed5stmJFGHzap0/4QRcRkczQuHGYEfPPf8KcObHT\nZLb6DEL1Gv5eX+2AxqxcyJC436GGa74CTgP6An2Az4DnzCwv9oB9/HH44IPQ1CciIpllwABYay24\n9dbYSTJbfabhXm1mFcuwNwMuNbP5ifutkhNr1dz9fcLYkwrTzWxzQlfOCenIENONN8Iee8Cuu8ZO\nIiIiVbVuDWecEQqQyy+Htm1jJ8pMdS1AXiCM+6gwDajaCfBCHR/zW2A5YRBrZe0JXTq19SrQfXUn\nlZSU0KZNm5WOFRcXU1xcXIenimf6dHjpJXj00dhJRESkJmedBTfcAHffHXbMzSalpaWUlpaudGz+\n/Pk1nF1/FoZbxGVm04FX3P3cxH0DPgVuc/cbavkYTwM/ufsRNXy9ACgrKyujoKAgScnT78gj4c03\nw1xzLXQjIpK5Tj4ZnnoKPv4YmjWLnaZhysvLKQy7nRa6e3kyHjNT3sKGAKeYWX8z2xoYRujOGQFg\nZoPNbGTFyWZ2rpkdamabm9m2ZnYLsC9wR4TsafPRRzB+PJx/vooPEZFMd/758OWXMG5c7CSZqdZv\nY2b2RzOr1RgPM9vVzA6q7WO7+0PAhcDVwAzCVN5eiam9EAajdqx0STPCuiFvAc8B2wM93P252j5n\nNrrllrDmR//+qz9XRETi2mYbOPDAMG4vAzobMk5dPkdvA8wxszvN7AAzW6/iC2bWxMx2MLMzzGwa\nMA74uS5B3P1Od9/E3Vu6ezd3f73S1wa4+x8q3b/B3Tu7e2t3X8/de7h7XceeZJXvv4d774Uzz4RW\naRnqKyIiDXXhhfDWWzBlSuwkmafWBYi79wf2A5oCY4GvzWyJmf0M/EpouRgIjAK2zvWCIN2GDQvr\nf5xxRuwkIiJSW/vsAwUFoRVEVlanWTDu/iZhrMZpwI5AJ6AlYSbLG+7+bfIjyq+/wu23h66X9fNq\ntxsRkexmFjap69cvtITsUJ+1wnNUXTejW8vM1gLWAD4EngUmEqbALqn0dUmisWPh66/DgCYREcku\nRx4JHTtqefaq6jqX4kfgh1XcKr4uSeIeWj8OPhi22mr154uISGZp2hTOOQcefBDmzVv9+fmirgXI\nvsAfErcehLEfx1c6VvF1SZJXX4UZMzT2Q0Qkmw0YELpj7r8/dpLMUdcxIM9Xvm9my4Hp7v5RUlPJ\n/xs2DDbZBHr2jJ1ERETqa9114aijYPjwMDNGazllzkJkUo3vvw9NdqedFnZYFBGR7HX66WFBycmT\nYyfJDCpAMtioUbB8OQwcGDuJiIg01G67hVkww4bFTpIZklGAaH23FHAPP6R9+2rqrYhILjALrSAT\nJsDnn8dOE1+dxoCY2fgqh1oAw8zsl8oH3b1PQ4Plu+eeg9mzQ3+hiIjkhn794KKL4J574MorY6eJ\nq64tIPOr3EYDX1ZzXBpo2DDo0gX22it2EhERSZY114TjjoN//AOWLYudJq66zoIZkKog8puvvw67\n3t50U2iyExGR3DFoUPiQ+fjj0Lt37DTxaBBqBrrvvrBwjXa9FRHJPTvuGAak5vtgVBUgGWb58jDu\no7gY2raNnUZERFLh9NPh6afhv/+NnSQeFSAZ5t//hk8/DU10IiKSm448EtZeG+6+O3aSeFSAZJi7\n7oLCQth559hJREQkVVq2DMuz33cfLF4cO00cKkAyyJw5MHGiWj9ERPLBaafBd9/Bv/4VO0kcKkAy\nyN13hylaxcWxk4iISKptuSX06BFavvORCpAMsWQJ3HtvmPnSunXsNCIikg6DBsHLL8Pbb8dOkn4q\nQDLEY4/B3LnqfhERySeHHQYdOuTnqtcqQDLEXXfBnnvCttvGTiIiIunStCmcfHLYfHTBgthp0ksF\nSAaYNQuefTbMCxcRkfxyyinwyy9QWho7SXqpAMkAw4dDu3bQR1v4iYjknU6d4KCDQku459H+8ipA\nIlu0CEaMgIEDoXnz2GlERCSGQYNgxgx47bXYSdInYwoQMzvTzD42s0VmNt3MarUUl5l1N7OlZlae\n6oypMG4c/PhjmA8uIiL5qVcv2GST/JqSmxEFiJkdDdwEXAF0Bd4EJplZu9Vc1wYYCUxJecgUGTYs\n/OBttlnsJCIiEkvjxnDqqfDgg/DDD7HTpEdGFCBACTDc3Ue5+yxgELAQGLia64YBY4DpKc6XEm+/\nDa+8oqm3IiICJ50Ey5bB6NGxk6RH9ALEzJoChcDUimPu7oRWjW6ruG4AsClwVaozpsqoUWHw6UEH\nxU4iIiKxrb8+HHggPPBA7CTpEb0AAdoBjYG5VY7PBTpUd4GZdQauA/q5+4rUxkuN5cth7Fg45pgw\nD1xEROT448NA1NmzYydJvSaxA9SVmTUidLtc4e4fVhyu7fUlJSW0adNmpWPFxcUUp3kDlmeegS+/\nDEuvi4iIABx8MLRtG1pB/vrXOBlKS0sprbIoyfz585P+POaRJx0numAWAn3dfUKl4yOANu7eu8r5\nbYAfgGX8Vng0Svx9GdDT3Z+r5nkKgLKysjIKCgpS8J3UTf/+8OqrMHMmWK3LJxERyXWnnQaTJsFH\nH0GjTOinAMrLyyksLAQodPekzDqN/q25+1KgDOhRcczMLHF/WjWX/ARsB+wE7Ji4DQNmJf7+Sooj\nN9iCBWH75eOPV/EhIiIrO/54mDMHXnwxdpLUypQumCHACDMrA14lzIppBYwAMLPBwIbufkJigOp7\nlS82s2+Axe4+M62p6+mRR2DhQjjuuNhJREQk03TvDptuGrph9t47dprUid4CAuDuDwEXAlcDM4Ad\ngF7uPi9xSgegY6R4SVfxQ7XxxrGTiIhIpjELrSD//GdYLTtXZUQBAuDud7r7Ju7e0t27ufvrlb42\nwN3/sIprr3L3+AM7auGLL2DKlPDDJSIiUp3jjoOffoIJE1Z/brbKmAIkX4wdG/Z8OeKI2ElERCRT\nde4Mu+2W22uCqABJswcegMMOgyozgUVERFbSvz/8+9/wzTexk6SGCpA0evPNsPy6ul9ERGR1jjoq\nTMN98MHYSVJDBUgajRoF660HPXvGTiIiIplu3XXDVh2jRsVOkhoqQNJk2bIw/uPYY7X0uoiI1E7/\n/lBWFhatzDUqQNJk6lT4+mt1v4iISO0deCCsvXZuDkZVAZImo0ZBly6QAavAi4hIlmjeHI4+GkaP\nhhVZufVqzVSApMHPP4fVT7X0uoiI1NXxx8Nnn8Hzz8dOklwqQNJg/HhYvBj69YudREREsk23brD5\n5rnXDaMCJA0eeAD22Qc6dYqdREREsk3F0uwPPxz2EcsVKkBS7PPP4ZlnNPhURETq77jjQnf+Y4/F\nTpI8KkBSbMyYMIiob9/YSUREJFttvjnsvntudcOoAEkh9zD7pXdvWGut2GlERCSb9e8PTz8dlnTI\nBSpAUmjGDHjvPXW/iIhIwx11FDRuDKWlsZMkhwqQFHrgAWjfHoqKYicREZFst/bacPDBudMNowIk\nRZYtC1XqscdCkyax04iISC7o3z+0rr/7buwkDacCJEUmT4a5c9X9IiIiyXPAAWGTulxoBVEBkiJj\nxsA228BOO8VOIiIiuaJZs7A0+9ixYaJDNlMBkgJLlsATT4QBQ1p6XUREkunII8PS7K+/HjtJw6gA\nSYFnn4X588P0WxERkWTaY4/QDfPII7GTNIwKkBQYPz4sGrP99rGTiIhIrmnSBA47DP71r+zuhlEB\nkmTLl8Ojj0KfPup+ERGR1OjTB95/H2bOjJ2k/lSAJNm0afDNN+GHQ0REJBV69IA11wwt7tkqYwoQ\nMzvTzD42s0VmNt3Mdl7Fud3N7CUz+9bMFprZTDM7L515a/LII7DBBrDLLrGTiIhIrmrRAg48MLvH\ngWREAWJmRwM3AVcAXYE3gUlm1q6GS34Bbgf2BLYGrgH+amYnpyFujdxDNdq7NzTKiFdWRERyVZ8+\nUF4On3wSO0n9ZMrbZAkw3N1HufssYBCwEBhY3cnu/oa7j3P3me7+qbuPBSYRCpJoZsyAOXPU/SIi\nIql3wAFht/VsbQWJXoCYWVOgEJhacczdHZgCdKvlY3RNnPtcCiLW2vjxsM46sNdeMVOIiEg+WHNN\n6Nkze8eBRC9AgHZAY2BuleNzgQ6rutDMPjOzxcCrwFB3vz81EWvnkUfgkEOgadOYKUREJF/07g0v\nvxy2/sg2mVCANMQehNaTQUBJYixJFLNmwXvvqftFRETS55BDwpjDxx6LnaTuMmGf1m+B5UD7Ksfb\nA1+v6kJ3n5P467tm1gG4Ehi3qmtKSkpo06bNSseKi4spLi6uQ+T/9cgj0Lo1FBU16GFERERqrV07\n2Hvv0A1z6qnJeczS0lJKS0tXOjZ//vzkPHgl5hmwjJqZTQdecfdzE/cN+BS4zd1vqOVjXA6c6O6b\n1fD1AqCsrKyMgoKCJCX/zc47w6abwkMPJf2hRUREajR0KJx3HsybB23bpuY5ysvLKSwsBCh09/Jk\nPGamdMEMAU4xs/5mtjUwDGgFjAAws8FmNrLiZDM7w8wONrMtEreTgAuAKBsUf/pp2BRIe7+IiEi6\nHX44LFsWNkHNJpnQBYO7P5RY8+NqQtfLG0Avd5+XOKUD0LHSJY2AwcAmwDLgQ+Aid787baErefTR\nsEXyQQfFeHYREclnv/sd7LprGApw3HGx09ReRhQgAO5+J3BnDV8bUOX+HcAd6chVG+PHw377wVpr\nxU4iIiL5qE8fuPJKWLgQWrWKnaZ2MqULJmt98w28+KJmv4iISDy9e8OiRTBpUuwktacCpIEmTAh/\nHnpo3BwiIpK/OneG7bfPrkXJVIA00COPwJ57wnrrxU4iIiL5rHfvMBB1yZLYSWpHBUgDzJ8PU6ao\n+0VEROLr0wd+/BGeey52ktpRAdIAEyeGSlPTb0VEJLYddoDNNsuebhgVIA0wfnxYgKxjx9WfKyIi\nkkpmoRXk0Udh+fLYaVZPBUg9LVoETz2l1g8REckcvXuHjemmT4+dZPVUgNTT5Mnwyy8a/yEiIplj\nt92gQ4fs6IZRAVJP48fDNtvAVlvFTiIiIhI0ahRaQcaPhwzY6m2VVIDUw9KlYf0PtX6IiEim6dMH\nPvkE3ngjdpJVUwFSD88/Dz/8oPEfIiKSefbeG9ZeO/O7YVSA1MMjj8DGG0PXrrGTiIiIrKxpUzjk\nkPBelclUgNTRihXhH7VPnzDlSUREJNP06QPvvguzZ8dOUjMVIHU0YwZ89RUcdljsJCIiItXr2RNa\ntIAnn4ydpGYqQOpo8mRo3Rq6dYudREREpHotW4Z9yiZPjp2kZipA6mjyZNhnH2jWLHYSERGRmhUV\nhUkTv/4aO0n1VIDUwcKF8NJL4R9VREQkkxUVhVW7p02LnaR6KkDq4MUXw+ZzKkBERCTT7bADrLde\n5nbDqACpg8mTYcMNoUuX2ElERERWrVEj2G8/FSA5YfLk0Pqh6bciIpINioqgrAy++y52kv+lAqSW\n5s6Ft95S94uIiGSP/fYLe8I880zsJP9LBUgtTZkS/uzRI24OERGR2urYMWyamondMCpAamnyZNh+\n+7DNsYiISLYoKgrvYZm2O64KkFpwDy0g6n4REZFsU1QUdsf96KPYSVaWMQWImZ1pZh+b2SIzm25m\nO6/i3N5m9rSZfWNm881smpn1TFW2WbPgiy9UgIiISPbZZx9o3DjzumEyogAxs6OBm4ArgK7Am8Ak\nM2tXwyV7AU8DBwAFwLPA42a2YyryTZ4cVj7da69UPLqIiEjqrLUW7LabCpCalADD3X2Uu88CBgEL\ngYHVnezuJe5+o7uXufuH7n4p8AFwSCrCTZ4M3btDq1apeHQREZHUKioKM2GWL4+d5DfRCxAzawoU\nAlMrjrm7A1OAWm35ZmYGrAl8n+x8S5fCc8+p+0VERLJXURH8+CO8/nrsJL+JXoAA7YDGwNwqx+cC\ntZ1zchHQGngoibkAmD4dFixQASIiItlrl11CV0wmdcM0iR2goczsWOAvwKHu/u3qzi8pKaFNmzYr\nHSsuLqa4uLja8ydPhnXWga5dk5FWREQk/Zo0gX33De9pl1226nNLS0spLS1d6dj8+fOTnsk88sTg\nRBfMQqCvu0+odHwE0Mbde6/i2mOAe4Aj3P3fq3meAqCsrKyMgoKCWufr1i0s5PJQ0ttWRERE0mfo\nUCgpge+/hzXWqNu15eXlFBYWAhS6e3ky8kTvgnH3pUAZ8P9rjCbGdPQAatxE2MyKgXuBY1ZXfNTX\njz/Cq6+q+0VERLJfUVEY1/j887GTBNELkIQhwClm1t/MtgaGAa2AEQBmNtjMRlacnOh2GQlcALxm\nZu0Tt7WSGerZZ2HFChUgIiKS/Tp3hk6dMmccSEaMAXH3hxJrflwNtAfeAHq5+7zEKR2AjpUuOYUw\ncHVo4lZhJDVM3a2PyZNhiy1gk02S9YgiIiJxmP22LHsmyIgCBMDd7wTurOFrA6rc3zcdmSZPVuuH\niIjkjqIiuPfesLr3734XN0umdMFknE8+gf/+VwWIiIjkjood3St2eI9JBUgNJk+GRo3CtCUREZFc\n0K5dWFYiE7phVIDUYPJk2HlnaNs2dhIREZHkKSoKLSCRV+FQAVKd5cth6lR1v4iISO4pKoK5c+Ht\nt+PmUAFSjRkzwkItKkBERCTX7LEHtGgRvxtGBUg1Jk+G1q3D9sUiIiK5pEUL2HNPFSAZafJk2Gcf\naNYsdhIREZHkKyqCF16AxYvjZVABUsXChfDyy+p+ERGR3FVUBIsWwbQaNzxJPRUgVbzwAixZogJE\nRERy1w47wHrrxe2GUQFSxeTJsOGG0KVL7CQiIiKp0agR7LefCpCMMmVKaP0wi51EREQkdYqKoLwc\nvvsuzvOrAKlk7lx46y11v4iISO4rKgqLkT3zTJznVwFSScXa+PvtFzeHiIhIqm20EWy9dbxuGBUg\nlTzzTBiY07597CQiIiKpV1QUVv6OQQVIJUOHwvjxsVOIiIikx5/+BK+9Fue5m8R52szUogVsvnns\nFCIiIumxwQbxnlstICIiIpJ2KkBEREQk7VSAiIiISNqpABEREZG0UwEiIiIiaacCRERERNJOBYiI\niIikXcYUIGZ2ppl9bGaLzGy6me28inM7mNkYM5ttZsvNbEg6s0rtlZaWxo6Qd/Sap59e8/TTa579\nMqIAMbOjgZuAK4CuwJvAJDNrV8MlzYFvgGuAN9ISUupFvyTST695+uk1Tz+95tkvIwoQoAQY7u6j\n3H0WMAhYCAys7mR3n+PuJe4+GvgpjTlFREQkCaIXIGbWFCgE/n87HHd3YArQLVYuERERSZ3oBQjQ\nDmgMzK1yfC7QIf1xREREJNXyaTO6FgAzZ86MnSOvzJ8/n/Ly8tgx8ope8/TTa55+es3Tq9J7Z4tk\nPaaF3o54El0wC4G+7j6h0vERQBt3772a658FZrj7+as571hgTMMTi4iI5K1+7j42GQ8UvQXE3Zea\nWRnQA5gAYGaWuH9bEp9qEtAP+ARYnMTHFRERyXUtgE0I76VJEb0ASRgCjEgUIq8SZsW0AkYAmNlg\nYEN3P6HiAjPbETBgDWC9xP0l7l5tH4u7fwckpWoTERHJQ9OS+WAZUYC4+0OJNT+uBtoT1vbo5e7z\nEqd0ADpWuWwGUNF/VAAcC8wBNkt9YhEREWmI6GNAREREJP9kwjRcERERyTMqQERERCTtcqYAqctm\ndonz9zGzMjNbbGbvm9kJqzpf/lcdNxDsbWZPm9k3ZjbfzKaZWc905s0Fdf05r3RddzNbamZaOKGO\n6vG7pZmZXWtmnyR+v3xkZiemKW5OqMdr3s/M3jCzX8zsSzO718zWSVfebGdme5rZBDP7wsxWmNmh\ntbimwe+hOVGA1HUzOzPbBHiCsPz7jsCtwD1mVpSOvLmgHhsI7gU8DRxAGDT8LPB4YvaS1EI9XvOK\n69oAIwnbG0gd1PM1/yewLzAA2BIoBmanOGrOqMfv8+6En+9/ANsARwC7AHenJXBuaE2Y/HEGv03u\nqFHS3kPdPetvwHTg1kr3DfgcuLiG868H3qpyrBSYGPt7yZZbXV/zGh7jHeCy2N9Lttzq+5onfrav\nIvxCL4/9fWTTrR6/W/YHvgfaxs6erbd6vOYXAB9UOXYW8Gns7yUbb8AK4NDVnJOU99CsbwGp52Z2\nu/G/nwYnreJ8qSQZGwgmFptbk/DLWlajvq+5mQ0ANiUUIFIH9XzNDwFeBy4xs8/NbLaZ3WBmSVu+\nOpfV8zX/D9DRzA5IPEZ74EjgydSmzWtJeQ/N+gKE+m1m16GG89cys+bJjZeTkrGB4EWEZr+Hkpgr\nl9X5NTezzsB1hKWTV6Q2Xk6qz8/5ZsCewLbA4cC5hC6BoSnKmGvq/Jq7+zTgOGCcmS0BvgJ+ILSC\nSGok5T00FwoQyTKJfXn+Ahzp7t/GzpOLzKwRYe+jK9z9w4rDESPli0aEJuxj3f11d/83cD5wgj7c\npIaZbUMYg3AlYXxZL0Kr3/CIsaQWMmIl1Ab6FlhOWEG1svbA1zVc83UN5//k7r8mN15Oqs9rDoCZ\nHUMYHHaEuz+bmng5qa6v+ZrA74GdzKzi03cjQu/XEqCnuz+Xoqy5oj4/518BX7j7gkrHZhKKv42A\nD6u9SirU5zX/I/Cyuw9J3H/HzM4AXjSzS9296id1abikvIdmfQuIuy8FKjazA1bazK6mdev/U/n8\nhJ6J47Ia9XzNMbNi4F7gmMQnQ6mlerzmPwHbATsRRqnvCAwDZiX+/kqKI2e9ev6cvwxsaGatKh3b\nitAq8nmKouaMer7mrYBlVY6tIMzmUKtfaiTnPTT2iNskjdo9ClgI9Ae2JjS9fQesl/j6YGBkpfM3\nAX4mjOTdijD1aAmwX+zvJVtu9XjNj028xoMIlXLFba3Y30u23Or6mldzvWbBpPg1J4xrmgOMA7oQ\npp/PBobF/l6y5VaP1/wE4NfE75ZNge6ETU2nxf5esuWW+LndkfCBZQVwXuJ+xxpe86S8h0b/xpP4\nAp4BfAIsIlRhv6/0tfuBZ6qcvxeh0l4EfAAcH/t7yLZbXV5zwrofy6u53Rf7+8imW11/zqtcqwIk\nDa85Ye2PScCCRDHyd6B57O8jm271eM3PBN5OvOafE9YF2SD295EtN2DvROFR7e/nVL2HajM6ERER\nSbusHwMiIiIi2UcFiIiIiKSdChARERFJOxUgIiIiknYqQERERCTtVICIiIhI2qkAERERkbRTASIi\nIiJppwJERERE0k4FiIiIiKSdChARERFJOxUgIiIiknZNYgcQkfxlZs8C7yTuHg8sBe5y98vjpRKR\ndFALiIjE1p9QeOwMnAOcb2YnxY0kIqlm7h47g4jkqUQLyHruvl2lY4OBQyofE5HcoxYQEYltepX7\n//zk1NAAAAC3SURBVAE6m5nFCCMi6aECRERERNJOBYiIxLZrlfvdgA9c/cMiOU0FiIjE1snMbjSz\nLc2sGDgLuCV2KBFJLU3DFZHYRgEtgVeBZcDN7n5P3EgikmoqQEQktqXufj5wZuwgIpI+6oIRERGR\ntFMBIiIxaaCpSJ7SQmQiIiKSdmoBERERkbRTASIiIiJppwJERERE0k4FiIiIiKSdChARERFJOxUg\nIiIiknYqQERERCTtVICIiIhI2v0f0iv6L2AQ9HEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7e41518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "# Ignore divide by zero error\n",
    "# Ref: http://stackoverflow.com/questions/14861891/runtimewarning-invalid-value-encountered-in-divide\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "entropy = lambda p: np.sum(p * np.log2(1 / p))\n",
    "\n",
    "p_list = np.linspace(0, 1, 50)\n",
    "plt.figure()\n",
    "plt.plot(p_list,\n",
    "         [entropy(np.array([p, 1-p])) for p in p_list])\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('H(Ber(p))')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Information divergence \n",
    "\n",
    "Information divergence (also called “Kullback-Leibler divergence\" or “KL divergence\" for short, or also “relative entropy\") is a measure of how different two distributions $p$ and $q$ (over the same alphabet) are. To come up with information divergence, first, note that entropy of a random variable with distribution $p$ could be thought of as the expected number of bits needed to encode a sample from $p$ using the information content according to distribution $p$:\n",
    "\n",
    "$$\\underbrace{\\sum _{x}p(x)}_{\\begin{matrix}\\text {expectation } \\\\ \\text{using }p\\end{matrix} }\\underbrace{\\log _{2}\\frac{1}{p(x)}}_{\\begin{matrix}\\text { information content } \\\\ \\text{ according to }p \\end{matrix}}\\triangleq \\mathbb {E}_{X \\sim p}\\Big[\\log _{2}\\frac{1}{p(X)}\\Big].$$\n",
    " \n",
    "Here, we have introduced a new notation: $\\mathbb {E}_{X \\sim p}$ means that we are taking the expectation with respect to random variable $X$ drawn from the distribution $p$. If it's clear which random variable we are taking the expectation with respect to, we will often just abbreviate the notation and write $\\mathbb {E}_ p$ instead of $\\mathbb {E}_{X \\sim p}$.\n",
    "\n",
    "If instead we look at the information content according to a different distribution $q$, we get\n",
    "\n",
    "$$\\underbrace{\\sum _{x}p(x)}_{\\begin{matrix}\\text {expectation } \\\\ \\text{using }p\\end{matrix} }\\underbrace{\\log _{2}\\frac{1}{p(x)}}_{\\begin{matrix}\\text { information content } \\\\ \\text{ according to }p \\end{matrix}}\\triangleq \\mathbb {E}_{X \\sim p}\\Big[\\log _{2}\\frac{1}{p(X)}\\Big].$$\n",
    " \n",
    "It turns out that if we are actually sampling from p but encoding samples as if they were from a different distribution $q$, then we always need to use more bits! This isn't terribly surprising in light of the fundamental result we alluded to that entropy of a random variable with distribution $p$ is the minimum number of bits needed to encode samples from $p$.\n",
    "\n",
    "Information divergence is the price you pay in bits for trying to encode a sample from $p$ using information content according to $q$ instead of according to $p$:\n",
    "\n",
    "$$D(p\\parallel q)=\\mathbb {E}_{X \\sim p}\\Big[\\log _{2}\\frac{1}{q(X)}\\Big]-\\mathbb {E}_{X \\sim p}\\Big[\\log _{2}\\frac{1}{p(X)}\\Big].$$\n",
    " \n",
    "Information divergence is always at least $0$, and when it is equal to $0$, then this means that $p$ and $q$ are the same distribution (i.e., $p(x) = q(x)$ for all $x$). This property is called Gibbs' inequality.\n",
    "\n",
    "Gibbs' inequality makes information divergence seem a bit like a distance. However, information divergence is not like a distance in that it is not symmetric: in general, $D(p \\parallel q) \\ne D(q \\parallel p).$\n",
    "\n",
    "Often times, the equation for information divergence is written more concisely as\n",
    "\n",
    "$$D(p\\parallel q) = \\sum _ x p(x) \\log \\frac{p(x)}{q(x)},$$\n",
    " \n",
    "which you can get as follows:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "D(p\\parallel q)\n",
    "&=&\n",
    "  \\mathbb{E}_{X \\sim p}\\Big[\\log_{2}\\frac{1}{q(X)}\\Big]\n",
    "- \\mathbb{E}_{X \\sim p}\\Big[\\log_{2}\\frac{1}{p(X)}\\Big] \\\\\n",
    "&=&\n",
    "  \\sum_x p(x) \\log_2 \\frac{1}{q(x)}\n",
    "- \\sum_x p(x) \\log_2 \\frac{1}{p(x)} \\\\\n",
    "&=&\n",
    "  \\sum_x p(x)\n",
    "  \\Big[ \\log_2 \\frac{1}{q(x)} - \\log_2 \\frac{1}{p(x)} \\Big] \\\\\n",
    "&=&\n",
    "  \\sum_x p(x)\n",
    "  \\log_2 \\frac{p(x)}{q(x)}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Example: Suppose $p$ is the distribution for a fair coin flip:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "p(x)\n",
    "&=&\n",
    "\\begin{cases}\n",
    "\\frac12 & \\text{if }x=\\text{heads}, \\\\\n",
    "\\frac12 & \\text{if }x=\\text{tails}. \\\\\n",
    "\\end{cases}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Meanwhile, suppose q is a distribution for a biased coin that always comes up heads (perhaps it's double-headed):\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "q(x)\n",
    "&=&\n",
    "\\begin{cases}\n",
    "1 & \\text{if }x=\\text{heads}, \\\\\n",
    "0 & \\text{if }x=\\text{tails}. \\\\\n",
    "\\end{cases}\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "D(p \\parallel q)\n",
    "&=&\n",
    "  p(\\text{heads}) \\log_2 \\frac{p(\\text{heads})}{q(\\text{heads})}\n",
    "+ p(\\text{tails}) \\log_2 \\frac{p(\\text{tails})}{q(\\text{tails})} \\\\\n",
    "&=&\n",
    "  \\frac12\\log_2 \\frac{\\frac12}1\n",
    "+ \\underbrace{\\frac12\\log_2 \\frac{\\frac12}0}_{\\infty} \\\\\n",
    "&=&\n",
    "  \\infty\\text{ bits}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "This is not surprising: If we are sampling from $p$ (for which we could get tails) but trying to encode the sample using $q$ (which cannot possibly encode tails), then if we get tails, we are stuck: we can't store it! This incurs a penalty of infinity bits.\n",
    "\n",
    "Meanwhile,\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "D(q \\parallel p)\n",
    "&=&\n",
    "  q(\\text{heads}) \\log_2 \\frac{q(\\text{heads})}{p(\\text{heads})}\n",
    "+ q(\\text{tails}) \\log_2 \\frac{q(\\text{tails})}{p(\\text{tails})} \\\\\n",
    "&=&\n",
    "  1 \\log_2 \\frac1{\\frac12}\n",
    "+ \\underbrace{0 \\log_2 \\frac0{\\frac12}}_0 \\\\\n",
    "&=&\n",
    "  1\\text{ bit}.\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "When we sample from $q$, we always get heads. In fact, as we saw previously, the entropy of the distribution for an always-heads coin flip is $0$ bits since there's no randomness. But here we are sampling from $q$ and storing the sample using distribution $p$. For a fair coin flip, encoding using distribution $p$ would store each sample using on average $1$ bit. Thus, even though a sample from $q$ is deterministically heads, we store it using $1$ bit. This is the penalty we pay for storing a sample from $q$ using distribution $p$.\n",
    "\n",
    "Notice that in this example, $D(p \\parallel q) \\ne D(q \\parallel p)$. They aren't even close — one is infinity and the other is finite!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Information Divergence\n",
    "\n",
    "We now look at a different way to think of Shannon entropy for a random variable $X$ with alphabet $\\mathcal{X}.$\n",
    "\n",
    "Let random variable $U$ have what's called a uniform distribution over alphabet $\\mathcal{X}$, meaning that\n",
    "\n",
    "$$p_ U(x) = \\frac{1}{|\\mathcal{X}|} \\qquad \\text {for all }x\\in \\mathcal{X}.$$\n",
    " \n",
    "Notationally, we can write $U \\sim \\text {Uniform}(\\mathcal{X})$.\n",
    "\n",
    "In the following problems, suppose the number of labels in $\\mathcal{X}$ is given by $k$, i.e., $k = |\\mathcal{X}|.$\n",
    "\n",
    "**Question:** What is $H(U)$ in terms of $k$?\n",
    "\n",
    "\n",
    "**Question:** You may use the function log, which for just this part you can treat as log base 2 even though we aren't explicitly writing out the base 2 part. (So for example, log(x^2) would be log base 2 of x^2.)\n",
    "\n",
    "Next, we examine the divergence between $p_X$ and the uniform distribution. Show that $D(p_ X \\parallel p_ U)$ can be written of the form\n",
    "\n",
    "$$D(p_ X \\parallel p_ U) = f(k) - H(X),$$\n",
    " \n",
    "for a function $f$ that you will determine:\n",
    "\n",
    "**Question:** What is $f$? \n",
    "\n",
    "**Question:** Your answers to the previous two parts should tell you how the entropy of a uniform distribution (over an alphabet of size $k$) relates to the entropy of any distribution $p_ X$ (over the same alphabet of size $k$).\n",
    "\n",
    "Fill in the blanks:\n",
    "\n",
    "**Question:** Because of Gibbs' inequality, the entropy of random variable _________ cannot be larger than the entropy of random variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
